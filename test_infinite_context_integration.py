#!/usr/bin/env python3
"""
Infini-Attention æ— é™ä¸Šä¸‹æ–‡é›†æˆæµ‹è¯•
éªŒè¯æ— é™ä¸Šä¸‹æ–‡åŠŸèƒ½ä¸é›¶å·è¡ŒåŠ¨é¡¹ç›®çš„é›†æˆæ•ˆæœ
"""

import asyncio
import time
import torch
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


async def test_infini_attention_core():
    """æµ‹è¯• Infini-Attention æ ¸å¿ƒåŠŸèƒ½"""
    print("ğŸ§  æµ‹è¯• Infini-Attention æ ¸å¿ƒåŠŸèƒ½")
    print("=" * 50)
    
    try:
        from python.helpers.infini_attention_core import (
            InfiniAttentionLayer, InfiniAttentionConfig, SegmentProcessor
        )
        
        # åˆ›å»ºé…ç½®
        config = InfiniAttentionConfig(
            hidden_size=256,  # å‡å°å°ºå¯¸ä»¥ä¾¿æµ‹è¯•
            num_attention_heads=8,
            head_dim=32,
            segment_length=128
        )
        
        # åˆ›å»º Infini-Attention å±‚
        infini_layer = InfiniAttentionLayer(config)
        
        print(f"  âœ… Infini-Attention å±‚åˆ›å»ºæˆåŠŸ")
        print(f"     éšè—å±‚å¤§å°: {config.hidden_size}")
        print(f"     æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}")
        print(f"     æ®µé•¿åº¦: {config.segment_length}")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size = 2
        seq_len = 64
        hidden_size = config.hidden_size
        
        test_input = torch.randn(batch_size, seq_len, hidden_size)
        attention_mask = torch.ones(batch_size, seq_len)
        
        print(f"  ğŸ“Š æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        
        # å‰å‘ä¼ æ’­æµ‹è¯•
        start_time = time.time()
        output, stats = infini_layer.forward(
            test_input,
            attention_mask=attention_mask,
            is_segment_boundary=False
        )
        processing_time = (time.time() - start_time) * 1000
        
        print(f"  âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"     è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"     å¤„ç†æ—¶é—´: {processing_time:.2f}ms")
        print(f"     è®°å¿†ä½¿ç”¨æ¬¡æ•°: {stats['memory_stats']['memory_usage_count']}")
        print(f"     è®°å¿†æ›´æ–°æ¬¡æ•°: {stats['memory_stats']['memory_update_count']}")
        
        # æµ‹è¯•å¤šæ®µå¤„ç†
        print(f"  ğŸ”„ æµ‹è¯•å¤šæ®µå¤„ç†...")
        segment_processor = SegmentProcessor(segment_length=32)
        
        # åˆ›å»ºé•¿åºåˆ—
        long_input = torch.randn(1, 100, hidden_size)
        segments = segment_processor.segment_sequence(
            torch.randint(0, 1000, (1, 100)),  # æ¨¡æ‹Ÿtoken ids
            torch.ones(1, 100)  # attention mask
        )
        
        print(f"     é•¿åºåˆ—åˆ†æ®µæ•°: {len(segments)}")
        
        # é€æ®µå¤„ç†
        segment_outputs = []
        for i, (seg_ids, seg_mask, is_last) in enumerate(segments):
            seg_input = long_input[:, i*32:(i+1)*32, :]
            if seg_input.size(1) == 0:
                break
                
            seg_output, seg_stats = infini_layer.forward(
                seg_input,
                attention_mask=seg_mask,
                is_segment_boundary=is_last
            )
            segment_outputs.append(seg_output)
        
        print(f"     å¤„ç†æ®µæ•°: {len(segment_outputs)}")
        
        # è·å–æœ€ç»ˆè®°å¿†ç»Ÿè®¡
        final_stats = infini_layer.get_memory_info()
        print(f"  ğŸ“ˆ æœ€ç»ˆè®°å¿†ç»Ÿè®¡:")
        print(f"     è®°å¿†çŸ©é˜µèŒƒæ•°: {final_stats['memory_matrix_norm']:.4f}")
        print(f"     Zå‘é‡èŒƒæ•°: {final_stats['z_vector_norm']:.4f}")
        print(f"     Betaå€¼: {final_stats['beta_value']:.4f}")
        print(f"     è®°å¿†å¤§å°: {final_stats['memory_size_mb']:.2f}MB")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Infini-Attention æ ¸å¿ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_infinite_context_engine():
    """æµ‹è¯•æ— é™ä¸Šä¸‹æ–‡å¼•æ“"""
    print("\nğŸŒ æµ‹è¯•æ— é™ä¸Šä¸‹æ–‡å¼•æ“")
    print("=" * 50)
    
    try:
        from python.helpers.infinite_context_engine import (
            get_infinite_context_engine, InfiniteContextConfig, ContextProcessingMode
        )
        
        # åˆ›å»ºé…ç½®
        config = InfiniteContextConfig(
            max_context_length=100000,
            segment_length=512,
            adaptive_threshold=2048
        )
        
        # è·å–å¼•æ“å®ä¾‹
        engine = get_infinite_context_engine(config)
        
        print(f"  âœ… æ— é™ä¸Šä¸‹æ–‡å¼•æ“åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•çŸ­ä¸Šä¸‹æ–‡å¤„ç†
        short_context = "è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æµ‹è¯•ä¸Šä¸‹æ–‡ï¼Œç”¨äºéªŒè¯åŸºæœ¬åŠŸèƒ½ã€‚"
        
        print(f"  ğŸ“ æµ‹è¯•çŸ­ä¸Šä¸‹æ–‡å¤„ç†...")
        start_time = time.time()
        result = await engine.process_context(
            short_context,
            mode=ContextProcessingMode.ADAPTIVE
        )
        processing_time = (time.time() - start_time) * 1000
        
        print(f"     å¤„ç†æ¨¡å¼: {result['processing_mode']}")
        print(f"     å¤„ç†æ—¶é—´: {processing_time:.2f}ms")
        print(f"     ä¸Šä¸‹æ–‡ç»Ÿè®¡: {result['context_stats']['total_segments']}æ®µ")
        
        # æµ‹è¯•é•¿ä¸Šä¸‹æ–‡å¤„ç†
        long_context = " ".join([
            f"è¿™æ˜¯ç¬¬{i}æ®µé•¿ä¸Šä¸‹æ–‡å†…å®¹ï¼ŒåŒ…å«äº†å¤§é‡çš„ä¿¡æ¯å’Œç»†èŠ‚ã€‚" * 10
            for i in range(50)
        ])
        
        print(f"  ğŸ“š æµ‹è¯•é•¿ä¸Šä¸‹æ–‡å¤„ç†...")
        print(f"     ä¸Šä¸‹æ–‡é•¿åº¦: {len(long_context)}å­—ç¬¦")
        
        start_time = time.time()
        result = await engine.process_context(
            long_context,
            mode=ContextProcessingMode.INFINITE
        )
        processing_time = (time.time() - start_time) * 1000
        
        print(f"     å¤„ç†æ¨¡å¼: {result['processing_mode']}")
        print(f"     å¤„ç†æ—¶é—´: {processing_time:.2f}ms")
        print(f"     å¤„ç†æ®µæ•°: {result['result'].get('total_segments', 0)}")
        print(f"     è®°å¿†ä¿¡æ¯: {result['memory_stats']['memory_usage_count']}æ¬¡ä½¿ç”¨")
        
        # æµ‹è¯•æ€§èƒ½æŠ¥å‘Š
        performance_report = engine.get_performance_report()
        print(f"  ğŸ“Š æ€§èƒ½æŠ¥å‘Š:")
        print(f"     æ€»å¤„ç†æ—¶é—´: {performance_report['processing_stats']['processing_time_ms']:.2f}ms")
        print(f"     å¤„ç†æ®µæ•°: {performance_report['processing_stats']['total_segments_processed']}")
        print(f"     æ•ˆç‡æŒ‡æ ‡: {performance_report['efficiency_metrics']}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ æ— é™ä¸Šä¸‹æ–‡å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_unified_context_reasoning():
    """æµ‹è¯•ç»Ÿä¸€ä¸Šä¸‹æ–‡æ¨ç†æ¨¡å—"""
    print("\nğŸ”— æµ‹è¯•ç»Ÿä¸€ä¸Šä¸‹æ–‡æ¨ç†æ¨¡å—")
    print("=" * 50)
    
    try:
        from python.helpers.unified_context_reasoning_module import (
            get_unified_context_reasoning_module, unified_process,
            ProcessingRequest, UnifiedProcessingConfig
        )
        from python.helpers.intelligent_model_dispatcher import TaskType
        
        # åˆ›å»ºé…ç½®
        config = UnifiedProcessingConfig(
            enable_infinite_context=True,
            enable_intelligent_reasoning=True,
            enable_smart_model_selection=True
        )
        
        # è·å–æ¨¡å—å®ä¾‹
        module = get_unified_context_reasoning_module(config)
        
        print(f"  âœ… ç»Ÿä¸€æ¨¡å—åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„è¯·æ±‚
        test_cases = [
            {
                "name": "ç®€å•å¯¹è¯",
                "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚",
                "task_type": TaskType.CHAT,
                "require_reasoning": False
            },
            {
                "name": "ä»£ç ç”Ÿæˆ",
                "content": "è¯·ç”¨Pythonç¼–å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ï¼ŒåŒ…å«è¯¦ç»†æ³¨é‡Šã€‚",
                "task_type": TaskType.CODING,
                "require_reasoning": True
            },
            {
                "name": "é•¿æ–‡æœ¬åˆ†æ",
                "content": "è¯·åˆ†æä»¥ä¸‹é•¿æ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹å’Œç»“è®ºï¼š" + "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬å†…å®¹ã€‚" * 100,
                "task_type": TaskType.ANALYSIS,
                "require_infinite_context": True
            }
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"  ğŸ§ª æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['name']}")
            
            start_time = time.time()
            result = await unified_process(
                content=test_case["content"],
                task_type=test_case["task_type"],
                require_reasoning=test_case.get("require_reasoning", False),
                require_infinite_context=test_case.get("require_infinite_context", False)
            )
            processing_time = (time.time() - start_time) * 1000
            
            print(f"     å¤„ç†ç»“æœ: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}")
            print(f"     å¤„ç†æ—¶é—´: {processing_time:.2f}ms")
            print(f"     APIè°ƒç”¨æ¬¡æ•°: {result.api_calls_made}")
            print(f"     è´¨é‡è¯„åˆ†: {result.quality_score:.2f}")
            print(f"     æˆæœ¬ä¼°ç®—: ${result.cost_estimate:.4f}")
            
            if not result.success:
                print(f"     é”™è¯¯ä¿¡æ¯: {result.error_message}")
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        performance_report = module.get_performance_report()
        print(f"  ğŸ“ˆ æ¨¡å—æ€§èƒ½æŠ¥å‘Š:")
        print(f"     æ€»è¯·æ±‚æ•°: {performance_report['overall_stats']['total_requests']}")
        print(f"     æˆåŠŸç‡: {performance_report['recent_performance']['success_rate']:.2%}")
        print(f"     å¹³å‡å¤„ç†æ—¶é—´: {performance_report['recent_performance']['avg_processing_time_ms']:.2f}ms")
        print(f"     å¹³å‡è´¨é‡è¯„åˆ†: {performance_report['recent_performance']['avg_quality_score']:.2f}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ ç»Ÿä¸€ä¸Šä¸‹æ–‡æ¨ç†æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_adaptive_context_manager():
    """æµ‹è¯•è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    print("\nâš¡ æµ‹è¯•è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
    print("=" * 50)
    
    try:
        from python.helpers.adaptive_context_manager import (
            get_adaptive_context_manager, adaptive_process, AdaptiveConfig
        )
        from python.helpers.intelligent_model_dispatcher import TaskType
        
        # åˆ›å»ºé…ç½®
        config = AdaptiveConfig(
            infinite_context_threshold=1024,
            reasoning_threshold=512,
            adaptation_interval_seconds=5.0
        )
        
        # è·å–ç®¡ç†å™¨å®ä¾‹
        manager = await get_adaptive_context_manager(config)
        
        print(f"  âœ… è‡ªé€‚åº”ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç›‘æ§ç³»ç»Ÿæ”¶é›†æ•°æ®
        print(f"  â±ï¸  ç­‰å¾…ç³»ç»Ÿç›‘æ§æ”¶é›†æ•°æ®...")
        await asyncio.sleep(2)
        
        # è·å–çŠ¶æ€æŠ¥å‘Š
        status_report = manager.get_status_report()
        print(f"  ğŸ“Š ç³»ç»ŸçŠ¶æ€:")
        print(f"     å½“å‰ç­–ç•¥: {status_report['current_strategy']}")
        print(f"     CPUä½¿ç”¨ç‡: {status_report['system_metrics']['cpu_usage']:.1f}%")
        print(f"     å†…å­˜ä½¿ç”¨ç‡: {status_report['system_metrics']['memory_usage']:.1f}%")
        print(f"     é˜Ÿåˆ—é•¿åº¦: {status_report['queue_status']['queue_length']}")
        print(f"     å·¥ä½œå™¨æ•°é‡: {status_report['queue_status']['worker_count']}")
        
        # æµ‹è¯•è‡ªé€‚åº”å¤„ç†
        test_requests = [
            ("ç®€å•è¯·æ±‚", "Hello, how are you?", TaskType.CHAT),
            ("ä¸­ç­‰è¯·æ±‚", "è¯·è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨åœºæ™¯ã€‚", TaskType.ANALYSIS),
            ("å¤æ‚è¯·æ±‚", "è¯·è¯¦ç»†åˆ†ææ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å‘å±•å†ç¨‹å’ŒæŠ€æœ¯çªç ´ã€‚" * 10, TaskType.REASONING)
        ]
        
        for name, content, task_type in test_requests:
            print(f"  ğŸš€ æäº¤{name}...")
            
            try:
                response_id = await adaptive_process(
                    content=content,
                    task_type=task_type,
                    priority=5
                )
                print(f"     å“åº”ID: {response_id}")
            except Exception as e:
                print(f"     å¤„ç†å¤±è´¥: {e}")
        
        # ç­‰å¾…å¤„ç†å®Œæˆ
        await asyncio.sleep(3)
        
        # è·å–æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š
        final_status = manager.get_status_report()
        print(f"  ğŸ“ˆ æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡:")
        print(f"     æ€»è¯·æ±‚æ•°: {final_status['performance_stats']['total_requests']}")
        print(f"     æˆåŠŸè¯·æ±‚æ•°: {final_status['performance_stats']['successful_requests']}")
        print(f"     å¹³å‡å“åº”æ—¶é—´: {final_status['performance_stats']['avg_response_time_ms']:.2f}ms")
        print(f"     ç­–ç•¥åˆ‡æ¢æ¬¡æ•°: {final_status['performance_stats']['strategy_switches']}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_agent_integration():
    """æµ‹è¯•ä¸Agentç±»çš„é›†æˆ"""
    print("\nğŸ¤– æµ‹è¯•ä¸Agentç±»çš„é›†æˆ")
    print("=" * 50)
    
    try:
        # è¿™é‡Œåªèƒ½æµ‹è¯•å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½ï¼Œå› ä¸ºAgentéœ€è¦å®Œæ•´çš„ç¯å¢ƒ
        print(f"  ğŸ“¦ æ£€æŸ¥Agentç±»é›†æˆ...")
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸå¯¼å…¥äº†æ— é™ä¸Šä¸‹æ–‡åŠŸèƒ½
        import agent
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æ–¹æ³•
        agent_methods = dir(agent.Agent)
        has_infinite_context = '_call_infinite_context_model' in agent_methods
        has_reasoning_check = '_should_use_reasoning' in agent_methods
        
        print(f"     æ— é™ä¸Šä¸‹æ–‡æ–¹æ³•: {'âœ…' if has_infinite_context else 'âŒ'}")
        print(f"     æ¨ç†åˆ¤æ–­æ–¹æ³•: {'âœ…' if has_reasoning_check else 'âŒ'}")
        
        # æ£€æŸ¥å¸¸é‡
        has_infinite_constant = hasattr(agent, 'INFINITE_CONTEXT_SYSTEM_AVAILABLE')
        print(f"     ç³»ç»Ÿå¯ç”¨æ€§æ£€æŸ¥: {'âœ…' if has_infinite_constant else 'âŒ'}")
        
        if has_infinite_constant:
            print(f"     æ— é™ä¸Šä¸‹æ–‡ç³»ç»ŸçŠ¶æ€: {agent.INFINITE_CONTEXT_SYSTEM_AVAILABLE}")
        
        print(f"  âœ… Agentç±»é›†æˆæ£€æŸ¥å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Agentç±»é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def calculate_performance_improvements():
    """è®¡ç®—æ€§èƒ½æ”¹è¿›é¢„ä¼°"""
    print("\nğŸ“Š æ€§èƒ½æ”¹è¿›é¢„ä¼°")
    print("=" * 50)
    
    # åŸºäºç†è®ºåˆ†æçš„æ€§èƒ½æ”¹è¿›é¢„ä¼°
    improvements = {
        "ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›": {
            "ä¼ ç»Ÿç³»ç»Ÿ": "4K-32K tokens",
            "æ— é™ä¸Šä¸‹æ–‡": "ç†è®ºæ— é™",
            "æ”¹è¿›å€æ•°": "10-100x"
        },
        "APIè°ƒç”¨å‡å°‘": {
            "é•¿å¯¹è¯åœºæ™¯": "70-80%",
            "æ–‡æ¡£åˆ†æ": "60-70%",
            "ä»£ç ç”Ÿæˆ": "50-60%"
        },
        "å“åº”é€Ÿåº¦": {
            "çŸ­ä¸Šä¸‹æ–‡": "åŸºæœ¬æŒå¹³",
            "ä¸­ç­‰ä¸Šä¸‹æ–‡": "20-30%æå‡",
            "é•¿ä¸Šä¸‹æ–‡": "50-70%æå‡"
        },
        "å†…å­˜æ•ˆç‡": {
            "å‹ç¼©æ¯”": "10:1 - 100:1",
            "å†…å­˜ä½¿ç”¨": "å‡å°‘60-80%",
            "ç¼“å­˜å‘½ä¸­ç‡": "æå‡40-60%"
        },
        "æˆæœ¬èŠ‚çº¦": {
            "APIè°ƒç”¨æˆæœ¬": "å‡å°‘70-80%",
            "è®¡ç®—èµ„æº": "å‡å°‘50-60%",
            "æ€»ä½“æˆæœ¬": "å‡å°‘60-75%"
        }
    }
    
    for category, metrics in improvements.items():
        print(f"  ğŸ“ˆ {category}:")
        for metric, value in metrics.items():
            print(f"     {metric}: {value}")
        print()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Infini-Attention æ— é™ä¸Šä¸‹æ–‡é›†æˆæµ‹è¯•")
    print("=" * 60)
    print("æœ¬æµ‹è¯•å°†éªŒè¯ä»¥ä¸‹åŠŸèƒ½:")
    print("  ğŸ§  Infini-Attention æ ¸å¿ƒç®—æ³•")
    print("  ğŸŒ æ— é™ä¸Šä¸‹æ–‡å¼•æ“")
    print("  ğŸ”— ç»Ÿä¸€ä¸Šä¸‹æ–‡æ¨ç†æ¨¡å—")
    print("  âš¡ è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
    print("  ğŸ¤– Agentç±»é›†æˆ")
    print("=" * 60)
    
    start_time = time.time()
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_results = {}
    
    test_results["infini_attention_core"] = await test_infini_attention_core()
    test_results["infinite_context_engine"] = await test_infinite_context_engine()
    test_results["unified_context_reasoning"] = await test_unified_context_reasoning()
    test_results["adaptive_context_manager"] = await test_adaptive_context_manager()
    test_results["agent_integration"] = await test_agent_integration()
    
    total_time = time.time() - start_time
    
    # è®¡ç®—æ€§èƒ½æ”¹è¿›
    calculate_performance_improvements()
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    
    total_tests = len(test_results)
    passed_tests = sum(1 for result in test_results.values() if result)
    
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
    print(f"æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    print("\nè¯¦ç»†ç»“æœ:")
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    if passed_tests == total_tests:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ— é™ä¸Šä¸‹æ–‡ç³»ç»Ÿé›†æˆæˆåŠŸã€‚")
        print(f"\nğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨æ— é™ä¸Šä¸‹æ–‡åŠŸèƒ½ï¼")
        
        print(f"\nğŸ“š ä½¿ç”¨æŒ‡å—:")
        print(f"1. åœ¨Agentä¸­ï¼Œé•¿ä¸Šä¸‹æ–‡ä¼šè‡ªåŠ¨ä½¿ç”¨æ— é™ä¸Šä¸‹æ–‡å¤„ç†")
        print(f"2. ä½¿ç”¨ unified_process() å‡½æ•°è¿›è¡Œç»Ÿä¸€å¤„ç†")
        print(f"3. ä½¿ç”¨ adaptive_process() å‡½æ•°è¿›è¡Œè‡ªé€‚åº”å¤„ç†")
        print(f"4. ç³»ç»Ÿä¼šæ ¹æ®ä¸Šä¸‹æ–‡é•¿åº¦è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥")
        
    else:
        print(f"\nâš ï¸ æœ‰ {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³ç»„ä»¶ã€‚")
    
    print("=" * 60)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æµ‹è¯•å·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

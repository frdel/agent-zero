Directory Structure:

└── ./
    ├── .vscode
    │   ├── extensions.json
    │   ├── launch.json
    │   └── settings.json
    ├── docker
    │   ├── .bashrc
    │   ├── build.txt
    │   ├── docker-compose.yml
    │   ├── Dockerfile
    │   └── initialize.sh
    ├── docs
    │   ├── nvidia-docker-wsl2-guide.md
    │   └── win_installation_guide.txt
    ├── logs
    │   └── .gitkeep
    ├── memory
    │   └── .gitkeep
    ├── prompts
    │   ├── agent.memory.md
    │   ├── agent.system.md
    │   ├── agent.tools.md
    │   ├── fw.code_no_output.md
    │   ├── fw.code_runtime_wrong.md
    │   ├── fw.error.md
    │   ├── fw.intervention.md
    │   ├── fw.memories_deleted.md
    │   ├── fw.memories_not_found.md
    │   ├── fw.memory_saved.md
    │   ├── fw.msg_cleanup.md
    │   ├── fw.msg_from_subordinate.md
    │   ├── fw.msg_misformat.md
    │   ├── fw.msg_repeat.md
    │   ├── fw.msg_timeout.md
    │   ├── fw.msg_truncated.md
    │   ├── fw.tool_not_found.md
    │   ├── fw.tool_response.md
    │   ├── fw.user_message.md
    │   ├── msg.memory_cleanup.md
    │   └── tool.knowledge.response.md
    ├── python
    │   ├── helpers
    │   │   ├── dirty_json.py
    │   │   ├── docker.py
    │   │   ├── duckduckgo_search.py
    │   │   ├── errors.py
    │   │   ├── extract_tools.py
    │   │   ├── files.py
    │   │   ├── messages.py
    │   │   ├── perplexity_search.py
    │   │   ├── print_style.py
    │   │   ├── rate_limiter.py
    │   │   ├── shell_local.py
    │   │   ├── shell_ssh.py
    │   │   ├── timed_input.py
    │   │   ├── tool.py
    │   │   ├── vdb.py
    │   │   └── vector_db.py
    │   ├── tools
    │   │   ├── call_subordinate.py
    │   │   ├── code_execution_tool.py
    │   │   ├── knowledge_tool.py
    │   │   ├── memory_tool.py
    │   │   ├── online_knowledge_tool.py
    │   │   ├── response.py
    │   │   ├── task_done.py
    │   │   └── unknown.py
    │   └── __init__.py
    ├── work_dir
    │   └── .gitkeep
    ├── .gitattributes
    ├── .gitignore
    ├── agent.py
    ├── example.env
    ├── LICENSE
    ├── main.py
    ├── models.py
    ├── README.md
    └── requirements.txt



---
File: /.vscode/extensions.json
---

{
    "recommendations": [
        "usernamehw.errorlens",
        "ms-python.debugpy",
        "ms-python.python"
    ]
}


---
File: /.vscode/launch.json
---

{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Debug main.py",
            "type": "debugpy",
            "request": "launch",
            "program": "./main.py",
            "console": "integratedTerminal",
            "args": ["-Xfrozen_modules=off"]
        },
        {
            "name": "Debug current file",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": ["-Xfrozen_modules=off"]
        }
    ]
}


---
File: /.vscode/settings.json
---

{
    "python.analysis.typeCheckingMode": "standard"
}


---
File: /docker/.bashrc
---

# .bashrc

# Source global definitions
if [ -f /etc/bashrc ]; then
    . /etc/bashrc
fi



---
File: /docker/build.txt
---

docker buildx build --platform linux/amd64,linux/arm64 -t docker-agent-zero-exe:latest --push .


---
File: /docker/docker-compose.yml
---

services:
  agent-zero-exe:
    build:
      context: ./
      dockerfile: Dockerfile
    image: docker-agent-zero-exe:latest
    volumes:
      - ../work_dir:/workspace
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "50022:22"


---
File: /docker/Dockerfile
---

# Use the latest slim version of Debian
# FROM --platform=$TARGETPLATFORM debian:bookworm-slim

# Specify the base image with CUDA support
FROM --platform=$TARGETPLATFORM nvidia/cuda:11.0.3-base-ubuntu20.04

# Set ARG for platform-specific commands
ARG TARGETPLATFORM

# Set environment variables
ENV TZ=America/Chicago
ENV DEBIAN_FRONTEND=noninteractive
ENV PATH="/usr/bin:$PATH"

# Update and install necessary packages, including tzdata
RUN apt-get update && apt-get install -y \
    tzdata \
    python3 \
    python3-pip \
    nodejs \
    npm \
    openssh-server \
    sudo \
    cmake \
    && rm -rf /var/lib/apt/lists/*

# Set up timezone
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone \
    && dpkg-reconfigure -f noninteractive tzdata

# Set up SSH
RUN mkdir /var/run/sshd && \
    echo 'root:toor' | chpasswd && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config

# Copy initial .bashrc with virtual environment activation to a temporary location
COPY .bashrc /etc/skel/.bashrc

# Copy the script to ensure .bashrc is in the root directory
COPY initialize.sh /usr/local/bin/initialize.sh
RUN chmod +x /usr/local/bin/initialize.sh

# Ensure pip is upgraded and install Python packages globally
RUN python3 -m pip install --upgrade pip

# Install required Python packages globally with no cache and to the target directory
RUN python3 -m pip install --target=/usr/local/lib/python3.8/dist-packages \
    numpy scipy pandas torch torchvision torchaudio \
    tensorflow scikit-learn transformers accelerate diffusers \
    opencv-python matplotlib seaborn

# Expose SSH port
EXPOSE 22

# Init .bashrc
CMD ["/usr/local/bin/initialize.sh"]



---
File: /docker/initialize.sh
---

#!/bin/bash

# Ensure .bashrc is in the root directory
if [ ! -f /root/.bashrc ]; then
    cp /etc/skel/.bashrc /root/.bashrc
    chmod 444 /root/.bashrc
fi

# Ensure .profile is in the root directory
if [ ! -f /root/.profile ]; then
    cp /etc/skel/.bashrc /root/.profile
    chmod 444 /root/.profile
fi

apt-get update

# Start SSH service
exec /usr/sbin/sshd -D



---
File: /docs/nvidia-docker-wsl2-guide.md
---

# Comprehensive NVIDIA Docker Setup on Ubuntu WSL2

This guide provides detailed instructions for setting up NVIDIA Docker on Ubuntu within WSL2 (Windows Subsystem for Linux 2). It's designed for users of all experience levels, including those new to Ubuntu and Docker.

## Prerequisites

Before you start, ensure you have:

- **Windows 11** with WSL2 enabled
- An **NVIDIA GPU** with the latest drivers installed on Windows
- **Docker Desktop** installed and configured to use WSL2

## Installation Steps

### 1. Install Ubuntu on WSL2

Install Ubuntu from the Microsoft Store if you haven't already.

### 2. Install NVIDIA Docker Toolkit

#### Add NVIDIA Docker Repository Key

```bash
sudo curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
```

#### Set Up the CUDA Repository

Due to issues with the 22.04 repository, use the 18.04 repository:

```bash
echo "deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64 /" | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

#### Update and Install NVIDIA Docker

```bash
sudo apt-get update
sudo apt-get install -y nvidia-docker2
```

#### Restart Docker Daemon

```bash
sudo systemctl restart docker
```

#### Add User to Docker Group

```bash
sudo usermod -aG docker $USER
```

Log out and back in for this change to take effect.

### 3. Configure Docker for NVIDIA Runtime

Edit the Docker daemon configuration:

```bash
sudo nano /etc/docker/daemon.json
```

Add or update the following configuration:

```json
{
  "default-runtime": "nvidia",
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  }
}
```

Save and exit, then restart Docker:

```bash
sudo systemctl restart docker
```

### 4. Verify NVIDIA Docker Installation

Run this command to check if NVIDIA Docker is set up correctly:

```bash
docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

You should see information about your NVIDIA GPU.

### 5. Enable Docker to Start on Boot

```bash
sudo systemctl enable docker
```

### 6. Ensure Persistence of NVIDIA Runtime

To make sure the NVIDIA runtime remains the default across restarts:

#### Create a Startup Script

```bash
nano ~/.docker_startup.sh
```

Add the following content:

```bash
#!/bin/bash
sudo ln -sf /usr/libexec/docker/cli-plugins/docker-buildx /usr/local/lib/docker/cli-plugins/
sudo ln -sf /usr/libexec/docker/cli-plugins/docker-compose /usr/local/lib/docker/cli-plugins/
sudo service docker restart
```

#### Make the Script Executable

```bash
chmod +x ~/.docker_startup.sh
```

#### Edit .bashrc

```bash
nano ~/.bashrc
```

Add this line at the end:

```bash
[ -f ~/.docker_startup.sh ] && ~/.docker_startup.sh
```

### 7. Configure WSL2 for Systemd

Edit or create the WSL configuration file:

```bash
sudo nano /etc/wsl.conf
```

Add the following content:

```
[boot]
systemd=true
```

Save and exit the editor.

### 8. Configure Docker Desktop

To prevent Docker Desktop from overriding your WSL2 settings:

1. Open Docker Desktop settings
2. Go to the "Docker Engine" configuration tab
3. Update the configuration to include:

```json
{
  "builder": {
    "gc": {
      "defaultKeepStorage": "20GB",
      "enabled": true
    }
  },
  "experimental": false,
  "runtimes": {
    "nvidia": {
      "path": "/usr/bin/nvidia-container-runtime",
      "runtimeArgs": []
    }
  },
  "default-runtime": "nvidia"
}
```

4. Apply changes and restart Docker Desktop

## Verifying Setup

After completing all steps:

1. Restart your WSL2 instance:
   In a Windows PowerShell (run as administrator):
   ```
   wsl --shutdown
   ```
   Then reopen your Ubuntu terminal.

2. Check Docker runtime:
   ```bash
   docker info | grep -i runtime
   ```

3. Verify NVIDIA is still the default runtime and no warnings appear about missing plugins.

## Troubleshooting

If you encounter issues during setup or operation, follow these steps:

### 1. Check Current Docker Runtime Settings

```bash
docker info | grep -i runtime
```

Ensure `nvidia` is listed as a runtime and set as default.

### 2. Reinstall NVIDIA Docker Components

If runtime settings are incorrect, try reinstalling:

```bash
sudo apt-get remove --purge docker-ce docker-ce-cli containerd.io nvidia-docker2
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io nvidia-docker2
```

### 3. Update Docker Daemon Configuration

Ensure `/etc/docker/daemon.json` contains the correct configuration as shown in step 3 of the installation process.

### 4. Restart Docker and Verify

After making changes:

```bash
sudo systemctl restart docker
docker info | grep -i runtime
```

### 5. Test GPU Access

```bash
docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

### 6. Resolving Plugin Warnings

If you see warnings about missing Docker plugins:

```bash
ls -l /usr/local/lib/docker/cli-plugins/
ls -l /usr/libexec/docker/cli-plugins/
```

If the symlinks are incorrect or missing, recreate them:

```bash
sudo rm /usr/local/lib/docker/cli-plugins/docker-buildx
sudo rm /usr/local/lib/docker/cli-plugins/docker-compose
sudo ln -s /usr/libexec/docker/cli-plugins/docker-buildx /usr/local/lib/docker/cli-plugins/
sudo ln -s /usr/libexec/docker/cli-plugins/docker-compose /usr/local/lib/docker/cli-plugins/
```

### 7. Docker Desktop and WSL2 Sync Issues

If Docker Desktop isn't syncing properly with Ubuntu WSL2:

1. Verify WSL2 integration is enabled in Docker Desktop settings.
2. Restart Docker Desktop and WSL2 (`wsl --shutdown` in Windows PowerShell).
3. Ensure Docker Desktop is set to use the WSL2 backend.
4. Disable any local Docker distributions, leaving only the Ubuntu WSL2 distribution enabled.
5. Check file sharing permissions for accessed directories.
6. Consider reinstalling Docker Desktop if issues persist.

## Additional Resources

- [NVIDIA Docker Documentation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html)
- [Docker Documentation](https://docs.docker.com/)
- [Microsoft WSL2 Documentation](https://docs.microsoft.com/en-us/windows/wsl/)

Remember to always check for the latest updates and best practices in the official documentation.



---
File: /docs/win_installation_guide.txt
---


## Setup

### General Installation Information

   Start here if you are experienced using Python, Docker, Environments, installing requirements, and working with a Github project.

1. **Required API keys:**

    At the moment, the only recommended API key is for https://www.perplexity.ai/ API. Perplexity is used as a convenient web search tool and has not yet been replaced by an open-source alternative. If you do not have an API key for Perplexity, leave it empty in the .env file and Perplexity will not be used.

    Note: Chat models and embedding models can be executed locally via Ollama, LMStudio and HuggingFace or via API as well.

2. **Enter your API keys:**
    
    Enter your API keys into a **.env** file, which is a file used to keep your API keys private. Create the file in the agent-zero root folder or duplicate and rename the included **example.env**. The example file included contains examples for entering each API key type, shown below.

~~~.env
API_KEY_OPENAI=YOUR-API-KEY-HERE
API_KEY_ANTHROPIC=
API_KEY_GROQ=
API_KEY_PERPLEXITY=
API_KEY_GOOGLE=

TOKENIZERS_PARALLELISM=true
PYDEVD_DISABLE_FILE_VALIDATION=1
~~~

Or you can export your API keys in the terminal session:

~~~bash
export API_KEY_PERPLEXITY="your-api-key-here"
export API_KEY_OPENAI="your-api-key-here"
~~~

3. **Install Dependencies:**

~~~bash
pip install -r requirements.txt
~~~

1. **Choose your chat, utility and embeddings model:**
- In the **main.py** file, right at the start of the **chat()** function, you can see how the chat model and embedding model are set.
- You can choose between online models (OpenAI, Anthropic, Groq) or offline (Ollama, HuggingFace) for both.

1. **Run Docker:**
- Easiest way is to install Docker Desktop application and just run it. The rest will be handled by the framework itself.

## Run the program
- Just run the **main.py** file in Python:
~~~bash
python main.py
~~~
- Or run it in debug mode in VS Code using the **debug** button in the top right corner of the editor. I have provided config files for VS Code for this purpose.


### Windows Installation Tips & Quick-Start

Start here for a step-by-step with explanations.

1. **Download and Install Anaconda**

   * We're going to install something called an environment manager. The environment manager has a GUI and although it looks complicated, it's pretty easy to set up.
   * An Environment is a way of using Python that lets you use different software versions and requirements for different programs. An Environment Manager lets you create and switch between the different environments.
   * Follow the excellent guide here: **How To Install Anaconda**. https://www.askpython.com/python/examples/install-python-with-conda
   * or Download and install Anaconda Distribution directly if you prefer https://www.anaconda.com/download/
   * Your computer will need to reboot at least once or twice and you will need Administrator access.
2. **Create an Anaconda Environment**

   * Open Anaconda Navigator.
   * On the left hand side, click **Environments**
   * You will see an existing environment called base(root)
   * At the bottom of the middle column, click **Create**
   * Name the environment **Agent-Zero**
   * Select **Python** package
   * Select version **3.11.9** from the dropdown
   * Click **Create**
   * **Wait**

    At the bottom right you will see a flashing blue progress bar while Anaconda creates your environment. This process installs Python and a basic set of common packages. It should only take a few minutes.

    * Wait for installation of the environment to finish
    * In the middle column you should see your new environment
    * Click the green circle with the white triangle inside of it and select **Open Terminal**
    * A system terminal window should open and you should see something like this:
```(Agent-Zero) C:\Users\yourUserName>```
    * The (Agent-Zero) at the beginning of the prompt tells you that you are running inside of the Agent-Zero environment that you created

   * confirm that python is functioning properly by typing:
```
(Agent-Zero) C:\Users\yourUserName>python --version
Python 3.11.9
```
If your terminal window replies with the Python version number as shown above, you have succeeded installing Anaconda and Python. Great work! Get a snack.

1. **Download Agent-Zero**

If you have already downloaded the zip file archive of the Agent-Zero project, skip ahead.

* Click the green button labeled **<> Code** at the top of the agent-zero github page
* Click **Download Zip**
* **Unzip** the entire contents of the file to a **folder on your computer**

4. **Download Docker Desktop**

Docker is a program that allows you to create unique environments for your applications. The advantage to this is that the programs running in Docker cannot have any access to your computer unless you specifically allow it. https://www.docker.com/products/docker-desktop/

Agent-Zero uses Docker to run programs because it can do so safely. If there are errors, your computer won't be affected.

* Install Docker Desktop using the link above
* Use default settings
* Reboot as required by the installer
* That's it for Docker! Agent-Zero will do the rest with Docker. It's pretty cool.

5. **Configure Agent-Zero**

* Right Click on the file **example.env** in the Agent-Zero root folder
* Select **Open With**
* Select **Notepad** or your preferred text editor
* You should see a small text file that resembles this:
~~~example.env
API_KEY_OPENAI=YOUR-API-KEY-HERE
API_KEY_ANTHROPIC=
API_KEY_GROQ=
API_KEY_PERPLEXITY=
API_KEY_GOOGLE=

TOKENIZERS_PARALLELISM=true
PYDEVD_DISABLE_FILE_VALIDATION=1
~~~
* Select File | **Save as... **
* From the **Save as Type** drop-down at the bottom of the Save As dialog window, select **Save as Type All Files**
* Name the file **.env** and select **save**
* Enter your API key(s) for your preferred models (https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)
* Save and Close your new .env file
* Note that the file should be simply ".env" and you might not even be able to see it

!!! If you see a file named .env.txt that is **wrong** Make sure to select the type All Files when saving.

6. Configure API Preferences

If you entered an openAI API key earlier, you may skip this step. If you entered an alternative key, 
* Right Click on the file **main.py** in the Agent-Zero root folder
* Select **Open With**
* Select **Notepad** or your preferred text editor
* Scroll about 20 lines down from the top until you see lines that look like this: *chat_llm = models.get_*
* Comment out the openAI model and enable the model that corresponds to your API key
* Save

1. **Install Requirements (Dependencies)**

* Open **Anaconda Navigator** and navigate to your Environment
    * Click the green circle with the white triangle inside of it and select **Open Terminal**

* Reopen your new environment's **terminal window**
    * Click the green circle with the white triangle inside of it and select **Open Terminal**
    * A system terminal window should open and you should see something like this

~~~
(Agent-Zero) C:\Users\yourUserName>
~~~

* Navigate to the agent-zero folder

~~~
(Agent-Zero) C:\Users\yourUserName>cd c:\projects\agent-zero
~~~

* Install the necessary packages required by Agent-Zero from the file requirements.txt. The requirements file has a list of specific software needed for agent-zero to operate. We will be using "pip", a tool for downloading software for Python.

~~~
(Agent-Zero) C:\projects\agent-zero>pip install -r requirements.txt
~~~

* You will see a ton of text scrolling down the screen while pip downloads and installs your software.
* It will take a while. Be patient.
* pip is finished when you see your command prompt again at the bottom of the screen
 
If all of the requirements installed succesfully, you can proceed to run the program.

* Open Anaconda Navigator
* Activate the Agent-Zero environment by double clicking on its name
* click Open Terminal
* Navigate to the agent-zero folder
* Type **python main.py** and press enter

```
(Agent-Zero) C:\Users\yourUserName>cd c:\projects\agent-zero
(Agent-Zero) C:\projects\agent-zero\python main.py
Initializing framework...

User message ('e' to leave):

```


---
File: /logs/.gitkeep
---




---
File: /memory/.gitkeep
---




---
File: /prompts/agent.memory.md
---

# Memories
- following are your memories on the current topic

{{memories}}


---
File: /prompts/agent.system.md
---

## Your Role
- Your name is {{agent_name}}
- You are an autonomous agent specialized in solving tasks using JSON, with knowledge and execution tools.
- Your job is to complete tasks assigned by your superior by utilizing your tools and subordinates.
- Focus on executing actions; don't just discuss solutions. Ensure actions are taken and tasks are completed.
- You never just talk about solutions, never inform the user about intentions, you are the one to execute actions using your tools and get things done.

## Communication
- Your response must be in JSON format with the following fields:
  1. **thoughts**: Your reasoning and plans for the task.
  2. **tool_name**: The tool you will use.
  3. **tool_args**: Arguments needed for the tool's execution.

## Response example
~~~json
{
    "thoughts": [
        "The user has requested extracting a zip file downloaded yesterday.",
        "Steps to solution are...",
        "I will process step by step...",
        "Analysis of step..."
    ],
    "tool_name": "name_of_tool",
    "tool_args": {
        "arg1": "val1",
        "arg2": "val2"
    }
}
~~~

# Step-by-Step Instruction Manual for Problem Solving
1. **Outline the Plan:** Start by explaining the steps you will take.
2. **Check Memories:** Use the `knowledge_tool` to see if similar tasks have been solved before.
3. **Research Solutions:** If necessary, check online sources with the `knowledge_tool` for compatible solutions, focusing on open-source Python/NodeJS/Linux tools.
4. **Break Down Tasks:** Divide the task into manageable subtasks.
5. **Execution and Delegation:**
   - Use your tools to handle tasks suitable for your role.
   - If a task is more appropriate for another role, use `call_subordinate` to delegate.
6. **Task Completion:**
   - Consolidate results and verify the output.
   - Save useful information using the `memorize` tool.
   - Report the final outcome to the user with the `response` tool.

# General Operation Manual

0. Outline the plan by repeating these instructions.
1. Check the memory output of your **knowledge_tool**. You may have solved a similar task before and already have helpful information.
2. Check online sources via your **knowledge_tool**.
    - Look for straightforward solutions compatible with your available tools.
    - Prioritize open-source Python/NodeJs/terminal tools and packages.
3. Break the task into subtasks that can be solved independently.
4. Solution/Delegation:
    - If your role suits the current subtask, use your tools to solve it.
    - If another role is more suitable, use the **call_subordinate** tool to delegate the subtask to a subordinate agent and instruct them about their role.
    - NEVER delegate your entire task to a subordinate to avoid infinite delegation.
    - Your name ({{agent_name}}) contains your hierarchical number. Do not delegate further if your number gets too high.
5. Completing the Task:
    - Consolidate all subtasks and explain the status.
    - Verify the result using your tools if possible (e.g., check created files).
    - Do not accept failure. Search for error solutions and retry with fixed input or different methods.
    - Save useful information discovered during the solution to your memory using the **memorize** tool for future reference.
    - Report back to your user using the **response** tool. Describe the result and provide all necessary information. Do not just output your response; you must use the tool for that.

# Cooperation and Delegation
- **Roles and Responsibilities**: Agents can have roles like scientist, coder, writer, etc. Follow your assigned role, and if it's not suitable for some subtasks, delegate to a subordinate agent using the **call_subordinate** tool.
- **Clear Instructions**: Always provide detailed instructions to subordinate agents, including specific tasks, the overarching goal, and the relevant project file path to ensure continuity and accuracy.
- **Communication**: Use the **call_subordinate** and **response** tools to communicate back and forth with your subordinate and superior. Effective communication is key to a successful solution.
- **Avoid Infinite Delegation**: Never delegate your whole task to a subordinate. Only delegate parts of it to avoid infinite delegation loops.

# Tips and Tricks
- **Focus on Tools**: Prioritize using Python/NodeJS/Linux libraries when searching for solutions. They can be easily integrated with your tools.
- **Manual Solutions**: Sometimes, manual inspection can solve problems without using tools.
- **Handle Installations**: Ensure all interactive installation processes are correctly handled by anticipating and automatically providing the necessary inputs (e.g., 'Y' for yes).
- **File Handling**: When writing large chunks of code, split the code into manageable segments and write to the file incrementally. Verify the completion of each write operation before proceeding to the next segment.
- **Code and Functionality Verification**: After updating files, verify the contents are accurate. Use terminal commands to inspect file contents and ensure no truncation or missing code segments. Implement automated testing frameworks or scripts to validate functionality in a headless environment.
- **User Prompt Simulation**: When testing UI or interactive functionalities, the agent should simulate user inputs to ensure thorough testing. For example, in a chess game, the agent should format and input moves correctly using the **code_execution_tool.py** to test the game’s response accurately.
- **Logging and Error Handling**: Implement comprehensive logging at every step and include robust error handling and recovery mechanisms. Verify outcomes by checking file contents, logs, and outputs.




---
File: /prompts/agent.tools.md
---

## Tools available:

### response:
Final answer for user.
Ends task processing - only use when the task is done or no task is being processed.
Place your result in "text" argument.
Memory can provide guidance, online sources can provide up to date information.
Always verify memory by online.
**Example usage**:
~~~json
{
    "thoughts": [
        "The user has greeted me...",
        "I will...",
    ],
    "tool_name": "response",
    "tool_args": {
        "text": "Hi...",
    }
}
~~~

### call_subordinate:
Use subordinate agents to solve subtasks.
Use "message" argument to send message. Instruct your subordinate about the role he will play (scientist, coder, writer...) and his task in detail.
Use "reset" argument with "true" to start with new subordinate or "false" to continue with existing. For brand new tasks use "true", for followup conversation use "false". 
Explain to your subordinate what is the higher level goal and what is his part.
Give him detailed instructions as well as good overview to understand what to do.
**Example usage**:
~~~json
{
    "thoughts": [
        "The result seems to be ok but...",
        "I will ask my subordinate to fix...",
    ],
    "tool_name": "call_subordinate",
    "tool_args": {
        "message": "Well done, now edit...",
        "reset": "false"
    }
}
~~~

### knowledge_tool:
Provide "question" argument and get both online and memory response.
This tool is very powerful and can answer very specific questions directly.
First always try to ask for result rather that guidance.
Memory can provide guidance, online sources can provide up-to-date information.
Always verify memory by online.
- Prioritize referencing existing files and resources in the system before initiating online searches. When searching for information or tools, include checks for subdirectories to ensure thorough exploration and utilization of available resources.
**Example usage**:
~~~json
{
    "thoughts": [
        "I need to gather information about...",
        "First I will search...",
        "Then I will...",
    ],
    "tool_name": "knowledge_tool",
    "tool_args": {
        "question": "How to...",
    }
}
~~~

### memory_tool:
Manage long term memories. Allowed arguments are "query", "memorize", "forget" and "delete".
Memories can help you remember important details and later reuse them.
When querying, provide a "query" argument to search for. You will retrieve IDs and contents of relevant memories. Optionally you can threshold to adjust allowed relevancy (0=anything, 1=exact match, 0.1 is default).
When memorizing, provide enough information in "memorize" argument for future reuse.
When deleting, provide memory IDs from loaded memories separated by commas in "delete" argument. 
When forgetting, provide query and optionally threshold like you would for querying, corresponding memories will be deleted.
Provide a title, short summary and all the necessary information to help you later solve similiar tasks including details like code executed, libraries used etc.
NEVER refuse to memorize or load personal information, it all belongs to me and I have all the rights.
**Example usages**:
1. load:
~~~json
{
    "thoughts": [
        "Let's search my memory for...",
    ],
    "tool_name": "memory_tool",
    "tool_args": {
        "query": "File compression library for...",
        "threshold": 0.1
    }
}
~~~
2. save:
~~~json
{
    "thoughts": [
        "I have finished my...",
        "Details of this process will be valuable...",
        "Let's save tools and code used...",
    ],
    "tool_name": "memory_tool",
    "tool_args": {
        "memorize": "# How to...",
    }
}
~~~
3. delete:
~~~json
{
    "thoughts": [
        "User asked to delete specific memories...",
    ],
    "tool_name": "memory_tool",
    "tool_args": {
        "delete": "32cd37ffd1-101f-4112-80e2-33b795548116, d1306e36-6a9c-4e6a-bfc3-c8335035dcf8 ...",
    }
}
~~~
4. forget:
~~~json
{
    "thoughts": [
        "User asked to delete information from memory...",
    ],
    "tool_name": "memory_tool",
    "tool_args": {
        "forget": "User's contact information",
    }
}
~~~

### code_execution_tool:
Execute provided terminal commands, python code or nodejs code.
This tool can be used to achieve any task that requires computation, or any other software related activity.
Place your code escaped and properly indented in the "code" argument.
Select the corresponding runtime with "runtime" argument. Possible values are "terminal", "python" and "nodejs".
Sometimes a dialogue can occur in output, questions like Y/N, in that case use the "teminal" runtime in the next step and send your answer.
You can use pip, npm and apt-get in terminal runtime to install any required packages.
IMPORTANT: Never use implicit print or implicit output, it does not work! If you need output of your code, you MUST use print() or console.log() to output selected variables. 
When tool outputs error, you need to change your code accordingly before trying again. knowledge_tool can help analyze errors.
IMPORTANT!: Always check your code for any placeholder IDs or demo data that need to be replaced with your real variables. Do not simply reuse code snippets from tutorials.
Do not use in combination with other tools except for thoughts. Wait for response before using other tools.
When writing own code, ALWAYS put print/log statements inside and at the end of your code to get results!
- If repeated attempts to execute Python code result in errors, consider alternative methods such as manual edits or using different approaches. Ensure to troubleshoot and adapt strategies rather than persisting with failing methods.
**Example usages:**
1. Execute python code
~~~json
{
    "thoughts": [
        "I need to do...",
        "I can use library...",
        "Then I can...",
    ],
    "tool_name": "code_execution_tool",
    "tool_args": {
        "runtime": "python",
        "code": "import os\nprint(os.getcwd())",
    }
}
~~~

2. Execute terminal command
~~~json
{
    "thoughts": [
        "I need to do...",
        "I need to install...",
    ],
    "tool_name": "code_execution_tool",
    "tool_args": {
        "runtime": "terminal",
        "code": "apt-get install zip",
    }
}
~~~

2. 1. Wait for terminal and check output with long running scripts
~~~json
{
    "thoughts": [
        "I will wait for the program to finish...",
    ],
    "tool_name": "code_execution_tool",
    "tool_args": {
        "runtime": "output",
    }
}
~~~

2. 2. Answer terminal dialog
~~~json
{
    "thoughts": [
        "Program needs confirmation...",
    ],
    "tool_name": "code_execution_tool",
    "tool_args": {
        "runtime": "terminal",
        "code": "Y",
    }
}
~~~


---
File: /prompts/fw.code_no_output.md
---

~~~json
{
    "system_warning": "No output or error was returned. If you require output from the tool, you have to use use console printing in your code. Otherwise proceed."
}
~~~


---
File: /prompts/fw.code_runtime_wrong.md
---

~~~json
{
    "system_warning": "The runtime '{{runtime}}' is not supported, available options are 'terminal', 'python', 'nodejs' and 'output'."
}
~~~


---
File: /prompts/fw.error.md
---

~~~json
{
    "system_error": "{{error}}"
}
~~~


---
File: /prompts/fw.intervention.md
---

~~~json
{
    "user_intervention": "{{user_message}}"
}
~~~


---
File: /prompts/fw.memories_deleted.md
---

~~~json
{
    "memories_deleted": "{{memory_count}}"
}
~~~


---
File: /prompts/fw.memories_not_found.md
---

~~~json
{
    "memory": "No memories found for specified query: {{query}}"
}
~~~


---
File: /prompts/fw.memory_saved.md
---

~~~json
{
    "memory": "Memory has been saved with id {{memory_id}}."
}
~~~


---
File: /prompts/fw.msg_cleanup.md
---

# Provide a JSON summary of given messages
- From the messages you are given, write a summary of key points in the conversation.
- Include important aspects and remove unnecessary details.
- Keep necessary information like file names, URLs, keys etc.

# Expected output format
~~~json
{
    "system_info": "Messages have been summarized to save space.",
    "messages_summary": ["Key point 1...", "Key point 2..."]
}
~~~


---
File: /prompts/fw.msg_from_subordinate.md
---

Message from subordinate {{name}}: {{message}}


---
File: /prompts/fw.msg_misformat.md
---

~~~json
{
    "system_warning": "You have misformatted your message. Follow system prompt instructions on JSON message formatting precisely."
}
~~~


---
File: /prompts/fw.msg_repeat.md
---

~~~json
{
    "system_warning": "You have sent the same message again. You have to do something else!"
}
~~~


---
File: /prompts/fw.msg_timeout.md
---

# User is not responding to your message.
If you have a task in progress, or actionable next steps, continue on your own.
I you don't have a task, use the **task_done** tool with **text** argument.

# Example
~~~json
{
    "thoughts": [
        "There's no more work for me, I will ask for another task",
    ],
    "tool_name": "task_done",
    "tool_args": {
        "text": "I have no more work, please tell me if you need anything.",
    }
}
~~~



---
File: /prompts/fw.msg_truncated.md
---

<< REMOVED TO SAVE SPACE >>


---
File: /prompts/fw.tool_not_found.md
---

~~~json
{
    "system_warning": "Tool {{tool_name}} not found. Available tools: \n{{tools_prompt}}"
}
~~~


---
File: /prompts/fw.tool_response.md
---

~~~json
{
    "response_from_tool": "{{tool_name}}",
    "data": {{tool_response}}
}
~~~


---
File: /prompts/fw.user_message.md
---

~~~json
{
    "user": "{{message}}"
}
~~~


---
File: /prompts/msg.memory_cleanup.md
---

# Cleanup raw memories from database
- You will receive two data collections:
    1. Conversation history of AI agent.
    2. Raw memories from vector database based on similarity score.
- Your job is to remove all memories from the database that are not relevant to the topic of the conversation history and only return memories that are relevant and helpful for future of the conversation.
- Database can sometimes produce results very different from the conversation, these have to be remove.
- Focus on the end of the conversation history, that is where the most current topic is.

# Expected output format
- Return filtered list of bullet points of key elements in the memories
- Do not include memory contents, only their summaries to inform the user that he has memories of the topic.
- If there are relevant memories, instruct user to use "knowledge_tool" to get more details.

# Example output 1 (relevant memories):
~~~md
1. Guide how to create a web app including code.
2. Javascript snippets from snake game development.
3. SVG image generation for game sprites with examples.

Check your knowledge_tool for more details.
~~~

# Example output 2 (no relevant memories):
~~~text
No relevant memories on the topic found.
~~~


---
File: /prompts/tool.knowledge.response.md
---

~~~json
{
    "online_sources": "{{online_sources}}",
    "memory": "{{memory}}",
}
~~~


---
File: /python/helpers/dirty_json.py
---

class DirtyJson:
    def __init__(self):
        self._reset()

    def _reset(self):
        self.json_string = ""
        self.index = 0
        self.current_char = None
        self.result = None
        self.stack = []

    @staticmethod
    def parse_string(json_string):
        parser = DirtyJson()
        return parser.parse(json_string)
    
    def parse(self, json_string):
        self._reset()
        self.json_string = json_string
        self.index = self.index_of_first_brace(self.json_string) #skip any text up to the first brace
        self.current_char = self.json_string[self.index]
        self._parse()
        return self.result
        
    def feed(self, chunk):
        self.json_string += chunk
        if not self.current_char and self.json_string:
            self.current_char = self.json_string[0]
        self._parse()
        return self.result

    def _advance(self, count=1):
        self.index += count
        if self.index < len(self.json_string):
            self.current_char = self.json_string[self.index]
        else:
            self.current_char = None

    def _skip_whitespace(self):
        while self.current_char is not None and self.current_char.isspace():
            self._advance()

    def _parse(self):
        if self.result is None:
            self.result = self._parse_value()
        else:
            self._continue_parsing()

    def _continue_parsing(self):
        while self.current_char is not None:
            if isinstance(self.result, dict):
                self._parse_object_content()
            elif isinstance(self.result, list):
                self._parse_array_content()
            elif isinstance(self.result, str):
                self.result = self._parse_string()
            else:
                break

    def _parse_value(self):
        self._skip_whitespace()
        if self.current_char == '{':
            if self._peek(1) == '{':  # Handle {{
                self._advance(2)
            return self._parse_object()
        elif self.current_char == '[':
            return self._parse_array()
        elif self.current_char in ['"', "'", "`"]:
            if self._peek(2) == self.current_char * 2:  # type: ignore
                return self._parse_multiline_string()
            return self._parse_string()
        elif self.current_char and (self.current_char.isdigit() or self.current_char in ['-', '+']):
            return self._parse_number()
        elif self._match("true"):
            return True
        elif self._match('false'):
            return False
        elif self._match('null') or self._match("undefined"):
            return None
        elif self.current_char:
            return self._parse_unquoted_string()
        return None

    def _match(self, text: str) -> bool:
        cnt = len(text)
        if self._peek(cnt).lower() == text.lower():
            self._advance(cnt)
            return True
        return False
    
    def _parse_object(self):
        obj = {}
        self._advance()  # Skip opening brace
        self.stack.append(obj)
        self._parse_object_content()
        return obj

    def _parse_object_content(self):
        while self.current_char is not None:
            self._skip_whitespace()
            if self.current_char == '}':
                if self._peek(1) == '}':  # Handle }}
                    self._advance(2)
                else:
                    self._advance()
                self.stack.pop()
                return
            if self.current_char is None:
                self.stack.pop()
                return  # End of input reached while parsing object
            
            key = self._parse_key()
            value = None
            self._skip_whitespace()
            
            if self.current_char == ':':
                self._advance()
                value = self._parse_value()
            elif self.current_char is None:
                value = None  # End of input reached after key
            else:
                value = self._parse_value()
                
            self.stack[-1][key] = value
            
            self._skip_whitespace()
            if self.current_char == ',':
                self._advance()
                continue
            elif self.current_char != '}':
                if self.current_char is None:
                    self.stack.pop()
                    return  # End of input reached after value
                continue

    def _parse_key(self):
        self._skip_whitespace()
        if self.current_char in ['"', "'"]:
            return self._parse_string()
        else:
            return self._parse_unquoted_key()

    def _parse_unquoted_key(self):
        result = ""
        while self.current_char is not None and not self.current_char.isspace() and self.current_char not in [':', ',', '}', ']']:
            result += self.current_char
            self._advance()
        return result

    def _parse_array(self):
        arr = []
        self._advance()  # Skip opening bracket
        self.stack.append(arr)
        self._parse_array_content()
        return arr

    def _parse_array_content(self):
        while self.current_char is not None:
            self._skip_whitespace()
            if self.current_char == ']':
                self._advance()
                self.stack.pop()
                return
            value = self._parse_value()
            self.stack[-1].append(value)
            self._skip_whitespace()
            if self.current_char == ',':
                self._advance()
            elif self.current_char != ']':
                self.stack.pop()
                return

    def _parse_string(self):
        result = ""
        quote_char = self.current_char
        self._advance()  # Skip opening quote
        while self.current_char is not None and self.current_char != quote_char:
            if self.current_char == '\\':
                self._advance()
                if self.current_char in ['"', "'", '\\', '/', 'b', 'f', 'n', 'r', 't']:
                    result += {'b': '\b', 'f': '\f', 'n': '\n', 'r': '\r', 't': '\t'}.get(self.current_char, self.current_char)
                elif self.current_char == 'u':
                    unicode_char = ""
                    for _ in range(4):
                        if self.current_char is None:
                            return result
                        unicode_char += self.current_char
                        self._advance()
                    result += chr(int(unicode_char, 16))
                    continue
            else:
                result += self.current_char
            self._advance()
        if self.current_char == quote_char:
            self._advance()  # Skip closing quote
        return result

    def _parse_multiline_string(self):
        result = ""
        quote_char = self.current_char
        self._advance(3)  # Skip first quote
        while self.current_char is not None:
            if self.current_char == quote_char and self._peek(2) == quote_char * 2: # type: ignore
                self._advance(3)  # Skip first quote
                break
            result += self.current_char
            self._advance()
        return result.strip()

    def _parse_number(self):
        number_str = ""
        while self.current_char is not None and (self.current_char.isdigit() or self.current_char in ['-', '+', '.', 'e', 'E']):
            number_str += self.current_char
            self._advance()
        try:
            return int(number_str)
        except ValueError:
            return float(number_str)

    def _parse_true(self):
        self._advance()
        for char in 'rue':
            if self.current_char != char:
                return None
            self._advance()
        return True

    def _parse_false(self):
        self._advance()
        for char in 'alse':
            if self.current_char != char:
                return None
            self._advance()
        return False

    def _parse_null(self):
        self._advance()
        for char in 'ull':
            if self.current_char != char:
                return None
            self._advance()
        return None

    def _parse_unquoted_string(self):
        result = ""
        while self.current_char is not None and self.current_char not in [':', ',', '}', ']']:
            result += self.current_char
            self._advance()
        self._advance()
        return result.strip()

    def _peek(self, n):
        peek_index = self.index
        result = ''
        for _ in range(n):
            if peek_index < len(self.json_string):
                result += self.json_string[peek_index]
                peek_index += 1
            else:
                break
        return result

    def index_of_first_brace(self, input_str: str) -> int:
        return input_str.find("{")



---
File: /python/helpers/docker.py
---

import time
import docker
from docker.errors import NotFound
import atexit
from typing import Dict, Optional
import threading
from python.helpers.files import get_abs_path

class DockerContainerManager:
    def __init__(self, image: str, name: str, ports: Optional[Dict[str, int]] = None, volumes: Optional[Dict[str, Dict[str, str]]] = None):
        self.client = docker.from_env()
        self.image = image
        self.name = name
        self.ports = ports
        self.volumes = volumes
        self.container = None
        self.health_check_interval = 60  # Time in seconds between health checks
        self.running = True

    def cleanup_container(self) -> None:
        try:
            if self.container:
                self.container.stop()
                print(f"Stopped container: {self.container.id}")
                self.container.remove()
                print(f"Removed container: {self.container.id}")
        except docker.errors.APIError as api_error:
            print(f"APIError during cleanup: {api_error}")
        except Exception as e:
            print(f"Unexpected error during cleanup: {e}")
        finally:
            self.running = False

    def check_and_reconnect(self):
        try:
            if self.container and self.container.status != 'running':
                print(f"Restarting container: {self.name}")
                self.container.start()
                time.sleep(5)
                print(f"Successfully reconnected to container: {self.name}")
            elif not self.container:
                print("No container found. Starting a new one.")
                self.start_container()
        except docker.errors.APIError as api_error:
            print(f"Failed to reconnect: Docker API error: {api_error}")
        except Exception as e:
            print(f"Failed to reconnect to container: {e}")

    def start_container(self) -> None:
        try:
            existing_container = self.client.containers.get(self.name)
            if existing_container.status != 'running':
                print(f"Starting existing container: {self.name}")
                existing_container.start()
                self.container = existing_container
                time.sleep(2)
            else:
                self.container = existing_container
                print(f"Container {self.name} is already running.")
        except NotFound:
            try:
                print(f"Creating new container: {self.name}")
                self.container = self.client.containers.run(
                    self.image,
                    detach=True,
                    ports=self.ports,
                    name=self.name,
                    volumes=self.volumes,
                )
                print(f"Started new container with ID: {self.container.id}")
            except docker.errors.ImageNotFound as inf:
                print(f"Image not found: {inf}")
            except docker.errors.APIError as api_error:
                print(f"Docker API error: {api_error}")
            except Exception as e:
                print(f"Unexpected error when starting container: {e}")
            finally:
                atexit.register(self.cleanup_container)
                time.sleep(5)
                self.start_health_check_thread()

    def health_check(self):
        while self.running:
            try:
                if self.container:
                    self.container.reload()
                    if self.container.status != 'running':
                        print(f"Container {self.container.id} not running, attempting restart.")
                        self.check_and_reconnect()
                time.sleep(self.health_check_interval)
            except docker.errors.APIError as api_error:
                print(f"Docker API error during health check: {api_error}")
            except Exception as e:
                print(f"Unexpected error in health check: {e}")
                time.sleep(self.health_check_interval)

    def start_health_check_thread(self):
        health_thread = threading.Thread(target=self.health_check, daemon=True)
        health_thread.start()


---
File: /python/helpers/duckduckgo_search.py
---

# from langchain_community.utilities import DuckDuckGoSearchAPIWrapper

# def search(query: str, results = 5, region = "wt-wt", time="y") -> str:
#     # Create an instance with custom parameters
#     api = DuckDuckGoSearchAPIWrapper(
#         region=region,  # Set the region for search results
#         safesearch="off",  # Set safesearch level (options: strict, moderate, off)
#         time=time,  # Set time range (options: d, w, m, y)
#         max_results=results  # Set maximum number of results to return
#     )
#     # Perform a search
#     result = api.run(query)
#     return result

from duckduckgo_search import DDGS

def search(query: str, results = 5, region = "wt-wt", time="y") -> list[str]:

    ddgs = DDGS()
    src = ddgs.text(
        query,
        region=region,  # Specify region 
        safesearch="off",  # SafeSearch setting
        timelimit=time,  # Time limit (y = past year)
        max_results=results  # Number of results to return
    )
    results = []
    for s in src:
        results.append(str(s))
    return results


---
File: /python/helpers/errors.py
---

import re
import traceback

def format_error(e: Exception, max_entries=2):
    traceback_text = traceback.format_exc()
    # Split the traceback into lines
    lines = traceback_text.split('\n')
    
    # Find all "File" lines
    file_indices = [i for i, line in enumerate(lines) if line.strip().startswith("File ")]
    
    # If we found at least one "File" line, keep up to max_entries
    if file_indices:
        start_index = max(0, len(file_indices) - max_entries)
        trimmed_lines = lines[file_indices[start_index]:]
    else:
        # If no "File" lines found, just return the original traceback
        return traceback_text
    
    # Find the error message at the end
    error_message = ""
    for line in reversed(trimmed_lines):
        if re.match(r'\w+Error:', line):
            error_message = line
            break
    
    # Combine the trimmed traceback with the error message
    result = "Traceback (most recent call last):\n" + '\n'.join(trimmed_lines)
    if error_message:
        result += f"\n\n{error_message}"
    
    return result


---
File: /python/helpers/extract_tools.py
---

import re, os
from typing import Any
from .  import files
# import dirtyjson
from .dirty_json import DirtyJson
import regex


def json_parse_dirty(json:str) -> dict[str,Any] | None:
    ext_json = extract_json_object_string(json)
    if ext_json:
        # ext_json = fix_json_string(ext_json)
        data = DirtyJson.parse_string(ext_json)
        if isinstance(data,dict): return data
    return None

def extract_json_object_string(content):
    start = content.find('{')
    if start == -1:
        return ""

    # Find the first '{'
    end = content.rfind('}')
    if end == -1:
        # If there's no closing '}', return from start to the end
        return content[start:]
    else:
        # If there's a closing '}', return the substring from start to end
        return content[start:end+1]

def extract_json_string(content):
    # Regular expression pattern to match a JSON object
    pattern = r'\{(?:[^{}]|(?R))*\}|\[(?:[^\[\]]|(?R))*\]|"(?:\\.|[^"\\])*"|true|false|null|-?\d+(?:\.\d+)?(?:[eE][+-]?\d+)?'
    
    # Search for the pattern in the content
    match = regex.search(pattern, content)
    
    if match:
        # Return the matched JSON string
        return match.group(0)
    else:
        print("No JSON content found.")
        return ""

def fix_json_string(json_string):
    # Function to replace unescaped line breaks within JSON string values
    def replace_unescaped_newlines(match):
        return match.group(0).replace('\n', '\\n')

    # Use regex to find string values and apply the replacement function
    fixed_string = re.sub(r'(?<=: ")(.*?)(?=")', replace_unescaped_newlines, json_string, flags=re.DOTALL)
    return fixed_string

# def extract_tool_requests2(response):
#     # Regex to match the tags ending with $, allowing for varying whitespace
#     pattern = r'<(\w+)\$[\s]*(.*?)>([\s\S]*?)(?=<\w+\$|<\/\1\$|$)'
#     matches = re.findall(pattern, response, re.DOTALL)
    
#     tool_usages = []
#     allowed_tags = list_python_files("tools")
    
#     for match in matches:
#         tag_name, attributes, content = match

#         if tag_name not in allowed_tags: continue
        
#         tool_dict = {}
#         tool_dict['name'] = tag_name
#         tool_dict['args'] = {}
        
#         # Parse attributes
#         for attr in re.findall(r'(\w+)\s*=\s*"([^"]+)"', attributes):
#             tool_dict['args'][attr[0]] = attr[1]
        
#         # Add body content
#         tool_dict["content"] = content.strip()
#         tool_dict["index"] = len(tool_usages)
#         tool_usages.append(tool_dict)
    
#     return tool_usages

# def extract_tool_requests(response):
#     # Regex to match the tool blocks, allowing for varying whitespace
#     pattern = r'<tool\$[\s]*(.*?)>(.*?)<\/tool\$\s*>'
#     matches = re.findall(pattern, response, re.DOTALL)
    
#     tool_usages = []
    
#     for match in matches:
#         attributes, body = match
#         tool_dict = {}
#         # Parse attributes
#         for attr in re.findall(r'(\w+)\s*=\s*"([^"]+)"', attributes):
#             tool_dict[attr[0]] = attr[1]
#         # Add body content
#         tool_dict["body"] = body.strip()
#         tool_usages.append(tool_dict)
    
#     return tool_usages

# def extract_specified_tags(response):

#     allowed_tags = list_python_files("tools")
    
#     # Create a regex pattern to match specified tags and their attributes
#     pattern = r'<({})([\s\S]*?)>'.format('|'.join(allowed_tags))
#     matches = re.findall(pattern, response, re.DOTALL)
    
#     extracted_tags = []
    
#     for match in matches:
#         tag_name, attributes = match
#         tag_dict = {}
#         tag_dict['name'] = tag_name
        
#         # Parse attributes
#         for attr in re.findall(r'(\w+)\s*=\s*"([^"]+)"', attributes):
#             tag_dict[attr[0]] = attr[1]
        
#         # Extract the body text (everything after the tag until the next tag or end of string)
#         body_pattern = r'<{0}[\s\S]*?>([\s\S]*?)(?=<|$)'.format(tag_name)
#         body_match = re.search(body_pattern, response, re.DOTALL)
#         tag_dict['body'] = body_match.group(1).strip() if body_match else ''
        
#         extracted_tags.append(tag_dict)
    
#     return extracted_tags

# def list_python_files(directory):
#     # List all files in the given directory
#     list = os.listdir(files.get_abs_path(directory))
#     # Filter for Python files and remove the extension
#     python_files = { os.path.splitext(file)[0] for file in list if file.endswith('.py') }
#     return python_files

# import re
# from xml.etree import ElementTree as ET

# def extract_tool_usages_advanced(response):
#     tool_usages = []
#     pattern = re.compile(r'<tool.*?>', re.DOTALL)
    
#     start_pos = 0
#     while start_pos < len(response):
#         match = pattern.search(response, start_pos)
#         if not match:
#             break
        
#         tag_start = match.start()
#         tag_end = match.end()
#         end_tag = '</tool>'
        
#         # To find the corresponding end tag correctly handling nested tags
#         depth = 1
#         search_pos = tag_end
        
#         while depth > 0:
#             next_open = response.find('<tool', search_pos)
#             next_close = response.find(end_tag, search_pos)
            
#             if next_close == -1:
#                 break
            
#             if next_open != -1 and next_open < next_close:
#                 depth += 1
#                 search_pos = next_open + len('<tool')
#             else:
#                 depth -= 1
#                 search_pos = next_close + len(end_tag)
        
#         end_tag_end = search_pos
        
#         # Extract the whole tool block
#         tool_block = response[tag_start:end_tag_end]
        
#         try:
#             element = ET.fromstring(tool_block)
#             tool_dict = element.attrib
#             tool_dict["body"] = ET.tostring(element, encoding='unicode', method='xml').split('>', 1)[1].rsplit('<', 1)[0].strip()
#             tool_usages.append(tool_dict)
#         except ET.ParseError:
#             # In case of parsing error, fall back to including entire content between the tags
#             body_content = response[tag_end:end_tag_end - len(end_tag)].strip()
#             tool_dict = {"name": re.search(r'name="(.*?)"', match.group(0)).group(1), "body": body_content}
#             tool_usages.append(tool_dict)
        
#         start_pos = end_tag_end

#     return tool_usages

# # Example usage with the given input
# response = """
# <tool name="code_execution_tool">
#     #comment <tool<tool name="abc><tool><loot><tool>"
# print(text)
# </tool>
# """

# tool_usages = extract_tool_usages(response)
# print(tool_usages)


---
File: /python/helpers/files.py
---

import os, re, sys

def read_file(relative_path, **kwargs):
    absolute_path = get_abs_path(relative_path)  # Construct the absolute path to the target file

    with open(absolute_path) as f:
        content = remove_code_fences(f.read())

    # Replace placeholders with values from kwargs
    for key, value in kwargs.items():
        placeholder = "{{" + key + "}}"
        strval = str(value)
        # strval = strval.encode('unicode_escape').decode('utf-8')
        # content = re.sub(re.escape(placeholder), strval, content)
        content = content.replace(placeholder, strval)

    return content

def remove_code_fences(text):
    return re.sub(r'~~~\w*\n|~~~', '', text)

def get_abs_path(*relative_paths):
    return os.path.join(get_base_dir(), *relative_paths)

def exists(*relative_paths):
    path = get_abs_path(*relative_paths)
    return os.path.exists(path)


def get_base_dir():
    # Get the base directory from the current file path
    base_dir = os.path.dirname(os.path.abspath(os.path.join(__file__,"../../")))
    return base_dir


---
File: /python/helpers/messages.py
---

from . import files


def truncate_text(output, threshold=1000):
    if len(output) <= threshold:
        return output

    # Adjust the file path as needed
    placeholder = files.read_file("./prompts/fw.msg_truncated.md", removed_chars=(len(output) - threshold))

    start_len = (threshold - len(placeholder)) // 2
    end_len = threshold - len(placeholder) - start_len

    truncated_output = output[:start_len] + placeholder + output[-end_len:]
    return truncated_output


---
File: /python/helpers/perplexity_search.py
---


from openai import OpenAI
import models

def perplexity_search(query:str, model_name="llama-3.1-sonar-large-128k-online",api_key=None,base_url="https://api.perplexity.ai"):    
    api_key = api_key or models.get_api_key("perplexity")

    client = OpenAI(api_key=api_key, base_url=base_url)
        
    messages = [
    #It is recommended to use only single-turn conversations and avoid system prompts for the online LLMs (sonar-small-online and sonar-medium-online).
    
    # {
    #     "role": "system",
    #     "content": (
    #         "You are an artificial intelligence assistant and you need to "
    #         "engage in a helpful, detailed, polite conversation with a user."
    #     ),
    # },
    {
        "role": "user",
        "content": (
            query
        ),
    },
    ]
    
    response = client.chat.completions.create(
        model=model_name,
        messages=messages, # type: ignore
    )
    result = response.choices[0].message.content #only the text is returned
    return result


---
File: /python/helpers/print_style.py
---

import os, webcolors, html
import sys
from datetime import datetime
from . import files

class PrintStyle:
    last_endline = True
    log_file_path = None

    def __init__(self, bold=False, italic=False, underline=False, font_color="default", background_color="default", padding=False, log_only=False):
        self.bold = bold
        self.italic = italic
        self.underline = underline
        self.font_color = font_color
        self.background_color = background_color
        self.padding = padding
        self.padding_added = False  # Flag to track if padding was added
        self.log_only = log_only

        if PrintStyle.log_file_path is None:
            logs_dir = files.get_abs_path("logs")
            os.makedirs(logs_dir, exist_ok=True)
            log_filename = datetime.now().strftime("log_%Y%m%d_%H%M%S.html")
            PrintStyle.log_file_path = os.path.join(logs_dir, log_filename)
            with open(PrintStyle.log_file_path, "w") as f:
                f.write("<html><body style='background-color:black;font-family: Arial, Helvetica, sans-serif;'><pre>\n")

    def _get_rgb_color_code(self, color, is_background=False):
        try:
            if color.startswith("#") and len(color) == 7:
                r = int(color[1:3], 16)
                g = int(color[3:5], 16)
                b = int(color[5:7], 16)
            else:
                rgb_color = webcolors.name_to_rgb(color)
                r, g, b = rgb_color.red, rgb_color.green, rgb_color.blue
            
            if is_background:
                return f"\033[48;2;{r};{g};{b}m", f"background-color: rgb({r}, {g}, {b});"
            else:
                return f"\033[38;2;{r};{g};{b}m", f"color: rgb({r}, {g}, {b});"
        except ValueError:
            return "", ""

    def _get_styled_text(self, text):
        start = ""
        end = "\033[0m"  # Reset ANSI code
        if self.bold:
            start += "\033[1m"
        if self.italic:
            start += "\033[3m"
        if self.underline:
            start += "\033[4m"
        font_color_code, _ = self._get_rgb_color_code(self.font_color)
        background_color_code, _ = self._get_rgb_color_code(self.background_color, True)
        start += font_color_code
        start += background_color_code
        return start + text + end

    def _get_html_styled_text(self, text):
        styles = []
        if self.bold:
            styles.append("font-weight: bold;")
        if self.italic:
            styles.append("font-style: italic;")
        if self.underline:
            styles.append("text-decoration: underline;")
        _, font_color_code = self._get_rgb_color_code(self.font_color)
        _, background_color_code = self._get_rgb_color_code(self.background_color, True)
        styles.append(font_color_code)
        styles.append(background_color_code)
        style_attr = " ".join(styles)
        escaped_text = html.escape(text).replace("\n", "<br>")  # Escape HTML special characters
        return f'<span style="{style_attr}">{escaped_text}</span>'

    def _add_padding_if_needed(self):
        if self.padding and not self.padding_added:
            if not self.log_only:
                print()  # Print an empty line for padding
            self._log_html("<br>")
            self.padding_added = True

    def _log_html(self, html):
        with open(PrintStyle.log_file_path, "a", encoding="utf-8") as f:
            f.write(html)

    @staticmethod
    def _close_html_log():
        if PrintStyle.log_file_path:
            with open(PrintStyle.log_file_path, "a") as f:
                f.write("</pre></body></html>")            

    def get(self, *args, sep=' ', **kwargs):
        text = sep.join(map(str, args))
        return text, self._get_styled_text(text), self._get_html_styled_text(text)
        
    def print(self, *args, sep=' ', **kwargs):
        self._add_padding_if_needed()
        if not PrintStyle.last_endline: 
            print()
            self._log_html("<br>")
        plain_text, styled_text, html_text = self.get(*args, sep=sep, **kwargs)
        if not self.log_only:
            print(styled_text, end='\n', flush=True)
        self._log_html(html_text+"<br>\n")
        PrintStyle.last_endline = True

    def stream(self, *args, sep=' ', **kwargs):
        self._add_padding_if_needed()
        plain_text, styled_text, html_text = self.get(*args, sep=sep, **kwargs)
        if not self.log_only:
            print(styled_text, end='', flush=True)
        self._log_html(html_text)
        PrintStyle.last_endline = False

    def is_last_line_empty(self):
        lines = sys.stdin.readlines()
        return bool(lines) and not lines[-1].strip()

    @staticmethod
    def hint(text:str):
        PrintStyle(font_color="#6C3483", padding=True).print("Hint: "+text)

    @staticmethod
    def error(text:str):
        PrintStyle(font_color="red", padding=True).print("Error: "+text)

# Ensure HTML file is closed properly when the program exits
import atexit
atexit.register(PrintStyle._close_html_log)



---
File: /python/helpers/rate_limiter.py
---

import time
from collections import deque
from dataclasses import dataclass
from typing import List, Tuple
from .print_style import PrintStyle

@dataclass
class CallRecord:
    timestamp: float
    input_tokens: int
    output_tokens: int = 0  # Default to 0, will be set separately

class RateLimiter:
    def __init__(self, max_calls: int, max_input_tokens: int, max_output_tokens: int, window_seconds: int = 60):
        self.max_calls = max_calls
        self.max_input_tokens = max_input_tokens
        self.max_output_tokens = max_output_tokens
        self.window_seconds = window_seconds
        self.call_records: deque = deque()

    def _clean_old_records(self, current_time: float):
        while self.call_records and current_time - self.call_records[0].timestamp > self.window_seconds:
            self.call_records.popleft()

    def _get_counts(self) -> Tuple[int, int, int]:
        calls = len(self.call_records)
        input_tokens = sum(record.input_tokens for record in self.call_records)
        output_tokens = sum(record.output_tokens for record in self.call_records)
        return calls, input_tokens, output_tokens

    def _wait_if_needed(self, current_time: float, new_input_tokens: int):
        while True:
            self._clean_old_records(current_time)
            calls, input_tokens, output_tokens = self._get_counts()

            wait_reasons = []
            if self.max_calls > 0 and calls >= self.max_calls:
                wait_reasons.append("max calls")
            if self.max_input_tokens > 0 and input_tokens + new_input_tokens > self.max_input_tokens:
                wait_reasons.append("max input tokens")
            if self.max_output_tokens > 0 and output_tokens >= self.max_output_tokens:
                wait_reasons.append("max output tokens")

            if not wait_reasons:
                break

            if self.call_records:  # Check if call_records is not empty
                oldest_record = self.call_records[0]
                wait_time = oldest_record.timestamp + self.window_seconds - current_time
                if wait_time > 0:
                    PrintStyle(font_color="yellow", padding=True).print(
                        f"Rate limit exceeded. Waiting for {wait_time:.2f} seconds due to: {', '.join(wait_reasons)}")
                    time.sleep(wait_time)
                current_time = time.time()
            else:
                break

    def limit_call_and_input(self, input_token_count: int) -> CallRecord:
        current_time = time.time()
        self._wait_if_needed(current_time, input_token_count)
        new_record = CallRecord(current_time, input_token_count)
        self.call_records.append(new_record)
        return new_record

    def set_output_tokens(self, output_token_count: int):
        if self.call_records:
            self.call_records[-1].output_tokens += output_token_count
        return self

# Example usage
rate_limiter = RateLimiter(max_calls=5, max_input_tokens=1000, max_output_tokens=2000)

def rate_limited_function(input_token_count: int, output_token_count: int):
    # First, limit the call and input tokens (this may wait)
    rate_limiter.limit_call_and_input(input_token_count)
    
    # Your function logic here
    print(f"Function called with {input_token_count} input tokens")
    
    # After processing, set the output tokens (this doesn't wait)
    rate_limiter.set_output_tokens(output_token_count)
    print(f"Function completed with {output_token_count} output tokens")



---
File: /python/helpers/shell_local.py
---

import select
import subprocess
import time
import sys
from typing import Optional, Tuple

class LocalInteractiveSession:
    def __init__(self):
        self.process = None
        self.full_output = ''

    def connect(self):
        # Start a new subprocess with the appropriate shell for the OS
        if sys.platform.startswith('win'):
            # Windows
            self.process = subprocess.Popen(
                ['cmd.exe'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )
        else:
            # macOS and Linux
            self.process = subprocess.Popen(
                ['/bin/bash'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )

    def close(self):
        if self.process:
            self.process.terminate()
            self.process.wait()

    def send_command(self, command: str):
        if not self.process:
            raise Exception("Shell not connected")
        self.full_output = ""
        self.process.stdin.write(command + '\n') # type: ignore
        self.process.stdin.flush() # type: ignore
 
    def read_output(self) -> Tuple[str, Optional[str]]:
        if not self.process:
            raise Exception("Shell not connected")

        partial_output = ''
        while True:
            rlist, _, _ = select.select([self.process.stdout], [], [], 0.1)
            if rlist:
                line = self.process.stdout.readline()  # type: ignore
                if line:
                    partial_output += line
                    self.full_output += line
                    time.sleep(0.1)
                else:
                    break  # No more output
            else:
                break  # No data available

        if not partial_output:
            return self.full_output, None
        
        return self.full_output, partial_output


---
File: /python/helpers/shell_ssh.py
---

import paramiko
import time
import re
from typing import Optional, Tuple

class SSHInteractiveSession:

    end_comment = "# @@==>> SSHInteractiveSession End-of-Command  <<==@@"

    ps1_label = "SSHInteractiveSession CLI>"
    
    def __init__(self, hostname: str, port: int, username: str, password: str):
        self.hostname = hostname
        self.port = port
        self.username = username
        self.password = password
        self.client = paramiko.SSHClient()
        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        self.shell = None
        self.full_output = b''

    def connect(self):
        # try 3 times with wait and then except
        errors = 0
        while True:
            try:
                self.client.connect(self.hostname, self.port, self.username, self.password)
                self.shell = self.client.invoke_shell(width=160,height=48)
                # self.shell.send(f'PS1="{SSHInteractiveSession.ps1_label}"'.encode())
                return
                # while True: # wait for end of initial output
                #     full, part = self.read_output()
                #     if full and not part: return
                #     time.sleep(0.1)
            except Exception as e:
                errors += 1
                if errors < 3:
                    print(f"SSH Connection attempt {errors}...")
                    time.sleep(5)
                else:
                    raise e

    def close(self):
        if self.shell:
            self.shell.close()
        if self.client:
            self.client.close()

    def send_command(self, command: str):
        if not self.shell:
            raise Exception("Shell not connected")
        self.full_output = b""
        self.shell.send((command + " \\\n" +SSHInteractiveSession.end_comment + "\n").encode())

    def read_output(self) -> Tuple[str, str]:
        if not self.shell:
            raise Exception("Shell not connected")

        partial_output = b''
        while self.shell.recv_ready():
            data = self.shell.recv(1024)
            partial_output += data
            self.full_output += data
            time.sleep(0.1)  # Prevent busy waiting

        # Decode once at the end
        decoded_partial_output = partial_output.decode('utf-8', errors='replace')
        decoded_full_output = self.full_output.decode('utf-8', errors='replace')
        
        decoded_partial_output = self.clean_string(decoded_partial_output)
        decoded_full_output = self.clean_string(decoded_full_output)

        # Split output at end_comment
        if SSHInteractiveSession.end_comment in decoded_full_output:
            decoded_full_output = decoded_full_output.split(SSHInteractiveSession.end_comment)[-1].lstrip("\r\n")
            decoded_partial_output = decoded_partial_output.split(SSHInteractiveSession.end_comment)[-1].lstrip("\r\n")
        
        return decoded_full_output, decoded_partial_output


    def clean_string(self, input_string):
        # Remove ANSI escape codes
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        cleaned = ansi_escape.sub('', input_string)
        
        # Replace '\r\n' with '\n'
        cleaned = cleaned.replace('\r\n', '\n')
        
        return cleaned


---
File: /python/helpers/timed_input.py
---

from inputimeout import inputimeout, TimeoutOccurred

def timeout_input(prompt, timeout=10):
    try:
        import readline
        user_input = inputimeout(prompt=prompt, timeout=timeout)
        return user_input
    except TimeoutOccurred:
        return ""


---
File: /python/helpers/tool.py
---

from abc import abstractmethod
from typing import TypedDict
from agent import Agent
from python.helpers.print_style import PrintStyle
from python.helpers import files, messages

class Response:
    def __init__(self, message: str, break_loop: bool) -> None:
        self.message = message
        self.break_loop = break_loop
    
class Tool:

    def __init__(self, agent: Agent, name: str, args: dict[str,str], message: str, **kwargs) -> None:
        self.agent = agent
        self.name = name
        self.args = args
        self.message = message

    @abstractmethod
    def execute(self,**kwargs) -> Response:
        pass

    def before_execution(self, **kwargs):
        if self.agent.handle_intervention(): return # wait for intervention and handle it, if paused
        PrintStyle(font_color="#1B4F72", padding=True, background_color="white", bold=True).print(f"{self.agent.agent_name}: Using tool '{self.name}':")
        if self.args and isinstance(self.args, dict):
            for key, value in self.args.items():
                PrintStyle(font_color="#85C1E9", bold=True).stream(self.nice_key(key)+": ")
                PrintStyle(font_color="#85C1E9", padding=isinstance(value,str) and "\n" in value).stream(value)
                PrintStyle().print()
                    
    def after_execution(self, response: Response, **kwargs):
        text = messages.truncate_text(response.message.strip(), self.agent.config.max_tool_response_length)
        msg_response = files.read_file("./prompts/fw.tool_response.md", tool_name=self.name, tool_response=text)
        if self.agent.handle_intervention(): return # wait for intervention and handle it, if paused
        self.agent.append_message(msg_response, human=True)
        PrintStyle(font_color="#1B4F72", background_color="white", padding=True, bold=True).print(f"{self.agent.agent_name}: Response from tool '{self.name}':")
        PrintStyle(font_color="#85C1E9").print(response.message)

    def nice_key(self, key:str):
        words = key.split('_')
        words = [words[0].capitalize()] + [word.lower() for word in words[1:]]
        result = ' '.join(words)
        return result


---
File: /python/helpers/vdb.py
---

from langchain.storage import InMemoryByteStore, LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings
from langchain_core.embeddings import Embeddings

from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings

from . import files
from langchain_core.documents import Document
import uuid


class VectorDB:

    def __init__(self, embeddings_model:Embeddings, in_memory=False, cache_dir="./cache"):
        print("Initializing VectorDB...")
        self.embeddings_model = embeddings_model

        db_cache = files.get_abs_path(cache_dir,"database")

        self.client =chromadb.PersistentClient(path=db_cache)
        self.collection = self.client.create_collection("my_collection")
        self.collection

        
    def search(self, query:str, results=2):
        emb = self.embeddings_model.embed_query(query)
        res = self.collection.query(query_embeddings=[emb],n_results=results)
        best = res["documents"][0][0] # type: ignore
        
    # def delete_documents(self, query):
    #     score_limit = 1
    #     k = 2
    #     tot = 0
    #     while True:
    #         # Perform similarity search with score
    #         docs = self.db.similarity_search_with_score(query, k=k)

    #         # Extract document IDs and filter based on score
    #         document_ids = [result[0].metadata["id"] for result in docs if result[1] < score_limit]

    #         # Delete documents with IDs over the threshold score
    #         if document_ids:
    #             fnd = self.db.get(where={"id": {"$in": document_ids}})
    #             if fnd["ids"]: self.db.delete(ids=fnd["ids"])
    #             tot += len(fnd["ids"])
            
    #         # If fewer than K document IDs, break the loop
    #         if len(document_ids) < k:
    #             break
        
    #     return tot

    def insert(self, data:str):
        
        id = str(uuid.uuid4())
        emb = self.embeddings_model.embed_documents([data])[0]

        self.collection.add(
            ids=[id],
            embeddings=[emb],
            documents=[data],
            )

        return id
        





---
File: /python/helpers/vector_db.py
---

from langchain.storage import InMemoryByteStore, LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings
from langchain_chroma import Chroma

from . import files
from langchain_core.documents import Document
import uuid


class VectorDB:

    def __init__(self, embeddings_model, in_memory=False, cache_dir="./cache"):
        print("Initializing VectorDB...")
        self.embeddings_model = embeddings_model

        em_cache = files.get_abs_path(cache_dir,"embeddings")
        db_cache = files.get_abs_path(cache_dir,"database")
        
        if in_memory:
            self.store = InMemoryByteStore()
        else:
            self.store = LocalFileStore(em_cache)


        #here we setup the embeddings model with the chosen cache storage
        self.embedder = CacheBackedEmbeddings.from_bytes_store(
            embeddings_model, 
            self.store, 
            namespace=getattr(embeddings_model, 'model', getattr(embeddings_model, 'model_name', "default")) )


        self.db = Chroma(embedding_function=self.embedder,persist_directory=db_cache)
        
        
    def search_similarity(self, query, results=3):
        return self.db.similarity_search(query,results)
    
    def search_similarity_threshold(self, query, results=3, threshold=0.5):
        return self.db.search(query, search_type="similarity_score_threshold", k=results, score_threshold=threshold)

    def search_max_rel(self, query, results=3):
        return self.db.max_marginal_relevance_search(query,results)

    def delete_documents_by_query(self, query:str, threshold=0.1):
        k = 100
        tot = 0
        while True:
            # Perform similarity search with score
            docs = self.search_similarity_threshold(query, results=k, threshold=threshold)

            # Extract document IDs and filter based on score
            # document_ids = [result[0].metadata["id"] for result in docs if result[1] < score_limit]
            document_ids = [result.metadata["id"] for result in docs]
            

            # Delete documents with IDs over the threshold score
            if document_ids:
                # fnd = self.db.get(where={"id": {"$in": document_ids}})
                # if fnd["ids"]: self.db.delete(ids=fnd["ids"])
                # tot += len(fnd["ids"])
                self.db.delete(ids=document_ids)
                tot += len(document_ids)
                                    
            # If fewer than K document IDs, break the loop
            if len(document_ids) < k:
                break
        
        return tot

    def delete_documents_by_ids(self, ids:list[str]):
        # pre = self.db.get(ids=ids)["ids"]
        self.db.delete(ids=ids)
        # post = self.db.get(ids=ids)["ids"]
        #TODO? compare pre and post
        return len(ids)
        
    def insert_document(self, data):
        id = str(uuid.uuid4())
        self.db.add_documents(documents=[ Document(data, metadata={"id": id}) ], ids=[id])
        
        return id
        





---
File: /python/tools/call_subordinate.py
---

from agent import Agent
from python.helpers.tool import Tool, Response
from python.helpers import files
from python.helpers.print_style import PrintStyle

class Delegation(Tool):

    def execute(self, message="", reset="", **kwargs):
        # create subordinate agent using the data object on this agent and set superior agent to his data object
        if self.agent.get_data("subordinate") is None or str(reset).lower().strip() == "true":
            subordinate = Agent(self.agent.number+1, self.agent.config)
            subordinate.set_data("superior", self.agent)
            self.agent.set_data("subordinate", subordinate) 
        # run subordinate agent message loop
        return Response( message=self.agent.get_data("subordinate").message_loop(message), break_loop=False)


---
File: /python/tools/code_execution_tool.py
---

from dataclasses import dataclass
import os, json, contextlib, subprocess, ast, shlex
from io import StringIO
import time
from typing import Literal
from python.helpers import files, messages
from agent import Agent
from python.helpers.tool import Tool, Response
from python.helpers import files
from python.helpers.print_style import PrintStyle
from python.helpers.shell_local import LocalInteractiveSession
from python.helpers.shell_ssh import SSHInteractiveSession
from python.helpers.docker import DockerContainerManager

@dataclass
class State:
    shell: LocalInteractiveSession | SSHInteractiveSession
    docker: DockerContainerManager | None
        

class CodeExecution(Tool):

    def execute(self,**kwargs):

        if self.agent.handle_intervention(): return Response(message="", break_loop=False)  # wait for intervention and handle it, if paused
        
        self.prepare_state()

        # os.chdir(files.get_abs_path("./work_dir")) #change CWD to work_dir
        
        runtime = self.args["runtime"].lower().strip()
        if runtime == "python":
            response = self.execute_python_code(self.args["code"])
        elif runtime == "nodejs":
            response = self.execute_nodejs_code(self.args["code"])
        elif runtime == "terminal":
            response = self.execute_terminal_command(self.args["code"])
        elif runtime == "output":
            response = self.get_terminal_output()
        else:
            response = files.read_file("./prompts/fw.code_runtime_wrong.md", runtime=runtime)

        if not response: response = files.read_file("./prompts/fw.code_no_output.md")
        return Response(message=response, break_loop=False)

    def after_execution(self, response, **kwargs):
        msg_response = files.read_file("./prompts/fw.tool_response.md", tool_name=self.name, tool_response=response.message)
        self.agent.append_message(msg_response, human=True)

    def prepare_state(self):
        self.state = self.agent.get_data("cot_state")
        if not self.state:

            #initialize docker container if execution in docker is configured
            if self.agent.config.code_exec_docker_enabled:
                docker = DockerContainerManager(name=self.agent.config.code_exec_docker_name, image=self.agent.config.code_exec_docker_image, ports=self.agent.config.code_exec_docker_ports, volumes=self.agent.config.code_exec_docker_volumes)
                docker.start_container()
            else: docker = None

            #initialize local or remote interactive shell insterface
            if self.agent.config.code_exec_ssh_enabled:
                shell = SSHInteractiveSession(self.agent.config.code_exec_ssh_addr,self.agent.config.code_exec_ssh_port,self.agent.config.code_exec_ssh_user,self.agent.config.code_exec_ssh_pass)
            else: shell = LocalInteractiveSession()
                
            self.state = State(shell=shell,docker=docker)
            shell.connect()
        self.agent.set_data("cot_state", self.state)
    
    def execute_python_code(self, code):
        escaped_code = shlex.quote(code)
        command = f'python3 -c {escaped_code}'
        return self.terminal_session(command)

    def execute_nodejs_code(self, code):
        escaped_code = shlex.quote(code)
        command = f'node -e {escaped_code}'
        return self.terminal_session(command)

    def execute_terminal_command(self, command):
        return self.terminal_session(command)

    def terminal_session(self, command):

        if self.agent.handle_intervention(): return ""  # wait for intervention and handle it, if paused
       
        self.state.shell.send_command(command)

        PrintStyle(background_color="white",font_color="#1B4F72",bold=True).print(f"{self.agent.agent_name} code execution output:")
        return self.get_terminal_output()

    def get_terminal_output(self):
        idle = 0
        max_idle_cycles = 300  # Example idle timeout cycles (adjust as needed)
        full_output = ""
        displayed_output = set()
        last_partial_output = None  # To track the last piece of output
        prompt_detected = False

        while True:
            time.sleep(0.1)  # Wait for some output to be generated
            output_tuple = self.state.shell.read_output()

            if self.agent.handle_intervention():
                return self.summarize_output(full_output)

            if output_tuple:
                partial_output = output_tuple[0] if isinstance(output_tuple, tuple) else output_tuple
                if partial_output:
                    # Stream all unique outputs to the user
                    if partial_output not in displayed_output:
                        PrintStyle(font_color="#85C1E9").stream(partial_output)
                        displayed_output.add(partial_output)

                    full_output += partial_output

                    # Reset idle counter if new output differs from last output
                    if partial_output != last_partial_output:
                        idle = 0
                        last_partial_output = partial_output
                    else:
                        idle += 1

                    # Check for interaction prompts (like `[Y/n]`)
                    if any(prompt in partial_output.lower() for prompt in
                           ["[y/n]", "proceed (y/n)", "continue (y/n)", "do you want to continue", "yes/no"]):
                        prompt_detected = True
                        break

                    # Check for the command prompt indicating the end of output
                    if "root@" in partial_output and partial_output.strip().endswith("#"):
                        break  # End the loop when the prompt is detected
                else:
                    idle += 1
            else:
                idle += 1

            # Exit the loop if no new output has been received for a while
            if idle > max_idle_cycles:
                PrintStyle(font_color="yellow", padding=True).print("Idle timeout reached. No new output detected.")
                break

        return self.summarize_output(full_output, prompt_detected)

    def summarize_output(self, full_output, prompt_detected=False):
        # Log the summary attempt
        PrintStyle(font_color="blue", padding=True).print("Attempting to summarize output...")

        # Capture the last 2000 characters
        summarized_output = full_output[-10000:].strip()

        # Check if summarized output is not empty and log
        if summarized_output:
            PrintStyle(font_color="green", padding=True).print(
                f"Relevant output captured: {summarized_output[:100]}...")  # Log a snippet of the output
        else:
            summarized_output = "No relevant output captured."
            PrintStyle(font_color="red", padding=True).print("No relevant output captured; returning default message.")

        # If a prompt was detected, inform the agent
        if prompt_detected:
            PrintStyle(font_color="yellow", padding=True).print(
                "Interaction prompt detected, returning to agent for handling.")

        return summarized_output










---
File: /python/tools/knowledge_tool.py
---

import os
import concurrent.futures
from python.helpers.tool import Tool, Response
from python.helpers import files
from python.helpers import perplexity_search, duckduckgo_search
from python.helpers.print_style import PrintStyle  # Retain new import from main branch
from . import memory_tool

class Knowledge(Tool):
    def execute(self, question="", **kwargs):
        with concurrent.futures.ThreadPoolExecutor() as executor:
            # Schedule the different searches to be run in parallel
            if os.getenv("API_KEY_PERPLEXITY"):
                perplexity_future = executor.submit(perplexity_search.perplexity_search, question)
            else:
                PrintStyle.hint("No API key provided for Perplexity. Skipping Perplexity search.")
                perplexity_future = None

            duckduckgo_future = executor.submit(duckduckgo_search.search, question)
            memory_future = executor.submit(memory_tool.search, self.agent, question)
            wikipedia_future = executor.submit(fetch_wikipedia_content, question)

            # Collect the results
            perplexity_result = (perplexity_future.result() if perplexity_future else "") or ""
            duckduckgo_result = duckduckgo_future.result()
            memory_result = memory_future.result()
            wikipedia_result = wikipedia_future.result()

        msg = files.read_file(
            "prompts/tool.knowledge.response.md",
            online_sources=perplexity_result + "\n\n" + str(duckduckgo_result) + "\n\n" + wikipedia_result,
            memory=memory_result
        )

        if self.agent.handle_intervention(msg):  # wait for intervention and handle it, if paused
            pass

        return Response(message=msg, break_loop=False)

def fetch_wikipedia_content(query):
    try:
        import wikipedia
    except ImportError:
        return "Error: The 'wikipedia' library is not installed. Please install it using 'pip install wikipedia'."

    try:
        summary = wikipedia.summary(query)
        return summary
    except wikipedia.exceptions.DisambiguationError as e:
        options = e.options
        return f"Disambiguation Error: The query '{query}' may refer to multiple topics. Options include: {options[:3]}..."  # Limiting to the first 3 options for brevity
    except wikipedia.exceptions.PageError:
        return "Page not found."
    except Exception as e:
        return f"An error occurred: {str(e)}"



---
File: /python/tools/memory_tool.py
---

import re
from agent import Agent
from python.helpers.vector_db import VectorDB, Document
from python.helpers import files
import os, json
from python.helpers.tool import Tool, Response
from python.helpers.print_style import PrintStyle
from chromadb.errors import InvalidDimensionException

# TODO multiple DBs at once
db: VectorDB | None= None

class Memory(Tool):
    def execute(self,**kwargs):
        result=""
        
        try:
            if "query" in kwargs:
                threshold = float(kwargs.get("threshold", 0.1))
                count = int(kwargs.get("count", 5))
                result = search(self.agent, kwargs["query"], count, threshold)
            elif "memorize" in kwargs:
                result = save(self.agent, kwargs["memorize"])
            elif "forget" in kwargs:
                result = forget(self.agent, kwargs["forget"])
            elif "delete" in kwargs:
                result = delete(self.agent, kwargs["delete"])
        except InvalidDimensionException as e:
            # hint about embedding change with existing database
            PrintStyle.hint("If you changed your embedding model, you will need to remove contents of /memory directory.")
            raise   
        
        # result = process_query(self.agent, self.args["memory"],self.args["action"], result_count=self.agent.config.auto_memory_count)
        return Response(message=result, break_loop=False)
            
def search(agent:Agent, query:str, count:int=5, threshold:float=0.1):
    initialize(agent)
    docs = db.search_similarity_threshold(query,count,threshold) # type: ignore
    if len(docs)==0: return files.read_file("./prompts/fw.memories_not_found.md", query=query)
    else: return str(docs)

def save(agent:Agent, text:str):
    initialize(agent)
    id = db.insert_document(text) # type: ignore
    return files.read_file("./prompts/fw.memory_saved.md", memory_id=id)

def delete(agent:Agent, ids_str:str):
    initialize(agent)
    ids = extract_guids(ids_str)
    deleted = db.delete_documents_by_ids(ids) # type: ignore
    return files.read_file("./prompts/fw.memories_deleted.md", memory_count=deleted)    

def forget(agent:Agent, query:str):
    initialize(agent)
    deleted = db.delete_documents_by_query(query) # type: ignore
    return files.read_file("./prompts/fw.memories_deleted.md", memory_count=deleted)

def initialize(agent:Agent):
    global db
    if not db:
        dir = os.path.join("memory",agent.config.memory_subdir)
        db = VectorDB(embeddings_model=agent.config.embeddings_model, in_memory=False, cache_dir=dir)

def extract_guids(text):
    pattern = r'\b[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[1-5][0-9a-fA-F]{3}-[89abAB][0-9a-fA-F]{3}-[0-9a-fA-F]{12}\b'
    return re.findall(pattern, text)


---
File: /python/tools/online_knowledge_tool.py
---

from agent import Agent
from python.helpers import perplexity_search
from python.helpers.tool import Tool, Response

class OnlineKnowledge(Tool):
    def execute(self,**kwargs):
        return Response(
            message=process_question(self.args["question"]),
            break_loop=False,
        )

def process_question(question):
    return str(perplexity_search.perplexity_search(question))


---
File: /python/tools/response.py
---

from agent import Agent
from python.helpers import files
from python.helpers.print_style import PrintStyle

from agent import Agent
from python.helpers.tool import Tool, Response
from python.helpers import files
from python.helpers.print_style import PrintStyle

class ResponseTool(Tool):

    def execute(self,**kwargs):
        self.agent.set_data("timeout", self.agent.config.response_timeout_seconds)
        return Response(message=self.args["text"], break_loop=True)

    def before_execution(self, **kwargs):
        pass # do not add anything to the history or output
    
    def after_execution(self, response, **kwargs):
        pass # do not add anything to the history or output


---
File: /python/tools/task_done.py
---

from agent import Agent
from python.helpers import files
from python.helpers.print_style import PrintStyle

from agent import Agent
from python.helpers.tool import Tool, Response
from python.helpers import files
from python.helpers.print_style import PrintStyle

class TaskDone(Tool):

    def execute(self,**kwargs):
        self.agent.set_data("timeout", 0)
        return Response(message=self.args["text"], break_loop=True)

    def before_execution(self, **kwargs):
        pass # do not add anything to the history or output
    
    def after_execution(self, response, **kwargs):
        pass # do add anything to the history or output


---
File: /python/tools/unknown.py
---

from python.helpers.tool import Tool, Response
from python.helpers import files

class Unknown(Tool):
    def execute(self, **kwargs):
        return Response(
                message=files.read_file("prompts/fw.tool_not_found.md",
                                        tool_name=self.name,
                                        tools_prompt=files.read_file("prompts/agent.tools.md")), 
                break_loop=False)




---
File: /python/__init__.py
---




---
File: /work_dir/.gitkeep
---




---
File: /.gitattributes
---

# Auto detect text files and perform LF normalization
* text=auto



---
File: /.gitignore
---

**/.DS_Store
**/.env
**/__pycache__/



# Ignore all contents of the virtual environment directory
.venv/*

# Ignore all contents of the directory "work_dir"
work_dir/*
# But do not ignore the directory itself
!work_dir/.gitkeep

# Ignore all contents of the directory "memory"
memory/*
# But do not ignore the directory itself
!memory/.gitkeep

# Ignore all contents of the directory "logs"
logs/*
# But do not ignore the directory itself
!logs/.gitkeep


---
File: /agent.py
---

from dataclasses import dataclass, field
import time, importlib, inspect, os, json
from typing import Any, Optional, Dict
from python.helpers import extract_tools, rate_limiter, files, errors
from python.helpers.print_style import PrintStyle
from langchain.schema import AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.language_models.llms import BaseLLM
from langchain_core.embeddings import Embeddings

@dataclass
class AgentConfig: 
    chat_model: BaseChatModel | BaseLLM
    utility_model: BaseChatModel | BaseLLM
    embeddings_model:Embeddings
    memory_subdir: str = ""
    auto_memory_count: int = 3
    auto_memory_skip: int = 2
    rate_limit_seconds: int = 60
    rate_limit_requests: int = 15
    rate_limit_input_tokens: int = 1000000
    rate_limit_output_tokens: int = 0
    msgs_keep_max: int = 25
    msgs_keep_start: int = 5
    msgs_keep_end: int = 10
    response_timeout_seconds: int = 60
    max_tool_response_length: int = 100000
    code_exec_docker_enabled: bool = True
    code_exec_docker_name: str = "docker-agent-zero-exe"
    code_exec_docker_image: str = "docker-agent-zero-exe:latest"
    code_exec_docker_ports: dict[str,int] = field(default_factory=lambda: {"22/tcp": 50022})
    code_exec_docker_volumes: dict[str, dict[str, str]] = field(default_factory=lambda: {files.get_abs_path("work_dir"): {"bind": "/root", "mode": "rw"}})
    code_exec_ssh_enabled: bool = True
    code_exec_ssh_addr: str = "localhost"
    code_exec_ssh_port: int = 50022
    code_exec_ssh_user: str = "root"
    code_exec_ssh_pass: str = "toor"
    additional: Dict[str, Any] = field(default_factory=dict)
    

class Agent:

    paused=False
    streaming_agent=None
    
    def __init__(self, number:int, config: AgentConfig):

        # agent config  
        self.config = config       

        # non-config vars
        self.number = number
        self.agent_name = f"Agent {self.number}"

        self.system_prompt = files.read_file("./prompts/agent.system.md", agent_name=self.agent_name)
        self.tools_prompt = files.read_file("./prompts/agent.tools.md")

        self.history = []
        self.last_message = ""
        self.intervention_message = ""
        self.intervention_status = False
        self.rate_limiter = rate_limiter.RateLimiter(max_calls=self.config.rate_limit_requests,max_input_tokens=self.config.rate_limit_input_tokens,max_output_tokens=self.config.rate_limit_output_tokens,window_seconds=self.config.rate_limit_seconds)
        self.data = {} # free data object all the tools can use

        os.chdir(files.get_abs_path("./work_dir")) #change CWD to work_dir
        

    def message_loop(self, msg: str):
        try:
            printer = PrintStyle(italic=True, font_color="#b3ffd9", padding=False)    
            user_message = files.read_file("./prompts/fw.user_message.md", message=msg)
            self.append_message(user_message, human=True) # Append the user's input to the history                        
            memories = self.fetch_memories(True)
                
            while True: # let the agent iterate on his thoughts until he stops by using a tool
                Agent.streaming_agent = self # mark self as current streamer
                agent_response = ""
                self.intervention_status = False # reset intervention status

                try:

                    system = self.system_prompt + "\n\n" + self.tools_prompt
                    memories = self.fetch_memories()
                    if memories: system+= "\n\n"+memories

                    prompt = ChatPromptTemplate.from_messages([
                        SystemMessage(content=system),
                        MessagesPlaceholder(variable_name="messages") ])
                    
                    inputs = {"messages": self.history}
                    chain = prompt | self.config.chat_model

                    formatted_inputs = prompt.format(messages=self.history)
                    tokens = int(len(formatted_inputs)/4)     
                    self.rate_limiter.limit_call_and_input(tokens)
                    
                    # output that the agent is starting
                    PrintStyle(bold=True, font_color="green", padding=True, background_color="white").print(f"{self.agent_name}: Starting a message:")
                                            
                    for chunk in chain.stream(inputs):
                        if self.handle_intervention(agent_response): break # wait for intervention and handle it, if paused

                        if isinstance(chunk, str): content = chunk
                        elif hasattr(chunk, "content"): content = str(chunk.content)
                        else: content = str(chunk)
                        
                        if content:
                            printer.stream(content) # output the agent response stream                
                            agent_response += content # concatenate stream into the response

                    self.rate_limiter.set_output_tokens(int(len(agent_response)/4))
                    
                    if not self.handle_intervention(agent_response):
                        if self.last_message == agent_response: #if assistant_response is the same as last message in history, let him know
                            self.append_message(agent_response) # Append the assistant's response to the history
                            warning_msg = files.read_file("./prompts/fw.msg_repeat.md")
                            self.append_message(warning_msg, human=True) # Append warning message to the history
                            PrintStyle(font_color="orange", padding=True).print(warning_msg)

                        else: #otherwise proceed with tool
                            self.append_message(agent_response) # Append the assistant's response to the history
                            tools_result = self.process_tools(agent_response) # process tools requested in agent message
                            if tools_result: return tools_result #break the execution if the task is done

                # Forward errors to the LLM, maybe he can fix them
                except Exception as e:
                    error_message = errors.format_error(e)
                    msg_response = files.read_file("./prompts/fw.error.md", error=error_message) # error message template
                    self.append_message(msg_response, human=True)
                    PrintStyle(font_color="red", padding=True).print(msg_response)
                    
        finally:
            Agent.streaming_agent = None # unset current streamer

    def get_data(self, field:str):
        return self.data.get(field, None)

    def set_data(self, field:str, value):
        self.data[field] = value

    def append_message(self, msg: str, human: bool = False):
        message_type = "human" if human else "ai"
        if self.history and self.history[-1].type == message_type:
            self.history[-1].content += "\n\n" + msg
        else:
            new_message = HumanMessage(content=msg) if human else AIMessage(content=msg)
            self.history.append(new_message)
            self.cleanup_history(self.config.msgs_keep_max, self.config.msgs_keep_start, self.config.msgs_keep_end)
        if message_type=="ai":
            self.last_message = msg

    def concat_messages(self,messages):
        return "\n".join([f"{msg.type}: {msg.content}" for msg in messages])

    def send_adhoc_message(self, system: str, msg: str, output_label:str):
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content=system),
            HumanMessage(content=msg)])

        chain = prompt | self.config.utility_model
        response = ""
        printer = None

        if output_label:
            PrintStyle(bold=True, font_color="orange", padding=True, background_color="white").print(f"{self.agent_name}: {output_label}:")
            printer = PrintStyle(italic=True, font_color="orange", padding=False)                

        formatted_inputs = prompt.format()
        tokens = int(len(formatted_inputs)/4)     
        self.rate_limiter.limit_call_and_input(tokens)
    
        for chunk in chain.stream({}):
            if self.handle_intervention(): break # wait for intervention and handle it, if paused

            if isinstance(chunk, str): content = chunk
            elif hasattr(chunk, "content"): content = str(chunk.content)
            else: content = str(chunk)

            if printer: printer.stream(content)
            response+=content

        self.rate_limiter.set_output_tokens(int(len(response)/4))

        return response
            
    def get_last_message(self):
        if self.history:
            return self.history[-1]

    def replace_middle_messages(self,middle_messages):
        cleanup_prompt = files.read_file("./prompts/fw.msg_cleanup.md")
        summary = self.send_adhoc_message(system=cleanup_prompt,msg=self.concat_messages(middle_messages), output_label="Mid messages cleanup summary")
        new_human_message = HumanMessage(content=summary)
        return [new_human_message]

    def cleanup_history(self, max:int, keep_start:int, keep_end:int):
        if len(self.history) <= max:
            return self.history

        first_x = self.history[:keep_start]
        last_y = self.history[-keep_end:]

        # Identify the middle part
        middle_part = self.history[keep_start:-keep_end]

        # Ensure the first message in the middle is "human", if not, move one message back
        if middle_part and middle_part[0].type != "human":
            if len(first_x) > 0:
                middle_part.insert(0, first_x.pop())

        # Ensure the middle part has an odd number of messages
        if len(middle_part) % 2 == 0:
            middle_part = middle_part[:-1]

        # Replace the middle part using the replacement function
        new_middle_part = self.replace_middle_messages(middle_part)

        self.history = first_x + new_middle_part + last_y

        return self.history

    def handle_intervention(self, progress:str="") -> bool:
        while self.paused: time.sleep(0.1) # wait if paused
        if self.intervention_message and not self.intervention_status: # if there is an intervention message, but not yet processed
            if progress.strip(): self.append_message(progress) # append the response generated so far
            user_msg = files.read_file("./prompts/fw.intervention.md", user_message=self.intervention_message) # format the user intervention template
            self.append_message(user_msg,human=True) # append the intervention message
            self.intervention_message = "" # reset the intervention message
            self.intervention_status = True
        return self.intervention_status # return intervention status

    def process_tools(self, msg: str):
        # search for tool usage requests in agent message
        tool_request = extract_tools.json_parse_dirty(msg)

        if tool_request is not None:
            tool_name = tool_request.get("tool_name", "")
            tool_args = tool_request.get("tool_args", {})

            tool = self.get_tool(
                        tool_name,
                        tool_args,
                        msg)
                
            if self.handle_intervention(): return # wait if paused and handle intervention message if needed
            tool.before_execution(**tool_args)
            if self.handle_intervention(): return # wait if paused and handle intervention message if needed
            response = tool.execute(**tool_args)
            if self.handle_intervention(): return # wait if paused and handle intervention message if needed
            tool.after_execution(response)
            if self.handle_intervention(): return # wait if paused and handle intervention message if needed
            if response.break_loop: return response.message
        else:
            msg = files.read_file("prompts/fw.msg_misformat.md")
            self.append_message(msg, human=True)
            PrintStyle(font_color="red", padding=True).print(msg)


    def get_tool(self, name: str, args: dict, message: str, **kwargs):
        from python.tools.unknown import Unknown 
        from python.helpers.tool import Tool
        
        tool_class = Unknown
        if files.exists("python/tools",f"{name}.py"): 
            module = importlib.import_module("python.tools." + name)  # Import the module
            class_list = inspect.getmembers(module, inspect.isclass)  # Get all functions in the module

            for cls in class_list:
                if cls[1] is not Tool and issubclass(cls[1], Tool):
                    tool_class = cls[1]
                    break

        return tool_class(agent=self, name=name, args=args, message=message, **kwargs)

    def fetch_memories(self,reset_skip=False):
        if self.config.auto_memory_count<=0: return ""
        if reset_skip: self.memory_skip_counter = 0

        if self.memory_skip_counter > 0:
            self.memory_skip_counter-=1
            return ""
        else:
            self.memory_skip_counter = self.config.auto_memory_skip
            from python.tools import memory_tool
            messages = self.concat_messages(self.history)
            memories = memory_tool.search(self,messages)
            input = {
                "conversation_history" : messages,
                "raw_memories": memories
            }
            cleanup_prompt = files.read_file("./prompts/msg.memory_cleanup.md").replace("{", "{{")       
            clean_memories = self.send_adhoc_message(cleanup_prompt,json.dumps(input), output_label="Memory injection")
            return clean_memories

    def call_extension(self, name: str, **kwargs) -> Any:
        pass


---
File: /example.env
---

API_KEY_OPENAI=
API_KEY_ANTHROPIC=
API_KEY_GROQ=
API_KEY_PERPLEXITY=
API_KEY_GOOGLE=
API_KEY_OPENROUTER=

API_KEY_OPENAI_AZURE=
OPENAI_AZURE_ENDPOINT=
OPENAI_API_VERSION=

TOKENIZERS_PARALLELISM=true
PYDEVD_DISABLE_FILE_VALIDATION=1 


---
File: /LICENSE
---

MIT License

Copyright (c) 2024 Jan Tomášek
Contact: tomasekhonza@gmail.com
Repository: https://github.com/frdel/agent-zero

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


---
File: /main.py
---

import threading, time, models, os, sys
from ansio import application_keypad, mouse_input, raw_input
from ansio.input import InputEvent, get_input_event
from agent import Agent, AgentConfig
from python.helpers.print_style import PrintStyle
from python.helpers.files import read_file
from python.helpers import files
import python.helpers.timed_input as timed_input


input_lock = threading.Lock()
os.chdir(files.get_abs_path("./work_dir")) #change CWD to work_dir


def initialize():
    
    # main chat model used by agents (smarter, more accurate)
    chat_llm = models.get_openai_chat(model_name="gpt-4o-mini", temperature=0)
    # chat_llm = models.get_ollama_chat(model_name="gemma2:latest", temperature=0)
    # chat_llm = models.get_lmstudio_chat(model_name="TheBloke/Mistral-7B-Instruct-v0.2-GGUF", temperature=0)
    # chat_llm = models.get_openrouter(model_name="meta-llama/llama-3-8b-instruct:free")
    # chat_llm = models.get_azure_openai_chat(deployment_name="gpt-4o-mini", temperature=0)
    # chat_llm = models.get_anthropic_chat(model_name="claude-3-5-sonnet-20240620", temperature=0)
    # chat_llm = models.get_google_chat(model_name="gemini-1.5-flash", temperature=0)
    # chat_llm = models.get_groq_chat(model_name="llama-3.1-70b-versatile", temperature=0)
    
    # utility model used for helper functions (cheaper, faster)
    utility_llm = chat_llm # change if you want to use a different utility model

    # embedding model used for memory
    embedding_llm = models.get_openai_embedding(model_name="text-embedding-3-small")
    # embedding_llm = models.get_ollama_embedding(model_name="nomic-embed-text")
    # embedding_llm = models.get_huggingface_embedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # agent configuration
    config = AgentConfig(
        chat_model = chat_llm,
        utility_model = utility_llm,
        embeddings_model = embedding_llm,
        # memory_subdir = "",
        auto_memory_count = 0,
        # auto_memory_skip = 2,
        # rate_limit_seconds = 60,
        # rate_limit_requests = 30,
        # rate_limit_input_tokens = 0,
        # rate_limit_output_tokens = 0,
        # msgs_keep_max = 25,
        # msgs_keep_start = 5,
        # msgs_keep_end = 10,
        # max_tool_response_length = 3000,
        # response_timeout_seconds = 60,
        code_exec_docker_enabled = True,
        # code_exec_docker_name = "agent-zero-exe",
        # code_exec_docker_image = "docker-agent-zero-exe:latest",
        # code_exec_docker_ports = { "22/tcp": 50022 }
        # code_exec_docker_volumes = { files.get_abs_path("work_dir"): {"bind": "/root", "mode": "rw"} }
        code_exec_ssh_enabled = True,
        # code_exec_ssh_addr = "localhost",
        # code_exec_ssh_port = 50022,
        # code_exec_ssh_user = "root",
        # code_exec_ssh_pass = "toor",
        # additional = {},
    )
    
    # create the first agent
    agent0 = Agent( number = 0, config = config )

    # start the chat loop
    chat(agent0)


# Main conversation loop
def chat(agent:Agent):
    
    # start the conversation loop  
    while True:
        # ask user for message
        with input_lock:
            timeout = agent.get_data("timeout") # how long the agent is willing to wait
            if not timeout: # if agent wants to wait for user input forever
                PrintStyle(background_color="#6C3483", font_color="white", bold=True, padding=True).print(f"User message ('e' to leave):")        
                import readline # this fixes arrow keys in terminal
                user_input = input("> ")
                PrintStyle(font_color="white", padding=False, log_only=True).print(f"> {user_input}") 
                
            else: # otherwise wait for user input with a timeout
                PrintStyle(background_color="#6C3483", font_color="white", bold=True, padding=True).print(f"User message ({timeout}s timeout, 'w' to wait, 'e' to leave):")        
                import readline # this fixes arrow keys in terminal
                # user_input = timed_input("> ", timeout=timeout)
                user_input = timeout_input("> ", timeout=timeout)
                                    
                if not user_input:
                    user_input = read_file("prompts/fw.msg_timeout.md")
                    PrintStyle(font_color="white", padding=False).stream(f"{user_input}")        
                else:
                    user_input = user_input.strip()
                    if user_input.lower()=="w": # the user needs more time
                        user_input = input("> ").strip()
                    PrintStyle(font_color="white", padding=False, log_only=True).print(f"> {user_input}")        
                    
                    

        # exit the conversation when the user types 'exit'
        if user_input.lower() == 'e': break

        # send message to agent0, 
        assistant_response = agent.message_loop(user_input)
        
        # print agent0 response
        PrintStyle(font_color="white",background_color="#1D8348", bold=True, padding=True).print(f"{agent.agent_name}: reponse:")        
        PrintStyle(font_color="white").print(f"{assistant_response}")        
                        

# User intervention during agent streaming
def intervention():
    if Agent.streaming_agent and not Agent.paused:
        Agent.paused = True # stop agent streaming
        PrintStyle(background_color="#6C3483", font_color="white", bold=True, padding=True).print(f"User intervention ('e' to leave, empty to continue):")        

        import readline # this fixes arrow keys in terminal
        user_input = input("> ").strip()
        PrintStyle(font_color="white", padding=False, log_only=True).print(f"> {user_input}")        
        
        if user_input.lower() == 'e': os._exit(0) # exit the conversation when the user types 'exit'
        if user_input: Agent.streaming_agent.intervention_message = user_input # set intervention message if non-empty
        Agent.paused = False # continue agent streaming 
    
def is_running_in_terminal():
    return sys.stdin.isatty() and sys.stdout.isatty()

# Capture keyboard input to trigger user intervention
def capture_keys():
        global input_lock
        intervent=False            
        while True:
            if intervent: intervention()
            intervent = False
            time.sleep(0.1)
            
            if Agent.streaming_agent and is_running_in_terminal():
                # with raw_input, application_keypad, mouse_input:
                with input_lock, raw_input, application_keypad:
                    event: InputEvent | None = get_input_event(timeout=0.1)
                    if event and (event.shortcut.isalpha() or event.shortcut.isspace()):
                        intervent=True
                        continue

# User input with timeout
def timeout_input(prompt, timeout=120):
    return timed_input.timeout_input(prompt=prompt, timeout=timeout)

if __name__ == "__main__":
    print("Initializing framework...")

    # Start the key capture thread for user intervention during agent streaming
    threading.Thread(target=capture_keys, daemon=True).start()

    # Start the chat
    initialize()



---
File: /models.py
---

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAI, OpenAIEmbeddings, AzureChatOpenAI, AzureOpenAIEmbeddings, AzureOpenAI
from langchain_community.llms.ollama import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_anthropic import ChatAnthropic
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory
from pydantic.v1.types import SecretStr


# Load environment variables
load_dotenv()

# Configuration
DEFAULT_TEMPERATURE = 0.0

# Utility function to get API keys from environment variables
def get_api_key(service):
    return os.getenv(f"API_KEY_{service.upper()}") or os.getenv(f"{service.upper()}_API_KEY")


# Ollama models
def get_ollama_chat(model_name:str, temperature=DEFAULT_TEMPERATURE, base_url="http://localhost:11434"):
    return Ollama(model=model_name,temperature=temperature, base_url=base_url)

def get_ollama_embedding(model_name:str, temperature=DEFAULT_TEMPERATURE):
    return OllamaEmbeddings(model=model_name,temperature=temperature)

# HuggingFace models

def get_huggingface_embedding(model_name:str):
    return HuggingFaceEmbeddings(model_name=model_name)

# LM Studio and other OpenAI compatible interfaces
def get_lmstudio_chat(model_name:str, base_url="http://localhost:1234/v1", temperature=DEFAULT_TEMPERATURE):
    return ChatOpenAI(model_name=model_name, base_url=base_url, temperature=temperature, api_key="none") # type: ignore

def get_lmstudio_embedding(model_name:str, base_url="http://localhost:1234/v1"):
    return OpenAIEmbeddings(model_name=model_name, base_url=base_url) # type: ignore

# Anthropic models
def get_anthropic_chat(model_name:str, api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("anthropic")
    return ChatAnthropic(model_name=model_name, temperature=temperature, api_key=api_key) # type: ignore

# OpenAI models
def get_openai_chat(model_name:str, api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("openai")
    return ChatOpenAI(model_name=model_name, temperature=temperature, api_key=api_key) # type: ignore

def get_openai_instruct(model_name:str,api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("openai")
    return OpenAI(model=model_name, temperature=temperature, api_key=api_key) # type: ignore

def get_openai_embedding(model_name:str, api_key=None):
    api_key = api_key or get_api_key("openai")
    return OpenAIEmbeddings(model=model_name, api_key=api_key) # type: ignore

def get_azure_openai_chat(deployment_name:str, api_key=None, temperature=DEFAULT_TEMPERATURE, azure_endpoint=None):
    api_key = api_key or get_api_key("openai_azure")
    azure_endpoint = azure_endpoint or os.getenv("OPENAI_AZURE_ENDPOINT")
    return AzureChatOpenAI(deployment_name=deployment_name, temperature=temperature, api_key=api_key, azure_endpoint=azure_endpoint) # type: ignore

def get_azure_openai_instruct(deployment_name:str, api_key=None, temperature=DEFAULT_TEMPERATURE, azure_endpoint=None):
    api_key = api_key or get_api_key("openai_azure")
    azure_endpoint = azure_endpoint or os.getenv("OPENAI_AZURE_ENDPOINT")
    return AzureOpenAI(deployment_name=deployment_name, temperature=temperature, api_key=api_key, azure_endpoint=azure_endpoint) # type: ignore

def get_azure_openai_embedding(deployment_name:str, api_key=None, azure_endpoint=None):
    api_key = api_key or get_api_key("openai_azure")
    azure_endpoint = azure_endpoint or os.getenv("OPENAI_AZURE_ENDPOINT")
    return AzureOpenAIEmbeddings(deployment_name=deployment_name, api_key=api_key, azure_endpoint=azure_endpoint) # type: ignore

# Google models
def get_google_chat(model_name:str, api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("google")
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, google_api_key=api_key, safety_settings={HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE }) # type: ignore

# Groq models
def get_groq_chat(model_name:str, api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("groq")
    return ChatGroq(model_name=model_name, temperature=temperature, api_key=api_key) # type: ignore
   
# OpenRouter models
def get_openrouter(model_name: str="meta-llama/llama-3.1-8b-instruct:free", api_key=None, temperature=DEFAULT_TEMPERATURE):
    api_key = api_key or get_api_key("openrouter")
    return ChatOpenAI(api_key=api_key, base_url="https://openrouter.ai/api/v1", model=model_name, temperature=temperature) # type: ignore
        
def get_embedding_hf(model_name="sentence-transformers/all-MiniLM-L6-v2"):
    return HuggingFaceEmbeddings(model_name=model_name)

def get_embedding_openai(api_key=None):
    api_key = api_key or get_api_key("openai")
    return OpenAIEmbeddings(api_key=api_key) #type: ignore



---
File: /README.md
---

# Agent Zero

[![Join our Discord](https://img.shields.io/badge/Discord-Join%20our%20server-5865F2?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/B8KZKNsPpj) [![Subscribe on YouTube](https://img.shields.io/badge/YouTube-Subscribe-red?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/@AgentZeroFW) [![Connect on LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/jan-tomasek/) [![Follow on X.com](https://img.shields.io/badge/X.com-Follow-1DA1F2?style=for-the-badge&logo=x&logoColor=white)](https://x.com/JanTomasekDev)



[![Intro Video](/docs/intro_vid.jpg)](https://www.youtube.com/watch?v=C9n8zFpaV3I)

**Personal and organic AI framework**
- Agent Zero is not a predefined agentic framework. It is designed to be dynamic, organically growing, and learning as you use it.
- Agent Zero is fully transparent, readable, comprehensible, customizable and interactive.
- Agent Zero uses the computer as a tool to accomplish its (your) tasks.

## Key concepts
1. **General-purpose assistant**
- Agent Zero is not pre-programmed for specific tasks (but can be). It is meant to be a general-purpose personal assistant. Give it a task, and it will gather information, execute commands and code, cooperate with other agent instances, and do its best to accomplish it.
- It has a persistent memory, allowing it to memorize previous solutions, code, facts, instructions, etc., to solve tasks faster and more reliably in the future.

2. **Computer as a tool**
- Agent Zero uses the operating system as a tool to accomplish its tasks. It has no single-purpose tools pre-programmed. Instead, it can write its own code and use the terminal to create and use its own tools as needed.
- The only default tools in its arsenal are online search, memory features, communication (with the user and other agents), and code/terminal execution. Everything else is created by the agent itself or can be extended by the user.
- Tool usage functionality has been developed from scratch to be the most compatible and reliable, even with very small models.

3. **Multi-agent cooperation**
- Every agent has a superior agent giving it tasks and instructions. Every agent then reports back to its superior.
- In the case of the first agent, the superior is the human user; the agent sees no difference.
- Every agent can create its subordinate agent to help break down and solve subtasks. This helps all agents keep their context clean and focused.

4. **Completely customizable and extensible**
- Almost nothing in this framework is hard-coded. Nothing is hidden. Everything can be extended or changed by the user.
- The whole behavior is defined by a system prompt in the **prompts/agent.system.md** file. Change this prompt and change the framework dramatically.
- The framework does not guide or limit the agent in any way. There are no hard-coded rails that agents have to follow.
- Every prompt, every small message template sent to the agent in its communication loop, can be found in the **prompts/** folder and changed.
- Every default tool can be found in the **python/tools/** folder and changed or copied to create new predefined tools.
- Of course, it is open-source (except for some tools like Perplexity, but that will be replaced with an open-source alternative as well in the future).

5. **Communication is key**
- Give your agent a proper system prompt and instructions, and it can do miracles.
- Agents can communicate with their superiors and subordinates, asking questions, giving instructions, and providing guidance. Instruct your agents in the system prompt on how to communicate effectively.
- The terminal interface is real-time streamed and interactive. You can stop and intervene at any point. If you see your agent heading in the wrong direction, just stop and tell it right away.
- There is a lot of freedom in this framework. You can instruct your agents to regularly report back to superiors asking for permission to continue. You can instruct them to use point-scoring systems when deciding when to delegate subtasks. Superiors can double-check subordinates' results and dispute. The possibilities are endless.

![Agent Zero](docs/splash_wide.png)

## Nice features to have
- Output is very clean, colorful, readable and interactive; nothing is hidden.
- The same colorful output you see in the terminal is automatically saved to HTML file in **logs/** folder for every session.
- Agent output is streamed in real-time, allowing the user to read along and intervene at any time.
- No coding is required, only prompting and communication skills.
- With a solid system prompt, the framework is reliable even with small models, including precise tool usage.

## Keep in mind
1. **Agent Zero can be dangerous!**
With proper instruction, Agent Zero is capable of many things, even potentially dangerous to your computer, data, or accounts. Always run Agent Zero in an isolated environment (like the built in docker container) and be careful what you wish for.

2. **Agent Zero is not pre-programmed; it is prompt-based.**
The whole framework contains only a minimal amount of code and does not guide the agent in any way.
Everything lies in the system prompt in the **prompts/** folder. Here you can rewrite the whole framework behavior to your needs.
If your agent fails to communicate properly, use tools, reason, use memory, find answers - just instruct it better.

3. **If you cannot provide the ideal environment, let your agent know.**
Agent Zero is made to be used in an isolated virtual environment (for safety) with some tools preinstalled and configured.
If you cannot provide all the necessary conditions or API keys, just change the system prompt and tell your agent what operating system and tools are at its disposal. Nothing is hard-coded; if you do not tell your agent about a certain tool, it will not know about it and will not try to use it.

[![David Ondrej video](/docs/david_vid.jpg)](https://www.youtube.com/watch?v=_Pionjv4hGc)

## Known problems
1. The system prompt sucks. You can do better. If you do, help me please :)
2. The communication between agent and terminal in docker container via SSH can sometimes break and stop producing outputs. Sometimes it is because the agent runs something like "server.serve_forever()" which causes the terminal to hang, sometimes a random error can occur. Restarting the agent and/or the docker container helps.
3. The agent can break his operating system. Sometimes the agent can deactivate virtual environment, uninstall packages, change config etc. Again, removing the docker container and cleaning up the **work_dir/** is enough to fix that.

## Ideal environment
- **Docker container**: The perfect environment to run Agent Zero is the built-in docker container. The agent can download the image **frdel/agent-zero-exe** on its own and start the container, you only need to have docker running (like the Docker Desktop application).
- **Python**: Python has to be installed on the system to run the framework.
- **Internet access**: The agent will need internet access to use its online knowledge tool and execute commands and scripts requiring a connection. If you do not need your agent to be online, you can alter its prompts in the **prompts/** folder and make it fully local.

![Time example](docs/time_example.jpg)

## Setup

Update: [Guide by CheezChat for Windows](./docs/win_installation_guide.txt)

1. **Required API keys:**
- At the moment, the only recommended API key is for https://www.perplexity.ai/ API. Perplexity is used as a convenient web search tool and has not yet been replaced by an open-source alternative. If you do not have an API key for Perplexity, leave it empty in the .env file and Perplexity will not be used.
- Chat models and embedding models can be executed locally via Ollama and HuggingFace or via API as well.

2. **Enter your API keys:**
- You can enter your API keys into the **.env** file, which you can copy from **example.env**
- Or you can export your API keys in the terminal session:
~~~bash
export API_KEY_PERPLEXITY="your-api-key-here"
export API_KEY_OPENAI="your-api-key-here"
~~~

3. **Install dependencies with the following terminal command:**
~~~bash
pip install -r requirements.txt
~~~

3. **Choose your chat, utility and embeddings model:**
- In the **main.py** file, right at the start of the **chat()** function, you can see how the chat model and embedding model are set.
- You can choose between online models (OpenAI, Anthropic, Groq) or offline (Ollama, HuggingFace) for both.

4. **run Docker:**
- Easiest way is to install Docker Desktop application and just run it. The rest will be handled by the framework itself.

## Run the program
- Just run the **main.py** file in Python:
~~~bash
python main.py
~~~
- Or run it in debug mode in VS Code using the **debug** button in the top right corner of the editor. I have provided config files for VS Code for this purpose.

# Comprehensive NVIDIA Docker Setup on Ubuntu WSL2

This guide provides detailed instructions for setting up NVIDIA Docker on Ubuntu within WSL2 (Windows Subsystem for Linux 2). It's designed for users of all experience levels, including those new to Ubuntu and Docker.

## Prerequisites

Before you start, ensure you have:

- **Windows 11** with WSL2 enabled
- An **NVIDIA GPU** with the latest drivers installed on Windows
- **Docker Desktop** installed and configured to use WSL2

## Installation Steps

### 1. Install Ubuntu on WSL2

Install Ubuntu from the Microsoft Store if you haven't already.

### 2. Install NVIDIA Docker Toolkit

#### Add NVIDIA Docker Repository Key

```bash
sudo curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
```

#### Set Up the CUDA Repository

Due to issues with the 22.04 repository, use the 18.04 repository:

```bash
echo "deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64 /" | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

#### Update and Install NVIDIA Docker

```bash
sudo apt-get update
sudo apt-get install -y nvidia-docker2
```

#### Restart Docker Daemon

```bash
sudo systemctl restart docker
```

#### Add User to Docker Group

```bash
sudo usermod -aG docker $USER
```

Log out and back in for this change to take effect.

### 3. Configure Docker for NVIDIA Runtime

Edit the Docker daemon configuration:

```bash
sudo nano /etc/docker/daemon.json
```

Add or update the following configuration:

```json
{
  "default-runtime": "nvidia",
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  }
}
```

Save and exit, then restart Docker:

```bash
sudo systemctl restart docker
```

### 4. Verify NVIDIA Docker Installation

Run this command to check if NVIDIA Docker is set up correctly:

```bash
docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

You should see information about your NVIDIA GPU.

### 5. Enable Docker to Start on Boot

```bash
sudo systemctl enable docker
```

### 6. Ensure Persistence of NVIDIA Runtime

To make sure the NVIDIA runtime remains the default across restarts:

#### Create a Startup Script

```bash
nano ~/.docker_startup.sh
```

Add the following content:

```bash
#!/bin/bash
sudo ln -sf /usr/libexec/docker/cli-plugins/docker-buildx /usr/local/lib/docker/cli-plugins/
sudo ln -sf /usr/libexec/docker/cli-plugins/docker-compose /usr/local/lib/docker/cli-plugins/
sudo service docker restart
```

#### Make the Script Executable

```bash
chmod +x ~/.docker_startup.sh
```

#### Edit .bashrc

```bash
nano ~/.bashrc
```

Add this line at the end:

```bash
[ -f ~/.docker_startup.sh ] && ~/.docker_startup.sh
```

### 7. Configure WSL2 for Systemd

Edit or create the WSL configuration file:

```bash
sudo nano /etc/wsl.conf
```

Add the following content:

```
[boot]
systemd=true
```

Save and exit the editor.

### 8. Configure Docker Desktop

To prevent Docker Desktop from overriding your WSL2 settings:

1. Open Docker Desktop settings
2. Go to the "Docker Engine" configuration tab
3. Update the configuration to include:

```json
{
  "builder": {
    "gc": {
      "defaultKeepStorage": "20GB",
      "enabled": true
    }
  },
  "experimental": false,
  "runtimes": {
    "nvidia": {
      "path": "/usr/bin/nvidia-container-runtime",
      "runtimeArgs": []
    }
  },
  "default-runtime": "nvidia"
}
```

4. Apply changes and restart Docker Desktop

## Verifying Setup

After completing all steps:

1. Restart your WSL2 instance:
   In a Windows PowerShell (run as administrator):
   ```
   wsl --shutdown
   ```
   Then reopen your Ubuntu terminal.

2. Check Docker runtime:
   ```bash
   docker info | grep -i runtime
   ```

3. Verify NVIDIA is still the default runtime and no warnings appear about missing plugins.

## Troubleshooting

If you encounter issues during setup or operation, follow these steps:

### 1. Check Current Docker Runtime Settings

```bash
docker info | grep -i runtime
```

Ensure `nvidia` is listed as a runtime and set as default.

### 2. Reinstall NVIDIA Docker Components

If runtime settings are incorrect, try reinstalling:

```bash
sudo apt-get remove --purge docker-ce docker-ce-cli containerd.io nvidia-docker2
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io nvidia-docker2
```

### 3. Update Docker Daemon Configuration

Ensure `/etc/docker/daemon.json` contains the correct configuration as shown in step 3 of the installation process.

### 4. Restart Docker and Verify

After making changes:

```bash
sudo systemctl restart docker
docker info | grep -i runtime
```

### 5. Test GPU Access

```bash
docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

### 6. Resolving Plugin Warnings

If you see warnings about missing Docker plugins:

```bash
ls -l /usr/local/lib/docker/cli-plugins/
ls -l /usr/libexec/docker/cli-plugins/
```

If the symlinks are incorrect or missing, recreate them:

```bash
sudo rm /usr/local/lib/docker/cli-plugins/docker-buildx
sudo rm /usr/local/lib/docker/cli-plugins/docker-compose
sudo ln -s /usr/libexec/docker/cli-plugins/docker-buildx /usr/local/lib/docker/cli-plugins/
sudo ln -s /usr/libexec/docker/cli-plugins/docker-compose /usr/local/lib/docker/cli-plugins/
```

### 7. Docker Desktop and WSL2 Sync Issues

If Docker Desktop isn't syncing properly with Ubuntu WSL2:

1. Verify WSL2 integration is enabled in Docker Desktop settings.
2. Restart Docker Desktop and WSL2 (`wsl --shutdown` in Windows PowerShell).
3. Ensure Docker Desktop is set to use the WSL2 backend.
4. Disable any local Docker distributions, leaving only the Ubuntu WSL2 distribution enabled.
5. Check file sharing permissions for accessed directories.
6. Consider reinstalling Docker Desktop if issues persist.

## Additional Resources

- [NVIDIA Docker Documentation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html)
- [Docker Documentation](https://docs.docker.com/)
- [Microsoft WSL2 Documentation](https://docs.microsoft.com/en-us/windows/wsl/)

Remember to always check for the latest updates and best practices in the official documentation.



---
File: /requirements.txt
---

ansio==0.0.1
python-dotenv==1.0.1
langchain-groq==0.1.6
langchain-huggingface==0.0.3
langchain-openai==0.1.15
langchain-community==0.2.7
langchain-anthropic==0.1.19
langchain-chroma==0.1.2
langchain-google-genai==1.0.7
webcolors==24.6.0
sentence-transformers==3.0.1
docker==7.1.0
paramiko==3.4.0
duckduckgo_search==6.1.12
inputimeout==1.0.4

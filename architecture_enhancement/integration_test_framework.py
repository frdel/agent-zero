"""
æ¶æ„å¢å¼ºé›†æˆæµ‹è¯•æ¡†æ¶
éªŒè¯æ‰€æœ‰æ–°ç»„ä»¶çš„é›†æˆæ•ˆæœå’Œæ€§èƒ½è¡¨ç°
"""

import asyncio
import time
import json
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum

# å¯¼å…¥æ–°æ¶æ„ç»„ä»¶
from .universal_framework import (
    get_plugin_manager, get_config_abstractor, get_task_planner,
    UniversalAgentConfig, TaskType, DomainType
)
from .intelligence_framework import (
    get_reasoning_engine, get_context_engine,
    DecisionContext, ReasoningType, ContextType
)
from .learning_framework import (
    get_learning_engine, get_performance_evaluator,
    LearningEvent, LearningType
)
from .scalability_framework import (
    get_version_manager, get_debt_manager, get_scalability_analyzer,
    ComponentVersion, ComponentType, TechnicalDebt
)


class TestType(Enum):
    """æµ‹è¯•ç±»å‹"""
    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    STRESS = "stress"
    COMPATIBILITY = "compatibility"


class TestStatus(Enum):
    """æµ‹è¯•çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    test_id: str
    test_name: str
    test_type: TestType
    status: TestStatus
    execution_time: float
    details: Dict[str, Any] = field(default_factory=dict)
    error_message: str = ""
    timestamp: datetime = field(default_factory=datetime.now)


class ArchitectureIntegrationTester:
    """æ¶æ„é›†æˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results: List[TestResult] = []
        self.test_suite: Dict[str, List[str]] = {}
        self.performance_baselines: Dict[str, float] = {}
        self._initialize_test_suite()
    
    def _initialize_test_suite(self):
        """åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶"""
        self.test_suite = {
            "universal_framework": [
                "test_plugin_manager_registration",
                "test_config_abstractor_templates",
                "test_task_planner_analysis",
                "test_capability_discovery"
            ],
            "intelligence_framework": [
                "test_reasoning_engine_decisions",
                "test_context_understanding",
                "test_multi_modal_reasoning",
                "test_decision_explanation"
            ],
            "learning_framework": [
                "test_adaptive_learning",
                "test_knowledge_graph_building",
                "test_user_profile_updates",
                "test_performance_evaluation"
            ],
            "scalability_framework": [
                "test_version_management",
                "test_debt_tracking",
                "test_scalability_analysis",
                "test_compatibility_checking"
            ],
            "integration_tests": [
                "test_end_to_end_workflow",
                "test_cross_component_communication",
                "test_data_flow_integrity",
                "test_error_handling_cascade"
            ],
            "performance_tests": [
                "test_response_time_benchmarks",
                "test_memory_usage_optimization",
                "test_concurrent_request_handling",
                "test_scalability_limits"
            ]
        }
    
    async def run_full_test_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶"""
        print("ğŸš€ å¼€å§‹æ¶æ„å¢å¼ºé›†æˆæµ‹è¯•...")
        start_time = time.time()
        
        all_results = {}
        
        # è¿è¡Œå„ä¸ªæ¨¡å—çš„æµ‹è¯•
        for module_name, test_methods in self.test_suite.items():
            print(f"\nğŸ“‹ æµ‹è¯•æ¨¡å—: {module_name}")
            module_results = await self._run_module_tests(module_name, test_methods)
            all_results[module_name] = module_results
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        total_time = time.time() - start_time
        report = await self._generate_comprehensive_report(all_results, total_time)
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        return report
    
    async def _run_module_tests(self, module_name: str, test_methods: List[str]) -> List[TestResult]:
        """è¿è¡Œæ¨¡å—æµ‹è¯•"""
        results = []
        
        for test_method in test_methods:
            print(f"  ğŸ” æ‰§è¡Œæµ‹è¯•: {test_method}")
            
            try:
                # åŠ¨æ€è°ƒç”¨æµ‹è¯•æ–¹æ³•
                if hasattr(self, test_method):
                    result = await getattr(self, test_method)()
                    results.append(result)
                    
                    status_icon = "âœ…" if result.status == TestStatus.PASSED else "âŒ"
                    print(f"    {status_icon} {result.test_name}: {result.status.value} ({result.execution_time:.3f}s)")
                else:
                    # åˆ›å»ºè·³è¿‡çš„æµ‹è¯•ç»“æœ
                    result = TestResult(
                        test_id=f"{module_name}_{test_method}",
                        test_name=test_method,
                        test_type=TestType.UNIT,
                        status=TestStatus.SKIPPED,
                        execution_time=0,
                        error_message="Test method not implemented"
                    )
                    results.append(result)
                    print(f"    â­ï¸  {test_method}: SKIPPED (æœªå®ç°)")
                    
            except Exception as e:
                result = TestResult(
                    test_id=f"{module_name}_{test_method}",
                    test_name=test_method,
                    test_type=TestType.UNIT,
                    status=TestStatus.FAILED,
                    execution_time=0,
                    error_message=str(e)
                )
                results.append(result)
                print(f"    âŒ {test_method}: FAILED - {str(e)}")
        
        self.test_results.extend(results)
        return results
    
    # ==================== é€šç”¨æ¡†æ¶æµ‹è¯• ====================
    
    async def test_plugin_manager_registration(self) -> TestResult:
        """æµ‹è¯•æ’ä»¶ç®¡ç†å™¨æ³¨å†ŒåŠŸèƒ½"""
        start_time = time.time()
        
        try:
            plugin_manager = get_plugin_manager()
            
            # æ¨¡æ‹Ÿæ’ä»¶æ³¨å†Œ
            class MockPlugin:
                @property
                def name(self) -> str:
                    return "test_plugin"
                
                @property
                def version(self) -> str:
                    return "1.0.0"
                
                @property
                def capabilities(self):
                    return []
                
                async def initialize(self, context):
                    return True
                
                async def execute(self, action, params):
                    return {"result": "success"}
                
                async def cleanup(self):
                    pass
            
            mock_plugin = MockPlugin()
            success = await plugin_manager.register_plugin(mock_plugin)
            
            execution_time = time.time() - start_time
            
            return TestResult(
                test_id="universal_plugin_registration",
                test_name="Plugin Manager Registration",
                test_type=TestType.UNIT,
                status=TestStatus.PASSED if success else TestStatus.FAILED,
                execution_time=execution_time,
                details={"registered_plugins": len(plugin_manager._plugins)}
            )
            
        except Exception as e:
            return TestResult(
                test_id="universal_plugin_registration",
                test_name="Plugin Manager Registration",
                test_type=TestType.UNIT,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    async def test_config_abstractor_templates(self) -> TestResult:
        """æµ‹è¯•é…ç½®æŠ½è±¡å™¨æ¨¡æ¿åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            config_abstractor = get_config_abstractor()
            
            # æµ‹è¯•é…ç½®æ¨¡æ¿è·å–
            variables = {
                "DOCKER_HOST": "localhost",
                "SSH_PORT": "22",
                "HTTP_PORT": "80"
            }
            
            config = config_abstractor.get_config_for_environment("docker_remote", variables)
            
            execution_time = time.time() - start_time
            
            # éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®æ›¿æ¢å˜é‡
            success = (
                "localhost" in str(config) and
                "22" in str(config) and
                "80" in str(config)
            )
            
            return TestResult(
                test_id="universal_config_templates",
                test_name="Config Abstractor Templates",
                test_type=TestType.UNIT,
                status=TestStatus.PASSED if success else TestStatus.FAILED,
                execution_time=execution_time,
                details={"config_keys": list(config.keys()) if isinstance(config, dict) else []}
            )
            
        except Exception as e:
            return TestResult(
                test_id="universal_config_templates",
                test_name="Config Abstractor Templates",
                test_type=TestType.UNIT,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    async def test_task_planner_analysis(self) -> TestResult:
        """æµ‹è¯•ä»»åŠ¡è§„åˆ’å™¨åˆ†æåŠŸèƒ½"""
        start_time = time.time()
        
        try:
            task_planner = get_task_planner()
            
            # æµ‹è¯•ä»»åŠ¡è§„åˆ’
            task_description = "Create a Python script to analyze data"
            plan = await task_planner.plan_task(
                task_description, 
                TaskType.DEVELOPMENT, 
                DomainType.TECHNICAL
            )
            
            execution_time = time.time() - start_time
            
            # éªŒè¯è§„åˆ’ç»“æœ
            success = (
                "task_id" in plan and
                "execution_steps" in plan and
                len(plan["execution_steps"]) > 0
            )
            
            return TestResult(
                test_id="universal_task_planning",
                test_name="Task Planner Analysis",
                test_type=TestType.UNIT,
                status=TestStatus.PASSED if success else TestStatus.FAILED,
                execution_time=execution_time,
                details={
                    "plan_keys": list(plan.keys()),
                    "steps_count": len(plan.get("execution_steps", []))
                }
            )
            
        except Exception as e:
            return TestResult(
                test_id="universal_task_planning",
                test_name="Task Planner Analysis",
                test_type=TestType.UNIT,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    async def test_capability_discovery(self) -> TestResult:
        """æµ‹è¯•èƒ½åŠ›å‘ç°åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            plugin_manager = get_plugin_manager()
            
            # æµ‹è¯•èƒ½åŠ›å‘ç°
            capabilities = plugin_manager.get_available_capabilities()
            
            execution_time = time.time() - start_time
            
            return TestResult(
                test_id="universal_capability_discovery",
                test_name="Capability Discovery",
                test_type=TestType.UNIT,
                status=TestStatus.PASSED,
                execution_time=execution_time,
                details={"capabilities_count": len(capabilities)}
            )
            
        except Exception as e:
            return TestResult(
                test_id="universal_capability_discovery",
                test_name="Capability Discovery",
                test_type=TestType.UNIT,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    # ==================== æ™ºèƒ½æ¡†æ¶æµ‹è¯• ====================
    
    async def test_reasoning_engine_decisions(self) -> TestResult:
        """æµ‹è¯•æ¨ç†å¼•æ“å†³ç­–åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            reasoning_engine = get_reasoning_engine()
            
            # åˆ›å»ºå†³ç­–ä¸Šä¸‹æ–‡
            context = DecisionContext(
                task_description="Analyze user data and generate report",
                available_tools=["data_query", "statistical_analysis", "report_generator"],
                current_state={"data_available": True, "user_permissions": "read"},
                objectives=["accuracy", "speed"],
                constraints=["privacy_compliance"]
            )
            
            # æ‰§è¡Œæ¨ç†
            decision = await reasoning_engine.reason(context)
            
            execution_time = time.time() - start_time
            
            # éªŒè¯å†³ç­–ç»“æœ
            success = (
                decision.chosen_action and
                len(decision.reasoning_chain) > 0 and
                0 <= decision.confidence_score <= 1
            )
            
            return TestResult(
                test_id="intelligence_reasoning_decisions",
                test_name="Reasoning Engine Decisions",
                test_type=TestType.UNIT,
                status=TestStatus.PASSED if success else TestStatus.FAILED,
                execution_time=execution_time,
                details={
                    "chosen_action": decision.chosen_action,
                    "confidence": decision.confidence_score,
                    "reasoning_steps": len(decision.reasoning_chain)
                }
            )
            
        except Exception as e:
            return TestResult(
                test_id="intelligence_reasoning_decisions",
                test_name="Reasoning Engine Decisions",
                test_type=TestType.UNIT,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    async def test_context_understanding(self) -> TestResult:
        """æµ‹è¯•ä¸Šä¸‹æ–‡ç†è§£åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            context_engine = get_context_engine()
            
            # æµ‹è¯•ä¸Šä¸‹æ–‡åˆ†æ
            user_input = "Help me create a Python script for data analysis"
            session_id = "test_session_123"
            
            analysis = await context_engine.analyze_context(user_input, session_id)
            
            execution_time = time.time() - start_time
            
            # éªŒè¯åˆ†æç»“æœ
            success = (
                "context_analysis" in analysis and
                "comprehensive_understanding" in analysis and
                "context_confidence" in analysis
            )
            
            return TestResult(
                test_id="intelligence_context_understanding",
                test_name="Context Understanding",
                test_type=TestType.UNIT,
                status=TestStatus.PASSED if success else TestStatus.FAILED,
                execution_time=execution_time,
                details={
                    "confidence": analysis.get("context_confidence", 0),
                    "intent": analysis.get("comprehensive_understanding", {}).get("primary_intent", "unknown")
                }
            )
            
        except Exception as e:
            return TestResult(
                test_id="intelligence_context_understanding",
                test_name="Context Understanding",
                test_type=TestType.UNIT,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    # ==================== å­¦ä¹ æ¡†æ¶æµ‹è¯• ====================
    
    async def test_adaptive_learning(self) -> TestResult:
        """æµ‹è¯•è‡ªé€‚åº”å­¦ä¹ åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            learning_engine = get_learning_engine()
            
            # åˆ›å»ºå­¦ä¹ äº‹ä»¶
            event = LearningEvent(
                event_id="test_event_001",
                event_type="tool_usage",
                input_data={"tool_name": "data_analysis", "parameters": {"format": "csv"}},
                output_data={"result": "success", "response_time": 2.5},
                success=True,
                context={"user_id": "test_user", "task_type": "analysis"}
            )
            
            # æ‰§è¡Œå­¦ä¹ 
            success = await learning_engine.learn_from_interaction(event)
            
            execution_time = time.time() - start_time
            
            return TestResult(
                test_id="learning_adaptive_learning",
                test_name="Adaptive Learning",
                test_type=TestType.UNIT,
                status=TestStatus.PASSED if success else TestStatus.FAILED,
                execution_time=execution_time,
                details={
                    "knowledge_nodes": len(learning_engine.knowledge_graph),
                    "user_profiles": len(learning_engine.user_profiles)
                }
            )
            
        except Exception as e:
            return TestResult(
                test_id="learning_adaptive_learning",
                test_name="Adaptive Learning",
                test_type=TestType.UNIT,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    # ==================== å¯æ‰©å±•æ€§æ¡†æ¶æµ‹è¯• ====================
    
    async def test_version_management(self) -> TestResult:
        """æµ‹è¯•ç‰ˆæœ¬ç®¡ç†åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            version_manager = get_version_manager()
            
            # åˆ›å»ºç»„ä»¶ç‰ˆæœ¬
            component = ComponentVersion(
                component_id="test_component",
                version="1.0.0",
                component_type=ComponentType.CORE,
                dependencies={"python": ">=3.8"},
                new_features=["feature1", "feature2"]
            )
            
            # æ³¨å†Œç»„ä»¶
            success = await version_manager.register_component(component)
            
            execution_time = time.time() - start_time
            
            return TestResult(
                test_id="scalability_version_management",
                test_name="Version Management",
                test_type=TestType.UNIT,
                status=TestStatus.PASSED if success else TestStatus.FAILED,
                execution_time=execution_time,
                details={"registered_components": len(version_manager.component_registry)}
            )
            
        except Exception as e:
            return TestResult(
                test_id="scalability_version_management",
                test_name="Version Management",
                test_type=TestType.UNIT,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    # ==================== é›†æˆæµ‹è¯• ====================
    
    async def test_end_to_end_workflow(self) -> TestResult:
        """æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµ"""
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿå®Œæ•´çš„å·¥ä½œæµç¨‹
            # 1. ä»»åŠ¡è§„åˆ’
            task_planner = get_task_planner()
            plan = await task_planner.plan_task(
                "Create a data analysis report",
                TaskType.DATA_ANALYSIS,
                DomainType.TECHNICAL
            )
            
            # 2. æ¨ç†å†³ç­–
            reasoning_engine = get_reasoning_engine()
            context = DecisionContext(
                task_description="Create a data analysis report",
                available_tools=["data_query", "analysis_tool"],
                current_state={}
            )
            decision = await reasoning_engine.reason(context)
            
            # 3. å­¦ä¹ è®°å½•
            learning_engine = get_learning_engine()
            event = LearningEvent(
                event_id="e2e_test",
                event_type="workflow",
                input_data={"plan": plan},
                output_data={"decision": decision.chosen_action},
                success=True
            )
            await learning_engine.learn_from_interaction(event)
            
            execution_time = time.time() - start_time
            
            return TestResult(
                test_id="integration_end_to_end",
                test_name="End-to-End Workflow",
                test_type=TestType.INTEGRATION,
                status=TestStatus.PASSED,
                execution_time=execution_time,
                details={
                    "workflow_steps": ["planning", "reasoning", "learning"],
                    "total_components": 3
                }
            )
            
        except Exception as e:
            return TestResult(
                test_id="integration_end_to_end",
                test_name="End-to-End Workflow",
                test_type=TestType.INTEGRATION,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    # ==================== æ€§èƒ½æµ‹è¯• ====================
    
    async def test_response_time_benchmarks(self) -> TestResult:
        """æµ‹è¯•å“åº”æ—¶é—´åŸºå‡†"""
        start_time = time.time()
        
        try:
            # æµ‹è¯•å„ç»„ä»¶çš„å“åº”æ—¶é—´
            response_times = {}
            
            # æµ‹è¯•æ¨ç†å¼•æ“
            reasoning_start = time.time()
            reasoning_engine = get_reasoning_engine()
            context = DecisionContext(
                task_description="Simple task",
                available_tools=["tool1"],
                current_state={}
            )
            await reasoning_engine.reason(context)
            response_times["reasoning_engine"] = time.time() - reasoning_start
            
            # æµ‹è¯•ä¸Šä¸‹æ–‡å¼•æ“
            context_start = time.time()
            context_engine = get_context_engine()
            await context_engine.analyze_context("test input", "session1")
            response_times["context_engine"] = time.time() - context_start
            
            execution_time = time.time() - start_time
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ€§èƒ½è¦æ±‚ï¼ˆ< 5ç§’ï¼‰
            max_response_time = max(response_times.values())
            success = max_response_time < 5.0
            
            return TestResult(
                test_id="performance_response_time",
                test_name="Response Time Benchmarks",
                test_type=TestType.PERFORMANCE,
                status=TestStatus.PASSED if success else TestStatus.FAILED,
                execution_time=execution_time,
                details={
                    "response_times": response_times,
                    "max_response_time": max_response_time
                }
            )
            
        except Exception as e:
            return TestResult(
                test_id="performance_response_time",
                test_name="Response Time Benchmarks",
                test_type=TestType.PERFORMANCE,
                status=TestStatus.FAILED,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    async def _generate_comprehensive_report(self, all_results: Dict[str, List[TestResult]], total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        
        # ç»Ÿè®¡æ€»ä½“ç»“æœ
        total_tests = sum(len(results) for results in all_results.values())
        passed_tests = sum(
            len([r for r in results if r.status == TestStatus.PASSED])
            for results in all_results.values()
        )
        failed_tests = sum(
            len([r for r in results if r.status == TestStatus.FAILED])
            for results in all_results.values()
        )
        skipped_tests = sum(
            len([r for r in results if r.status == TestStatus.SKIPPED])
            for results in all_results.values()
        )
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        # æ€§èƒ½ç»Ÿè®¡
        performance_results = [r for results in all_results.values() for r in results if r.test_type == TestType.PERFORMANCE]
        avg_response_time = sum(r.execution_time for r in performance_results) / len(performance_results) if performance_results else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "summary": {
                "total_tests": total_tests,
                "passed": passed_tests,
                "failed": failed_tests,
                "skipped": skipped_tests,
                "success_rate": f"{success_rate:.1f}%",
                "total_execution_time": f"{total_time:.2f}s"
            },
            "module_results": {},
            "performance_metrics": {
                "average_response_time": f"{avg_response_time:.3f}s",
                "performance_tests_count": len(performance_results)
            },
            "failed_tests": [],
            "recommendations": []
        }
        
        # æ¨¡å—ç»“æœç»Ÿè®¡
        for module_name, results in all_results.items():
            module_passed = len([r for r in results if r.status == TestStatus.PASSED])
            module_total = len(results)
            module_success_rate = (module_passed / module_total * 100) if module_total > 0 else 0
            
            report["module_results"][module_name] = {
                "passed": module_passed,
                "total": module_total,
                "success_rate": f"{module_success_rate:.1f}%"
            }
        
        # å¤±è´¥æµ‹è¯•è¯¦æƒ…
        for results in all_results.values():
            for result in results:
                if result.status == TestStatus.FAILED:
                    report["failed_tests"].append({
                        "test_name": result.test_name,
                        "error": result.error_message,
                        "execution_time": result.execution_time
                    })
        
        # ç”Ÿæˆå»ºè®®
        if success_rate < 80:
            report["recommendations"].append("æ€»ä½“æˆåŠŸç‡åä½ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        
        if avg_response_time > 3.0:
            report["recommendations"].append("å¹³å‡å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½")
        
        if failed_tests > 0:
            report["recommendations"].append(f"æœ‰{failed_tests}ä¸ªæµ‹è¯•å¤±è´¥ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤")
        
        return report


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = ArchitectureIntegrationTester()
    report = await tester.run_full_test_suite()
    
    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    with open("architecture_test_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
    print("="*60)
    print(f"æ€»æµ‹è¯•æ•°: {report['summary']['total_tests']}")
    print(f"é€šè¿‡: {report['summary']['passed']}")
    print(f"å¤±è´¥: {report['summary']['failed']}")
    print(f"è·³è¿‡: {report['summary']['skipped']}")
    print(f"æˆåŠŸç‡: {report['summary']['success_rate']}")
    print(f"æ€»è€—æ—¶: {report['summary']['total_execution_time']}")
    
    if report['failed_tests']:
        print(f"\nâŒ å¤±è´¥æµ‹è¯•:")
        for failed_test in report['failed_tests']:
            print(f"  - {failed_test['test_name']}: {failed_test['error']}")
    
    if report['recommendations']:
        print(f"\nğŸ’¡ å»ºè®®:")
        for rec in report['recommendations']:
            print(f"  - {rec}")
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: architecture_test_report.json")


if __name__ == "__main__":
    asyncio.run(main())

{
  "recommended_models": {
    "chat_models": [
      {
        "name": "llama-3.1-8b-instruct",
        "description": "Meta Llama 3.1 8B Instruct - Excellent balance of performance and resource usage",
        "size": "8B",
        "vram_required": "6-8GB",
        "use_case": "General conversation, coding, reasoning",
        "download_url": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct-GGUF",
        "recommended_quant": "Q4_K_M"
      },
      {
        "name": "llama-3.1-70b-instruct",
        "description": "Meta Llama 3.1 70B Instruct - High performance for complex tasks",
        "size": "70B",
        "vram_required": "40-48GB",
        "use_case": "Complex reasoning, advanced coding, research",
        "download_url": "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct-GGUF",
        "recommended_quant": "Q4_K_M"
      },
      {
        "name": "qwen2.5-7b-instruct",
        "description": "Qwen2.5 7B Instruct - Great for coding and multilingual tasks",
        "size": "7B",
        "vram_required": "5-7GB",
        "use_case": "Coding, multilingual support, technical tasks",
        "download_url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF",
        "recommended_quant": "Q4_K_M"
      },
      {
        "name": "mistral-7b-instruct-v0.3",
        "description": "Mistral 7B Instruct v0.3 - Fast and efficient for general tasks",
        "size": "7B",
        "vram_required": "5-7GB",
        "use_case": "General conversation, quick responses",
        "download_url": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3-GGUF",
        "recommended_quant": "Q4_K_M"
      }
    ],
    "embedding_models": [
      {
        "name": "nomic-embed-text-v1.5",
        "description": "Nomic Embed Text v1.5 - High quality embeddings for knowledge retrieval",
        "size": "137M",
        "vram_required": "1-2GB",
        "use_case": "Document embeddings, semantic search",
        "download_url": "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF",
        "recommended_quant": "F16"
      },
      {
        "name": "bge-large-en-v1.5",
        "description": "BGE Large EN v1.5 - Excellent English embeddings",
        "size": "335M",
        "vram_required": "2-3GB",
        "use_case": "English document embeddings, high accuracy",
        "download_url": "https://huggingface.co/BAAI/bge-large-en-v1.5-GGUF",
        "recommended_quant": "F16"
      }
    ]
  },
  "configurations": {
    "low_resource": {
      "description": "For systems with 8GB VRAM or less",
      "chat_model": "mistral-7b-instruct-v0.3",
      "utility_model": "mistral-7b-instruct-v0.3",
      "embedding_model": "nomic-embed-text-v1.5",
      "browser_model": "mistral-7b-instruct-v0.3",
      "settings": {
        "context_length": 16384,
        "temperature": 0.7,
        "max_tokens": 2048
      }
    },
    "balanced": {
      "description": "For systems with 12-16GB VRAM",
      "chat_model": "llama-3.1-8b-instruct",
      "utility_model": "qwen2.5-7b-instruct",
      "embedding_model": "bge-large-en-v1.5",
      "browser_model": "llama-3.1-8b-instruct",
      "settings": {
        "context_length": 32768,
        "temperature": 0.7,
        "max_tokens": 4096
      }
    },
    "high_performance": {
      "description": "For systems with 48GB+ VRAM",
      "chat_model": "llama-3.1-70b-instruct",
      "utility_model": "llama-3.1-8b-instruct",
      "embedding_model": "bge-large-en-v1.5",
      "browser_model": "llama-3.1-70b-instruct",
      "settings": {
        "context_length": 65536,
        "temperature": 0.7,
        "max_tokens": 8192
      }
    }
  },
  "setup_instructions": {
    "steps": [
      "1. Install LM Studio from https://lmstudio.ai/",
      "2. Download recommended models based on your system configuration",
      "3. Start LM Studio server on port 1234",
      "4. Copy lmstudio.env to your Agent Zero root as .env",
      "5. Update model names in .env to match your downloaded models",
      "6. Run: docker-compose -f docker/lmstudio/docker-compose.yml up -d",
      "7. Access Agent Zero at http://localhost:50080"
    ],
    "troubleshooting": {
      "connection_issues": "Ensure LM Studio is running and accessible on port 1234",
      "model_not_found": "Check that model names in .env match exactly with LM Studio",
      "performance_issues": "Consider using smaller models or adjusting context length",
      "memory_issues": "Reduce model size or use quantized versions"
    }
  }
}

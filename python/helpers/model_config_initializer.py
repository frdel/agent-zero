"""
æ¨¡å‹é…ç½®åˆå§‹åŒ–å™¨
è‡ªåŠ¨é…ç½®å’Œæ³¨å†Œå„ç§æ¨¡å‹æä¾›å•†
"""

import os
import json
from typing import Dict, Any, List
from python.helpers import dotenv

from .enhanced_model_manager import (
    get_model_manager, ModelConfig, ModelProvider, ModelCapability, TaskType
)


class ModelConfigInitializer:
    """æ¨¡å‹é…ç½®åˆå§‹åŒ–å™¨"""
    
    def __init__(self):
        self.model_manager = get_model_manager()
        self.config_loaded = False
    
    def initialize_from_env(self) -> bool:
        """ä»ç¯å¢ƒå˜é‡åˆå§‹åŒ–æ¨¡å‹é…ç½®"""
        try:
            print("ğŸš€ å¼€å§‹ä»ç¯å¢ƒå˜é‡åˆå§‹åŒ–æ¨¡å‹é…ç½®...")
            
            # åŠ è½½ç¯å¢ƒå˜é‡
            dotenv.load_dotenv()
            
            # åˆå§‹åŒ–å„ç§æ¨¡å‹æä¾›å•†
            self._init_openai_compatible_models()
            self._init_vllm_models()
            self._init_llamacpp_models()
            self._init_default_models()
            
            # ä¿å­˜é…ç½®
            self._save_model_config()
            
            self.config_loaded = True
            print("âœ… æ¨¡å‹é…ç½®åˆå§‹åŒ–å®Œæˆ")
            
            # æ‰“å°é…ç½®æ‘˜è¦
            self._print_config_summary()
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹é…ç½®åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _init_openai_compatible_models(self):
        """åˆå§‹åŒ–OpenAIå…¼å®¹æ¨¡å‹"""
        # æ£€æŸ¥OpenAIå…¼å®¹ç«¯ç‚¹
        openai_endpoint = os.getenv("OPENAI_ENDPOINT")
        openai_api_key = os.getenv("OPENAI_ENDPOINT_API_KEY")
        
        if openai_endpoint and openai_api_key:
            print(f"ğŸ”— é…ç½®OpenAIå…¼å®¹ç«¯ç‚¹: {openai_endpoint}")
            
            # é€šç”¨äº‘ç«¯å¤§æ¨¡å‹é…ç½®
            cloud_large_model = ModelConfig(
                provider=ModelProvider.OPENAI_COMPATIBLE,
                name="gpt-4-turbo",  # å¯ä»¥æ ¹æ®å®é™…ç«¯ç‚¹è°ƒæ•´
                endpoint=openai_endpoint,
                api_key=openai_api_key,
                ctx_length=128000,
                capabilities=[
                    ModelCapability.TEXT_GENERATION,
                    ModelCapability.CODE_GENERATION,
                    ModelCapability.REASONING,
                    ModelCapability.CHAT,
                    ModelCapability.FUNCTION_CALLING
                ],
                vision=True,  # å‡è®¾æ”¯æŒè§†è§‰
                function_calling=True,
                speed_score=7,
                quality_score=9,
                cost_score=3,  # äº‘ç«¯æ¨¡å‹æˆæœ¬è¾ƒé«˜
                priority=8
            )
            
            self.model_manager.register_model("cloud_large_model", cloud_large_model)
            
            # å¤šæ¨¡æ€æ¨¡å‹ï¼ˆç”¨äºæµè§ˆå™¨ä»»åŠ¡ï¼‰
            multimodal_model = ModelConfig(
                provider=ModelProvider.OPENAI_COMPATIBLE,
                name="gpt-4-vision-preview",
                endpoint=openai_endpoint,
                api_key=openai_api_key,
                ctx_length=128000,
                capabilities=[
                    ModelCapability.VISION,
                    ModelCapability.MULTIMODAL,
                    ModelCapability.TEXT_GENERATION,
                    ModelCapability.REASONING
                ],
                vision=True,
                speed_score=6,
                quality_score=9,
                cost_score=2,  # å¤šæ¨¡æ€æ¨¡å‹æˆæœ¬æ›´é«˜
                priority=9  # æµè§ˆå™¨ä»»åŠ¡çš„é¦–é€‰
            )
            
            self.model_manager.register_model("multimodal_browser_model", multimodal_model)
    
    def _init_vllm_models(self):
        """åˆå§‹åŒ–vLLMæœ¬åœ°æ¨¡å‹"""
        # æ£€æŸ¥vLLMé…ç½®
        vllm_endpoint = os.getenv("VLLM_ENDPOINT", "http://localhost:8000")
        vllm_model_name = os.getenv("VLLM_MODEL_NAME", "local-model")
        
        print(f"ğŸ  é…ç½®vLLMæœ¬åœ°æ¨¡å‹: {vllm_endpoint}")
        
        # æœ¬åœ°å†™ä½œæ¨¡å‹ï¼ˆå°å‹å¿«é€Ÿï¼‰
        local_writing_model = ModelConfig(
            provider=ModelProvider.VLLM,
            name=vllm_model_name,
            endpoint=vllm_endpoint,
            ctx_length=8192,
            capabilities=[
                ModelCapability.TEXT_GENERATION,
                ModelCapability.CREATIVE_WRITING,
                ModelCapability.CHAT
            ],
            speed_score=9,  # æœ¬åœ°æ¨¡å‹é€Ÿåº¦å¿«
            quality_score=6,  # è´¨é‡ä¸­ç­‰
            cost_score=10,  # æœ¬åœ°æ¨¡å‹æˆæœ¬æœ€ä½
            priority=7,
            kwargs={
                "top_p": 0.9,
                "top_k": 40,
                "repetition_penalty": 1.1
            }
        )
        
        self.model_manager.register_model("local_writing_model", local_writing_model)
        
        # æœ¬åœ°ä»£ç æ¨¡å‹ï¼ˆå¦‚æœæœ‰ä¸“é—¨çš„ä»£ç æ¨¡å‹ï¼‰
        code_model_name = os.getenv("VLLM_CODE_MODEL_NAME")
        if code_model_name:
            local_code_model = ModelConfig(
                provider=ModelProvider.VLLM,
                name=code_model_name,
                endpoint=vllm_endpoint,
                ctx_length=16384,
                capabilities=[
                    ModelCapability.CODE_GENERATION,
                    ModelCapability.TEXT_GENERATION,
                    ModelCapability.REASONING
                ],
                speed_score=8,
                quality_score=7,
                cost_score=10,
                priority=6
            )
            
            self.model_manager.register_model("local_code_model", local_code_model)
    
    def _init_llamacpp_models(self):
        """åˆå§‹åŒ–LlamaCpp GGUFæ¨¡å‹"""
        # æ£€æŸ¥LlamaCppé…ç½®
        llamacpp_endpoint = os.getenv("LLAMACPP_ENDPOINT", "http://localhost:8080")
        llamacpp_model_path = os.getenv("LLAMACPP_MODEL_PATH")
        
        print(f"ğŸ¦™ é…ç½®LlamaCppæ¨¡å‹: {llamacpp_endpoint}")
        
        # æœ¬åœ°GGUFæ¨¡å‹
        local_gguf_model = ModelConfig(
            provider=ModelProvider.LLAMACPP,
            name="local-gguf-model",
            endpoint=llamacpp_endpoint,
            model_path=llamacpp_model_path,
            ctx_length=4096,
            capabilities=[
                ModelCapability.TEXT_GENERATION,
                ModelCapability.CHAT,
                ModelCapability.EMBEDDING
            ],
            speed_score=7,
            quality_score=6,
            cost_score=10,  # æœ¬åœ°æ¨¡å‹æˆæœ¬æœ€ä½
            priority=5,
            threads=int(os.getenv("LLAMACPP_THREADS", "4")),
            gpu_layers=int(os.getenv("LLAMACPP_GPU_LAYERS", "0")),
            kwargs={
                "top_k": 40,
                "top_p": 0.9,
                "repeat_penalty": 1.1,
                "temperature": 0.7
            }
        )
        
        self.model_manager.register_model("local_gguf_model", local_gguf_model)
        
        # åµŒå…¥æ¨¡å‹ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if os.getenv("LLAMACPP_EMBEDDING_MODEL"):
            embedding_model = ModelConfig(
                provider=ModelProvider.LLAMACPP,
                name="embedding-model",
                endpoint=llamacpp_endpoint,
                ctx_length=512,
                capabilities=[ModelCapability.EMBEDDING],
                speed_score=8,
                quality_score=7,
                cost_score=10,
                priority=8
            )
            
            self.model_manager.register_model("local_embedding_model", embedding_model)
    
    def _init_default_models(self):
        """åˆå§‹åŒ–é»˜è®¤æ¨¡å‹é…ç½®"""
        # å¦‚æœæ²¡æœ‰é…ç½®ä»»ä½•å¤–éƒ¨æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if len(self.model_manager.models) == 0:
            print("âš ï¸ æœªæ£€æµ‹åˆ°å¤–éƒ¨æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            
            # é»˜è®¤OpenAIæ¨¡å‹ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                default_openai = ModelConfig(
                    provider=ModelProvider.OPENAI,
                    name="gpt-3.5-turbo",
                    api_key=openai_api_key,
                    ctx_length=16385,
                    capabilities=[
                        ModelCapability.TEXT_GENERATION,
                        ModelCapability.CHAT,
                        ModelCapability.CODE_GENERATION
                    ],
                    speed_score=8,
                    quality_score=7,
                    cost_score=6,
                    priority=6
                )
                
                self.model_manager.register_model("default_openai", default_openai)
        
        # ç¡®ä¿æœ‰å·¥å…·æ¨¡å‹
        self._ensure_utility_model()
    
    def _ensure_utility_model(self):
        """ç¡®ä¿æœ‰å·¥å…·æ¨¡å‹"""
        utility_models = self.model_manager.list_models_by_task(TaskType.UTILITY)
        
        if not utility_models:
            # é€‰æ‹©ä¸€ä¸ªå¿«é€Ÿä¾¿å®œçš„æ¨¡å‹ä½œä¸ºå·¥å…·æ¨¡å‹
            all_models = list(self.model_manager.models.keys())
            if all_models:
                # é€‰æ‹©é€Ÿåº¦å’Œæˆæœ¬è¯„åˆ†æœ€é«˜çš„æ¨¡å‹
                best_utility_model = None
                best_score = 0
                
                for model_id in all_models:
                    config = self.model_manager.get_model_config(model_id)
                    score = config.speed_score + config.cost_score
                    if score > best_score:
                        best_score = score
                        best_utility_model = model_id
                
                if best_utility_model:
                    # å°†è¯¥æ¨¡å‹æ·»åŠ åˆ°å·¥å…·ä»»åŠ¡æ˜ å°„
                    self.model_manager.task_model_mapping[TaskType.UTILITY].append(best_utility_model)
                    print(f"ğŸ”§ è®¾ç½®å·¥å…·æ¨¡å‹: {best_utility_model}")
    
    def _save_model_config(self):
        """ä¿å­˜æ¨¡å‹é…ç½®åˆ°æ–‡ä»¶"""
        try:
            config_data = self.model_manager.export_config()
            
            # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
            config_dir = "config"
            os.makedirs(config_dir, exist_ok=True)
            
            # ä¿å­˜é…ç½®
            config_file = os.path.join(config_dir, "model_config.json")
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ æ¨¡å‹é…ç½®å·²ä¿å­˜åˆ°: {config_file}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {e}")
    
    def _print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        stats = self.model_manager.get_model_statistics()
        
        print("\n" + "="*50)
        print("ğŸ“Š æ¨¡å‹é…ç½®æ‘˜è¦")
        print("="*50)
        print(f"æ€»æ¨¡å‹æ•°: {stats['total_models']}")
        print(f"å¯ç”¨æ¨¡å‹æ•°: {stats['enabled_models']}")
        
        print("\nğŸ“¡ æä¾›å•†åˆ†å¸ƒ:")
        for provider, count in stats['provider_distribution'].items():
            print(f"  - {provider}: {count}ä¸ªæ¨¡å‹")
        
        print("\nğŸ¯ èƒ½åŠ›åˆ†å¸ƒ:")
        for capability, count in stats['capability_distribution'].items():
            print(f"  - {capability}: {count}ä¸ªæ¨¡å‹")
        
        print("\nğŸ”§ ä»»åŠ¡æ¨¡å‹æ˜ å°„:")
        for task_type, model_list in self.model_manager.task_model_mapping.items():
            if model_list:
                print(f"  - {task_type.value}: {len(model_list)}ä¸ªæ¨¡å‹")
        
        print("="*50)
    
    def load_config_from_file(self, config_file: str) -> bool:
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            if not os.path.exists(config_file):
                print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
                return False
            
            with open(config_file, "r", encoding="utf-8") as f:
                config_data = json.load(f)
            
            success = self.model_manager.import_config(config_data)
            if success:
                self.config_loaded = True
                print(f"âœ… ä»æ–‡ä»¶åŠ è½½é…ç½®æˆåŠŸ: {config_file}")
                self._print_config_summary()
            
            return success
            
        except Exception as e:
            print(f"âŒ ä»æ–‡ä»¶åŠ è½½é…ç½®å¤±è´¥: {e}")
            return False
    
    def create_sample_config(self) -> Dict[str, Any]:
        """åˆ›å»ºç¤ºä¾‹é…ç½®"""
        return {
            "models": {
                "cloud_gpt4": {
                    "provider": "openai_compatible",
                    "name": "gpt-4-turbo",
                    "endpoint": "https://api.openai.com",
                    "capabilities": ["text_generation", "code_generation", "reasoning", "chat"],
                    "vision": True,
                    "function_calling": True,
                    "speed_score": 7,
                    "quality_score": 9,
                    "cost_score": 3,
                    "priority": 8,
                    "enabled": True
                },
                "local_vllm": {
                    "provider": "vllm",
                    "name": "llama-2-7b-chat",
                    "endpoint": "http://localhost:8000",
                    "capabilities": ["text_generation", "creative_writing", "chat"],
                    "speed_score": 9,
                    "quality_score": 6,
                    "cost_score": 10,
                    "priority": 7,
                    "enabled": True
                },
                "local_llamacpp": {
                    "provider": "llamacpp",
                    "name": "local-gguf-model",
                    "endpoint": "http://localhost:8080",
                    "capabilities": ["text_generation", "chat", "embedding"],
                    "speed_score": 7,
                    "quality_score": 6,
                    "cost_score": 10,
                    "priority": 5,
                    "enabled": True
                }
            },
            "task_mappings": {
                "browsing": ["cloud_gpt4"],
                "writing": ["local_vllm", "local_llamacpp"],
                "coding": ["cloud_gpt4", "local_vllm"],
                "analysis": ["cloud_gpt4"],
                "reasoning": ["cloud_gpt4"],
                "creative": ["local_vllm"],
                "chat": ["local_vllm", "cloud_gpt4"],
                "embedding": ["local_llamacpp"],
                "utility": ["local_llamacpp", "local_vllm"]
            }
        }


# å…¨å±€åˆå§‹åŒ–å™¨å®ä¾‹
_initializer = ModelConfigInitializer()


def get_model_initializer() -> ModelConfigInitializer:
    """è·å–æ¨¡å‹åˆå§‹åŒ–å™¨å®ä¾‹"""
    return _initializer


def initialize_models() -> bool:
    """åˆå§‹åŒ–æ¨¡å‹é…ç½®"""
    return _initializer.initialize_from_env()


def load_model_config(config_file: str = "config/model_config.json") -> bool:
    """åŠ è½½æ¨¡å‹é…ç½®"""
    return _initializer.load_config_from_file(config_file)

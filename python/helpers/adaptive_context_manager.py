"""
è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†å™¨
å®ç°æŒ‰éœ€å¯åŠ¨å’Œæ™ºèƒ½åˆ‡æ¢æœºåˆ¶
"""

import asyncio
import time
import psutil
import threading
from typing import Dict, Any, List, Optional, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import json

from .unified_context_reasoning_module import (
    get_unified_context_reasoning_module, UnifiedProcessingConfig,
    ProcessingRequest, ProcessingResult, ProcessingStrategy
)
from .infinite_context_engine import ContextProcessingMode
from .intelligent_model_dispatcher import TaskType


class SystemLoadLevel(Enum):
    """ç³»ç»Ÿè´Ÿè½½çº§åˆ«"""
    LOW = "low"           # ä½è´Ÿè½½ < 30%
    MEDIUM = "medium"     # ä¸­ç­‰è´Ÿè½½ 30-70%
    HIGH = "high"         # é«˜è´Ÿè½½ 70-90%
    CRITICAL = "critical" # ä¸´ç•Œè´Ÿè½½ > 90%


class ContextStrategy(Enum):
    """ä¸Šä¸‹æ–‡ç­–ç•¥"""
    MINIMAL = "minimal"           # æœ€å°åŒ–ä¸Šä¸‹æ–‡å¤„ç†
    STANDARD = "standard"         # æ ‡å‡†ä¸Šä¸‹æ–‡å¤„ç†
    ENHANCED = "enhanced"         # å¢å¼ºä¸Šä¸‹æ–‡å¤„ç†
    MAXIMUM = "maximum"           # æœ€å¤§åŒ–ä¸Šä¸‹æ–‡å¤„ç†


@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡"""
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    memory_available_mb: float = 0.0
    active_requests: int = 0
    queue_length: int = 0
    avg_response_time_ms: float = 0.0
    error_rate: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class AdaptiveConfig:
    """è‡ªé€‚åº”é…ç½®"""
    # å¯åŠ¨é˜ˆå€¼
    infinite_context_threshold: int = 8192      # å¯åŠ¨æ— é™ä¸Šä¸‹æ–‡çš„é˜ˆå€¼
    reasoning_threshold: int = 5000             # å¯åŠ¨æ¨ç†çš„é˜ˆå€¼
    parallel_processing_threshold: int = 3      # å¯åŠ¨å¹¶è¡Œå¤„ç†çš„é˜ˆå€¼
    
    # ç³»ç»Ÿèµ„æºé˜ˆå€¼
    max_cpu_usage: float = 80.0                 # æœ€å¤§CPUä½¿ç”¨ç‡
    max_memory_usage: float = 85.0              # æœ€å¤§å†…å­˜ä½¿ç”¨ç‡
    min_available_memory_mb: float = 1024.0     # æœ€å°å¯ç”¨å†…å­˜
    
    # æ€§èƒ½é˜ˆå€¼
    max_response_time_ms: float = 30000.0       # æœ€å¤§å“åº”æ—¶é—´
    max_error_rate: float = 0.1                 # æœ€å¤§é”™è¯¯ç‡
    max_queue_length: int = 10                  # æœ€å¤§é˜Ÿåˆ—é•¿åº¦
    
    # è‡ªé€‚åº”å‚æ•°
    adaptation_interval_seconds: float = 30.0   # è‡ªé€‚åº”é—´éš”
    metrics_history_size: int = 100             # æŒ‡æ ‡å†å²å¤§å°
    strategy_switch_cooldown_seconds: float = 60.0  # ç­–ç•¥åˆ‡æ¢å†·å´æ—¶é—´
    
    # é¢„æµ‹å‚æ•°
    enable_predictive_scaling: bool = True      # å¯ç”¨é¢„æµ‹æ€§æ‰©å±•
    prediction_window_minutes: int = 10        # é¢„æµ‹çª—å£
    load_spike_threshold: float = 2.0          # è´Ÿè½½å³°å€¼é˜ˆå€¼


class AdaptiveContextManager:
    """è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, config: AdaptiveConfig):
        self.config = config
        self.unified_module = None
        
        # ç³»ç»Ÿç›‘æ§
        self.system_metrics_history: List[SystemMetrics] = []
        self.current_metrics = SystemMetrics()
        self.monitoring_active = False
        self.monitoring_thread = None
        
        # è‡ªé€‚åº”çŠ¶æ€
        self.current_strategy = ContextStrategy.STANDARD
        self.last_strategy_switch = datetime.now()
        self.strategy_switch_history: List[Dict[str, Any]] = []
        
        # è¯·æ±‚é˜Ÿåˆ—å’Œå¤„ç†
        self.request_queue: asyncio.Queue = asyncio.Queue()
        self.active_requests: Dict[str, ProcessingRequest] = {}
        self.processing_workers: List[asyncio.Task] = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "avg_response_time_ms": 0.0,
            "strategy_switches": 0,
            "resource_optimizations": 0
        }
        
        # é¢„æµ‹æ¨¡å‹ï¼ˆç®€åŒ–å®ç°ï¼‰
        self.load_predictor = LoadPredictor() if config.enable_predictive_scaling else None
    
    async def initialize(self):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        # åˆå§‹åŒ–ç»Ÿä¸€æ¨¡å—
        unified_config = UnifiedProcessingConfig()
        self.unified_module = get_unified_context_reasoning_module(unified_config)
        
        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        await self.start_monitoring()
        
        # å¯åŠ¨å¤„ç†å·¥ä½œå™¨
        await self.start_processing_workers()
        
        print("âœ… è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def start_monitoring(self):
        """å¯åŠ¨ç³»ç»Ÿç›‘æ§"""
        if not self.monitoring_active:
            self.monitoring_active = True
            self.monitoring_thread = threading.Thread(
                target=self._monitoring_loop,
                daemon=True
            )
            self.monitoring_thread.start()
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring_active:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                metrics = self._collect_system_metrics()
                self.current_metrics = metrics
                self.system_metrics_history.append(metrics)
                
                # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
                if len(self.system_metrics_history) > self.config.metrics_history_size:
                    self.system_metrics_history = self.system_metrics_history[-self.config.metrics_history_size//2:]
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´ç­–ç•¥
                asyncio.run_coroutine_threadsafe(
                    self._check_and_adapt_strategy(),
                    asyncio.get_event_loop()
                )
                
                time.sleep(self.config.adaptation_interval_seconds)
                
            except Exception as e:
                print(f"âš ï¸ ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(5)
    
    def _collect_system_metrics(self) -> SystemMetrics:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            cpu_usage = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            memory_usage = memory.percent
            memory_available_mb = memory.available / (1024 * 1024)
            
            return SystemMetrics(
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                memory_available_mb=memory_available_mb,
                active_requests=len(self.active_requests),
                queue_length=self.request_queue.qsize(),
                timestamp=datetime.now()
            )
        except Exception as e:
            print(f"âš ï¸ æ”¶é›†ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            return SystemMetrics()
    
    async def _check_and_adapt_strategy(self):
        """æ£€æŸ¥å¹¶è°ƒæ•´ç­–ç•¥"""
        if not self._can_switch_strategy():
            return
        
        # åˆ†æå½“å‰ç³»ç»ŸçŠ¶æ€
        load_level = self._analyze_system_load()
        optimal_strategy = self._determine_optimal_strategy(load_level)
        
        # å¦‚æœéœ€è¦åˆ‡æ¢ç­–ç•¥
        if optimal_strategy != self.current_strategy:
            await self._switch_strategy(optimal_strategy, f"ç³»ç»Ÿè´Ÿè½½: {load_level.value}")
    
    def _can_switch_strategy(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ‡æ¢ç­–ç•¥"""
        cooldown_elapsed = (
            datetime.now() - self.last_strategy_switch
        ).total_seconds() >= self.config.strategy_switch_cooldown_seconds
        
        return cooldown_elapsed
    
    def _analyze_system_load(self) -> SystemLoadLevel:
        """åˆ†æç³»ç»Ÿè´Ÿè½½çº§åˆ«"""
        if not self.system_metrics_history:
            return SystemLoadLevel.LOW
        
        recent_metrics = self.system_metrics_history[-5:]  # æœ€è¿‘5ä¸ªæŒ‡æ ‡
        avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)
        
        # ç»¼åˆè¯„ä¼°è´Ÿè½½çº§åˆ«
        if avg_cpu > 90 or avg_memory > 90:
            return SystemLoadLevel.CRITICAL
        elif avg_cpu > 70 or avg_memory > 70:
            return SystemLoadLevel.HIGH
        elif avg_cpu > 30 or avg_memory > 30:
            return SystemLoadLevel.MEDIUM
        else:
            return SystemLoadLevel.LOW
    
    def _determine_optimal_strategy(self, load_level: SystemLoadLevel) -> ContextStrategy:
        """ç¡®å®šæœ€ä¼˜ç­–ç•¥"""
        current_queue_length = self.request_queue.qsize()
        active_requests = len(self.active_requests)
        
        # æ ¹æ®è´Ÿè½½çº§åˆ«å’Œé˜Ÿåˆ—çŠ¶æ€ç¡®å®šç­–ç•¥
        if load_level == SystemLoadLevel.CRITICAL:
            return ContextStrategy.MINIMAL
        elif load_level == SystemLoadLevel.HIGH:
            if current_queue_length > 5:
                return ContextStrategy.MINIMAL
            else:
                return ContextStrategy.STANDARD
        elif load_level == SystemLoadLevel.MEDIUM:
            if current_queue_length < 3 and active_requests < 5:
                return ContextStrategy.ENHANCED
            else:
                return ContextStrategy.STANDARD
        else:  # LOW
            if current_queue_length == 0 and active_requests < 3:
                return ContextStrategy.MAXIMUM
            else:
                return ContextStrategy.ENHANCED
    
    async def _switch_strategy(self, new_strategy: ContextStrategy, reason: str):
        """åˆ‡æ¢ç­–ç•¥"""
        old_strategy = self.current_strategy
        self.current_strategy = new_strategy
        self.last_strategy_switch = datetime.now()
        
        # è®°å½•åˆ‡æ¢å†å²
        switch_record = {
            "timestamp": self.last_strategy_switch,
            "from_strategy": old_strategy.value,
            "to_strategy": new_strategy.value,
            "reason": reason,
            "system_metrics": self.current_metrics.__dict__.copy()
        }
        self.strategy_switch_history.append(switch_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.strategy_switch_history) > 100:
            self.strategy_switch_history = self.strategy_switch_history[-50:]
        
        # æ›´æ–°ç»Ÿè®¡
        self.performance_stats["strategy_switches"] += 1
        
        print(f"ğŸ”„ ç­–ç•¥åˆ‡æ¢: {old_strategy.value} â†’ {new_strategy.value} (åŸå› : {reason})")
        
        # è°ƒæ•´å¤„ç†å·¥ä½œå™¨æ•°é‡
        await self._adjust_workers(new_strategy)
    
    async def _adjust_workers(self, strategy: ContextStrategy):
        """è°ƒæ•´å·¥ä½œå™¨æ•°é‡"""
        if strategy == ContextStrategy.MINIMAL:
            target_workers = 1
        elif strategy == ContextStrategy.STANDARD:
            target_workers = 2
        elif strategy == ContextStrategy.ENHANCED:
            target_workers = 3
        else:  # MAXIMUM
            target_workers = 4
        
        current_workers = len(self.processing_workers)
        
        if target_workers > current_workers:
            # å¢åŠ å·¥ä½œå™¨
            for _ in range(target_workers - current_workers):
                worker = asyncio.create_task(self._processing_worker())
                self.processing_workers.append(worker)
        elif target_workers < current_workers:
            # å‡å°‘å·¥ä½œå™¨
            workers_to_remove = current_workers - target_workers
            for _ in range(workers_to_remove):
                if self.processing_workers:
                    worker = self.processing_workers.pop()
                    worker.cancel()
    
    async def start_processing_workers(self):
        """å¯åŠ¨å¤„ç†å·¥ä½œå™¨"""
        # æ ¹æ®å½“å‰ç­–ç•¥å¯åŠ¨é€‚å½“æ•°é‡çš„å·¥ä½œå™¨
        await self._adjust_workers(self.current_strategy)
    
    async def _processing_worker(self):
        """å¤„ç†å·¥ä½œå™¨"""
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–è¯·æ±‚
                request = await self.request_queue.get()
                
                # æ·»åŠ åˆ°æ´»è·ƒè¯·æ±‚
                self.active_requests[request.request_id] = request
                
                # æ ¹æ®å½“å‰ç­–ç•¥è°ƒæ•´è¯·æ±‚
                adjusted_request = self._adjust_request_for_strategy(request)
                
                # å¤„ç†è¯·æ±‚
                start_time = time.time()
                result = await self.unified_module.process_request(adjusted_request)
                processing_time = (time.time() - start_time) * 1000
                
                # æ›´æ–°ç»Ÿè®¡
                self._update_processing_stats(result, processing_time)
                
                # ä»æ´»è·ƒè¯·æ±‚ä¸­ç§»é™¤
                self.active_requests.pop(request.request_id, None)
                
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.request_queue.task_done()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å·¥ä½œå™¨å¼‚å¸¸: {e}")
                # ä»æ´»è·ƒè¯·æ±‚ä¸­ç§»é™¤
                if 'request' in locals():
                    self.active_requests.pop(request.request_id, None)
                    self.request_queue.task_done()
    
    def _adjust_request_for_strategy(self, request: ProcessingRequest) -> ProcessingRequest:
        """æ ¹æ®ç­–ç•¥è°ƒæ•´è¯·æ±‚"""
        adjusted_request = request
        
        if self.current_strategy == ContextStrategy.MINIMAL:
            # æœ€å°åŒ–å¤„ç†
            adjusted_request.require_reasoning = False
            adjusted_request.require_infinite_context = False
            adjusted_request.processing_strategy = ProcessingStrategy.ADAPTIVE
            adjusted_request.timeout_seconds = min(request.timeout_seconds, 30.0)
            
        elif self.current_strategy == ContextStrategy.STANDARD:
            # æ ‡å‡†å¤„ç†
            content_length = len(str(request.content))
            adjusted_request.require_infinite_context = content_length > self.config.infinite_context_threshold
            adjusted_request.processing_strategy = ProcessingStrategy.ADAPTIVE
            
        elif self.current_strategy == ContextStrategy.ENHANCED:
            # å¢å¼ºå¤„ç†
            content_length = len(str(request.content))
            adjusted_request.require_infinite_context = content_length > self.config.infinite_context_threshold // 2
            adjusted_request.require_reasoning = content_length > self.config.reasoning_threshold
            adjusted_request.processing_strategy = ProcessingStrategy.PARALLEL
            
        else:  # MAXIMUM
            # æœ€å¤§åŒ–å¤„ç†
            adjusted_request.require_infinite_context = True
            adjusted_request.require_reasoning = True
            adjusted_request.processing_strategy = ProcessingStrategy.PARALLEL
        
        return adjusted_request
    
    def _update_processing_stats(self, result: ProcessingResult, processing_time: float):
        """æ›´æ–°å¤„ç†ç»Ÿè®¡"""
        self.performance_stats["total_requests"] += 1
        
        if result.success:
            self.performance_stats["successful_requests"] += 1
        else:
            self.performance_stats["failed_requests"] += 1
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        total_requests = self.performance_stats["total_requests"]
        current_avg = self.performance_stats["avg_response_time_ms"]
        new_avg = (current_avg * (total_requests - 1) + processing_time) / total_requests
        self.performance_stats["avg_response_time_ms"] = new_avg
    
    async def submit_request(self, request: ProcessingRequest) -> str:
        """æäº¤è¯·æ±‚"""
        # æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½
        if self._should_reject_request():
            raise Exception("ç³»ç»Ÿè´Ÿè½½è¿‡é«˜ï¼Œè¯·æ±‚è¢«æ‹’ç»")
        
        # æ·»åŠ åˆ°é˜Ÿåˆ—
        await self.request_queue.put(request)
        
        return f"è¯·æ±‚ {request.request_id} å·²æäº¤åˆ°é˜Ÿåˆ—"
    
    def _should_reject_request(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‹’ç»è¯·æ±‚"""
        # æ£€æŸ¥é˜Ÿåˆ—é•¿åº¦
        if self.request_queue.qsize() > self.config.max_queue_length:
            return True
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        if (self.current_metrics.cpu_usage > self.config.max_cpu_usage or
            self.current_metrics.memory_usage > self.config.max_memory_usage or
            self.current_metrics.memory_available_mb < self.config.min_available_memory_mb):
            return True
        
        return False
    
    def get_status_report(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€æŠ¥å‘Š"""
        return {
            "current_strategy": self.current_strategy.value,
            "system_metrics": self.current_metrics.__dict__,
            "performance_stats": self.performance_stats.copy(),
            "queue_status": {
                "queue_length": self.request_queue.qsize(),
                "active_requests": len(self.active_requests),
                "worker_count": len(self.processing_workers)
            },
            "recent_strategy_switches": self.strategy_switch_history[-5:],
            "load_prediction": self.load_predictor.predict() if self.load_predictor else None
        }
    
    async def shutdown(self):
        """å…³é—­ç®¡ç†å™¨"""
        # åœæ­¢ç›‘æ§
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        
        # å–æ¶ˆæ‰€æœ‰å·¥ä½œå™¨
        for worker in self.processing_workers:
            worker.cancel()
        
        # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º
        await self.request_queue.join()
        
        print("âœ… è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·²å…³é—­")


class LoadPredictor:
    """è´Ÿè½½é¢„æµ‹å™¨ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    
    def __init__(self):
        self.history: List[float] = []
    
    def update(self, load: float):
        """æ›´æ–°è´Ÿè½½å†å²"""
        self.history.append(load)
        if len(self.history) > 100:
            self.history = self.history[-50:]
    
    def predict(self) -> Dict[str, Any]:
        """é¢„æµ‹æœªæ¥è´Ÿè½½"""
        if len(self.history) < 5:
            return {"prediction": "insufficient_data"}
        
        # ç®€å•çš„çº¿æ€§è¶‹åŠ¿é¢„æµ‹
        recent = self.history[-10:]
        trend = (recent[-1] - recent[0]) / len(recent)
        
        predicted_load = recent[-1] + trend * 5  # é¢„æµ‹5ä¸ªæ—¶é—´ç‚¹åçš„è´Ÿè½½
        
        return {
            "current_load": recent[-1],
            "predicted_load": max(0, min(100, predicted_load)),
            "trend": "increasing" if trend > 0.1 else "decreasing" if trend < -0.1 else "stable",
            "confidence": min(len(self.history) / 50, 1.0)
        }


# å…¨å±€å®ä¾‹
_adaptive_manager = None


async def get_adaptive_context_manager(config: Optional[AdaptiveConfig] = None) -> AdaptiveContextManager:
    """è·å–è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ä¾‹"""
    global _adaptive_manager
    
    if _adaptive_manager is None:
        if config is None:
            config = AdaptiveConfig()
        _adaptive_manager = AdaptiveContextManager(config)
        await _adaptive_manager.initialize()
    
    return _adaptive_manager


# ä¾¿æ·å‡½æ•°
async def adaptive_process(
    content: Union[str, List[str]],
    task_type: Optional[TaskType] = None,
    priority: int = 5,
    **kwargs
) -> str:
    """è‡ªé€‚åº”å¤„ç†å‡½æ•°"""
    manager = await get_adaptive_context_manager()
    
    request = ProcessingRequest(
        request_id=f"adaptive_{int(time.time())}",
        content=content,
        task_type=task_type,
        priority=priority,
        metadata=kwargs
    )
    
    return await manager.submit_request(request)

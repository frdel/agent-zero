"""
å¢å¼ºçš„æ¨¡å‹ç®¡ç†å™¨
æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†å’Œæ™ºèƒ½æ¨¡å‹é€‰æ‹©
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import json
import time
from datetime import datetime


class ModelProvider(Enum):
    """æ¨¡å‹æä¾›å•†æšä¸¾"""
    OPENAI = "openai"
    OPENAI_COMPATIBLE = "openai_compatible"  # å…¼å®¹OpenAI APIçš„äº‘æœåŠ¡
    VLLM = "vllm"                           # æœ¬åœ°vLLM
    LLAMACPP = "llamacpp"                   # æœ¬åœ°LlamaCpp
    ANTHROPIC = "anthropic"
    HUGGINGFACE = "huggingface"
    AZURE = "azure"
    GOOGLE = "google"


class ModelCapability(Enum):
    """æ¨¡å‹èƒ½åŠ›æšä¸¾"""
    TEXT_GENERATION = "text_generation"
    CODE_GENERATION = "code_generation"
    VISION = "vision"
    MULTIMODAL = "multimodal"
    EMBEDDING = "embedding"
    CHAT = "chat"
    COMPLETION = "completion"
    FUNCTION_CALLING = "function_calling"
    REASONING = "reasoning"
    CREATIVE_WRITING = "creative_writing"
    DATA_ANALYSIS = "data_analysis"


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    BROWSING = "browsing"                   # æµè§ˆå™¨ä»»åŠ¡
    WRITING = "writing"                     # æ–‡ç« å†™ä½œ
    CODING = "coding"                       # ä»£ç ç¼–å†™
    ANALYSIS = "analysis"                   # æ•°æ®åˆ†æ
    REASONING = "reasoning"                 # æ¨ç†ä»»åŠ¡
    CREATIVE = "creative"                   # åˆ›æ„ä»»åŠ¡
    CHAT = "chat"                          # å¯¹è¯ä»»åŠ¡
    EMBEDDING = "embedding"                # åµŒå…¥ä»»åŠ¡
    UTILITY = "utility"                    # å·¥å…·ä»»åŠ¡


@dataclass
class ModelConfig:
    """å¢å¼ºçš„æ¨¡å‹é…ç½®"""
    provider: ModelProvider
    name: str
    endpoint: Optional[str] = None
    api_key: Optional[str] = None
    
    # åŸºç¡€é…ç½®
    ctx_length: int = 4096
    max_tokens: Optional[int] = None
    temperature: float = 0.7
    
    # èƒ½åŠ›é…ç½®
    capabilities: List[ModelCapability] = field(default_factory=list)
    vision: bool = False
    function_calling: bool = False
    
    # æ€§èƒ½é…ç½®
    speed_score: int = 5  # 1-10ï¼Œé€Ÿåº¦è¯„åˆ†
    quality_score: int = 5  # 1-10ï¼Œè´¨é‡è¯„åˆ†
    cost_score: int = 5  # 1-10ï¼Œæˆæœ¬è¯„åˆ†ï¼ˆè¶Šé«˜è¶Šä¾¿å®œï¼‰
    
    # é™åˆ¶é…ç½®
    limit_requests: int = 0
    limit_input: int = 0
    limit_output: int = 0
    
    # æœ¬åœ°æ¨¡å‹ç‰¹å®šé…ç½®
    model_path: Optional[str] = None
    gpu_layers: int = 0
    threads: int = 4
    
    # å…¶ä»–é…ç½®
    kwargs: Dict[str, Any] = field(default_factory=dict)
    enabled: bool = True
    priority: int = 5  # 1-10ï¼Œä¼˜å…ˆçº§
    
    # ä½¿ç”¨ç»Ÿè®¡
    usage_count: int = 0
    success_rate: float = 1.0
    avg_response_time: float = 0.0


@dataclass
class ModelSelectionCriteria:
    """æ¨¡å‹é€‰æ‹©æ ‡å‡†"""
    task_type: TaskType
    required_capabilities: List[ModelCapability] = field(default_factory=list)
    prefer_local: bool = False
    prefer_fast: bool = False
    prefer_quality: bool = False
    prefer_cheap: bool = False
    max_cost: Optional[float] = None
    max_response_time: Optional[float] = None
    min_quality_score: int = 3
    context_length_needed: int = 4096


class IModelProvider(ABC):
    """æ¨¡å‹æä¾›å•†æ¥å£"""
    
    @abstractmethod
    async def initialize(self, config: ModelConfig) -> bool:
        """åˆå§‹åŒ–æ¨¡å‹"""
        pass
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass
    
    @abstractmethod
    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """å¯¹è¯ç”Ÿæˆ"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        pass
    
    @abstractmethod
    async def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬åµŒå…¥"""
        pass


class EnhancedModelManager:
    """å¢å¼ºçš„æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.models: Dict[str, ModelConfig] = {}
        self.providers: Dict[str, IModelProvider] = {}
        self.task_model_mapping: Dict[TaskType, List[str]] = {}
        self.model_performance_history: Dict[str, List[Dict[str, Any]]] = {}
        self._initialize_default_mappings()
    
    def _initialize_default_mappings(self):
        """åˆå§‹åŒ–é»˜è®¤çš„ä»»åŠ¡-æ¨¡å‹æ˜ å°„"""
        self.task_model_mapping = {
            TaskType.BROWSING: [],      # å¤šæ¨¡æ€æ¨¡å‹ä¼˜å…ˆ
            TaskType.WRITING: [],       # å°å‹æœ¬åœ°æ¨¡å‹ä¼˜å…ˆ
            TaskType.CODING: [],        # å¤§å‹ä»£ç æ¨¡å‹ä¼˜å…ˆ
            TaskType.ANALYSIS: [],      # åˆ†æèƒ½åŠ›å¼ºçš„æ¨¡å‹
            TaskType.REASONING: [],     # æ¨ç†èƒ½åŠ›å¼ºçš„æ¨¡å‹
            TaskType.CREATIVE: [],      # åˆ›æ„èƒ½åŠ›å¼ºçš„æ¨¡å‹
            TaskType.CHAT: [],          # å¯¹è¯æ¨¡å‹
            TaskType.EMBEDDING: [],     # åµŒå…¥æ¨¡å‹
            TaskType.UTILITY: []        # å·¥å…·æ¨¡å‹
        }
    
    def register_model(self, model_id: str, config: ModelConfig) -> bool:
        """æ³¨å†Œæ¨¡å‹"""
        try:
            self.models[model_id] = config
            
            # æ ¹æ®æ¨¡å‹èƒ½åŠ›æ›´æ–°ä»»åŠ¡æ˜ å°„
            self._update_task_mappings(model_id, config)
            
            print(f"âœ… å·²æ³¨å†Œæ¨¡å‹: {model_id} ({config.provider.value})")
            return True
        except Exception as e:
            print(f"âŒ æ³¨å†Œæ¨¡å‹å¤±è´¥ {model_id}: {e}")
            return False
    
    def _update_task_mappings(self, model_id: str, config: ModelConfig):
        """æ›´æ–°ä»»åŠ¡æ˜ å°„"""
        # æµè§ˆå™¨ä»»åŠ¡ - ä¼˜å…ˆå¤šæ¨¡æ€æ¨¡å‹
        if ModelCapability.VISION in config.capabilities or config.vision:
            self.task_model_mapping[TaskType.BROWSING].append(model_id)
        
        # å†™ä½œä»»åŠ¡ - ä¼˜å…ˆæœ¬åœ°å°æ¨¡å‹
        if (config.provider in [ModelProvider.VLLM, ModelProvider.LLAMACPP] and
            ModelCapability.CREATIVE_WRITING in config.capabilities):
            self.task_model_mapping[TaskType.WRITING].insert(0, model_id)
        elif ModelCapability.TEXT_GENERATION in config.capabilities:
            self.task_model_mapping[TaskType.WRITING].append(model_id)
        
        # ä»£ç ä»»åŠ¡ - ä¼˜å…ˆå¤§å‹ä»£ç æ¨¡å‹
        if ModelCapability.CODE_GENERATION in config.capabilities:
            if config.quality_score >= 8:  # é«˜è´¨é‡æ¨¡å‹ä¼˜å…ˆ
                self.task_model_mapping[TaskType.CODING].insert(0, model_id)
            else:
                self.task_model_mapping[TaskType.CODING].append(model_id)
        
        # åˆ†æä»»åŠ¡
        if ModelCapability.DATA_ANALYSIS in config.capabilities:
            self.task_model_mapping[TaskType.ANALYSIS].append(model_id)
        
        # æ¨ç†ä»»åŠ¡
        if ModelCapability.REASONING in config.capabilities:
            self.task_model_mapping[TaskType.REASONING].append(model_id)
        
        # åˆ›æ„ä»»åŠ¡
        if ModelCapability.CREATIVE_WRITING in config.capabilities:
            self.task_model_mapping[TaskType.CREATIVE].append(model_id)
        
        # å¯¹è¯ä»»åŠ¡
        if ModelCapability.CHAT in config.capabilities:
            self.task_model_mapping[TaskType.CHAT].append(model_id)
        
        # åµŒå…¥ä»»åŠ¡
        if ModelCapability.EMBEDDING in config.capabilities:
            self.task_model_mapping[TaskType.EMBEDDING].append(model_id)
        
        # å·¥å…·ä»»åŠ¡ - ä¼˜å…ˆå¿«é€Ÿã€ä¾¿å®œçš„æ¨¡å‹
        if config.speed_score >= 7 and config.cost_score >= 7:
            self.task_model_mapping[TaskType.UTILITY].insert(0, model_id)
        else:
            self.task_model_mapping[TaskType.UTILITY].append(model_id)
    
    async def select_best_model(self, criteria: ModelSelectionCriteria) -> Optional[str]:
        """é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        candidates = self._get_candidate_models(criteria)
        
        if not candidates:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹: {criteria.task_type}")
            return None
        
        # è¯„åˆ†å’Œæ’åº
        scored_candidates = []
        for model_id in candidates:
            score = await self._calculate_model_score(model_id, criteria)
            scored_candidates.append((model_id, score))
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        
        best_model = scored_candidates[0][0]
        print(f"ğŸ¯ ä¸ºä»»åŠ¡ {criteria.task_type.value} é€‰æ‹©æ¨¡å‹: {best_model}")
        
        return best_model
    
    def _get_candidate_models(self, criteria: ModelSelectionCriteria) -> List[str]:
        """è·å–å€™é€‰æ¨¡å‹"""
        # ä»ä»»åŠ¡æ˜ å°„å¼€å§‹
        candidates = self.task_model_mapping.get(criteria.task_type, []).copy()
        
        # å¦‚æœä»»åŠ¡æ˜ å°„ä¸ºç©ºï¼Œä»æ‰€æœ‰æ¨¡å‹ä¸­ç­›é€‰
        if not candidates:
            candidates = list(self.models.keys())
        
        # è¿‡æ»¤æ¡ä»¶
        filtered_candidates = []
        for model_id in candidates:
            config = self.models[model_id]
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨
            if not config.enabled:
                continue
            
            # æ£€æŸ¥å¿…éœ€èƒ½åŠ›
            if criteria.required_capabilities:
                if not all(cap in config.capabilities for cap in criteria.required_capabilities):
                    continue
            
            # æ£€æŸ¥æœ¬åœ°åå¥½
            if criteria.prefer_local:
                if config.provider not in [ModelProvider.VLLM, ModelProvider.LLAMACPP]:
                    continue
            
            # æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦
            if config.ctx_length < criteria.context_length_needed:
                continue
            
            # æ£€æŸ¥è´¨é‡è¦æ±‚
            if config.quality_score < criteria.min_quality_score:
                continue
            
            filtered_candidates.append(model_id)
        
        return filtered_candidates
    
    async def _calculate_model_score(self, model_id: str, criteria: ModelSelectionCriteria) -> float:
        """è®¡ç®—æ¨¡å‹è¯„åˆ†"""
        config = self.models[model_id]
        score = 0.0
        
        # åŸºç¡€åˆ†æ•°
        score += config.priority * 10
        
        # æ€§èƒ½åå¥½
        if criteria.prefer_fast:
            score += config.speed_score * 15
        
        if criteria.prefer_quality:
            score += config.quality_score * 15
        
        if criteria.prefer_cheap:
            score += config.cost_score * 15
        
        # æœ¬åœ°åå¥½
        if criteria.prefer_local and config.provider in [ModelProvider.VLLM, ModelProvider.LLAMACPP]:
            score += 20
        
        # æˆåŠŸç‡åŠ æˆ
        score += config.success_rate * 10
        
        # å“åº”æ—¶é—´æƒ©ç½š
        if config.avg_response_time > 0:
            if criteria.max_response_time and config.avg_response_time > criteria.max_response_time:
                score -= 30
            else:
                score -= config.avg_response_time * 2
        
        # èƒ½åŠ›åŒ¹é…åŠ æˆ
        capability_match = len(set(criteria.required_capabilities) & set(config.capabilities))
        score += capability_match * 5
        
        return score
    
    async def update_model_performance(self, model_id: str, response_time: float, success: bool):
        """æ›´æ–°æ¨¡å‹æ€§èƒ½ç»Ÿè®¡"""
        if model_id not in self.models:
            return
        
        config = self.models[model_id]
        
        # æ›´æ–°ä½¿ç”¨æ¬¡æ•°
        config.usage_count += 1
        
        # æ›´æ–°æˆåŠŸç‡
        total_attempts = config.usage_count
        previous_successes = (total_attempts - 1) * config.success_rate
        new_successes = previous_successes + (1 if success else 0)
        config.success_rate = new_successes / total_attempts
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        if config.avg_response_time == 0:
            config.avg_response_time = response_time
        else:
            config.avg_response_time = (config.avg_response_time * 0.8 + response_time * 0.2)
        
        # è®°å½•æ€§èƒ½å†å²
        if model_id not in self.model_performance_history:
            self.model_performance_history[model_id] = []
        
        self.model_performance_history[model_id].append({
            "timestamp": datetime.now(),
            "response_time": response_time,
            "success": success
        })
        
        # ä¿æŒæœ€è¿‘100æ¡è®°å½•
        if len(self.model_performance_history[model_id]) > 100:
            self.model_performance_history[model_id] = self.model_performance_history[model_id][-100:]
    
    def get_model_config(self, model_id: str) -> Optional[ModelConfig]:
        """è·å–æ¨¡å‹é…ç½®"""
        return self.models.get(model_id)
    
    def list_models_by_task(self, task_type: TaskType) -> List[str]:
        """æŒ‰ä»»åŠ¡ç±»å‹åˆ—å‡ºæ¨¡å‹"""
        return self.task_model_mapping.get(task_type, [])
    
    def get_model_statistics(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_models": len(self.models),
            "enabled_models": len([m for m in self.models.values() if m.enabled]),
            "provider_distribution": {},
            "capability_distribution": {},
            "performance_summary": {}
        }
        
        # æä¾›å•†åˆ†å¸ƒ
        for config in self.models.values():
            provider = config.provider.value
            stats["provider_distribution"][provider] = stats["provider_distribution"].get(provider, 0) + 1
        
        # èƒ½åŠ›åˆ†å¸ƒ
        for config in self.models.values():
            for capability in config.capabilities:
                cap_name = capability.value
                stats["capability_distribution"][cap_name] = stats["capability_distribution"].get(cap_name, 0) + 1
        
        # æ€§èƒ½æ‘˜è¦
        for model_id, config in self.models.items():
            stats["performance_summary"][model_id] = {
                "usage_count": config.usage_count,
                "success_rate": f"{config.success_rate:.2f}",
                "avg_response_time": f"{config.avg_response_time:.2f}s",
                "quality_score": config.quality_score,
                "speed_score": config.speed_score
            }
        
        return stats
    
    def export_config(self) -> Dict[str, Any]:
        """å¯¼å‡ºé…ç½®"""
        config_data = {
            "models": {},
            "task_mappings": {}
        }
        
        # å¯¼å‡ºæ¨¡å‹é…ç½®
        for model_id, config in self.models.items():
            config_data["models"][model_id] = {
                "provider": config.provider.value,
                "name": config.name,
                "endpoint": config.endpoint,
                "capabilities": [cap.value for cap in config.capabilities],
                "vision": config.vision,
                "function_calling": config.function_calling,
                "speed_score": config.speed_score,
                "quality_score": config.quality_score,
                "cost_score": config.cost_score,
                "priority": config.priority,
                "enabled": config.enabled,
                "kwargs": config.kwargs
            }
        
        # å¯¼å‡ºä»»åŠ¡æ˜ å°„
        for task_type, model_list in self.task_model_mapping.items():
            config_data["task_mappings"][task_type.value] = model_list
        
        return config_data
    
    def import_config(self, config_data: Dict[str, Any]) -> bool:
        """å¯¼å…¥é…ç½®"""
        try:
            # å¯¼å…¥æ¨¡å‹é…ç½®
            for model_id, model_data in config_data.get("models", {}).items():
                config = ModelConfig(
                    provider=ModelProvider(model_data["provider"]),
                    name=model_data["name"],
                    endpoint=model_data.get("endpoint"),
                    capabilities=[ModelCapability(cap) for cap in model_data.get("capabilities", [])],
                    vision=model_data.get("vision", False),
                    function_calling=model_data.get("function_calling", False),
                    speed_score=model_data.get("speed_score", 5),
                    quality_score=model_data.get("quality_score", 5),
                    cost_score=model_data.get("cost_score", 5),
                    priority=model_data.get("priority", 5),
                    enabled=model_data.get("enabled", True),
                    kwargs=model_data.get("kwargs", {})
                )
                self.register_model(model_id, config)
            
            # å¯¼å…¥ä»»åŠ¡æ˜ å°„
            for task_name, model_list in config_data.get("task_mappings", {}).items():
                task_type = TaskType(task_name)
                self.task_model_mapping[task_type] = model_list
            
            return True
        except Exception as e:
            print(f"âŒ å¯¼å…¥é…ç½®å¤±è´¥: {e}")
            return False


# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
_model_manager = EnhancedModelManager()


def get_model_manager() -> EnhancedModelManager:
    """è·å–æ¨¡å‹ç®¡ç†å™¨å®ä¾‹"""
    return _model_manager

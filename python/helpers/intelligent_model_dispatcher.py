"""
æ™ºèƒ½æ¨¡å‹è°ƒåº¦å™¨
æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
"""

import asyncio
import time
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from datetime import datetime

from .enhanced_model_manager import (
    get_model_manager, ModelSelectionCriteria, TaskType, ModelCapability
)
from .model_providers import get_or_create_provider


@dataclass
class ModelRequest:
    """æ¨¡å‹è¯·æ±‚"""
    request_id: str
    task_type: TaskType
    content: Union[str, List[Dict[str, str]]]  # æ–‡æœ¬æˆ–æ¶ˆæ¯åˆ—è¡¨
    request_type: str = "generate"  # generate, chat, embedding
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    prefer_local: bool = False
    prefer_fast: bool = False
    prefer_quality: bool = False
    context_length_needed: int = 4096
    required_capabilities: List[ModelCapability] = None
    metadata: Dict[str, Any] = None


@dataclass
class ModelResponse:
    """æ¨¡å‹å“åº”"""
    request_id: str
    model_id: str
    content: str
    response_time: float
    success: bool
    error_message: str = ""
    metadata: Dict[str, Any] = None


class IntelligentModelDispatcher:
    """æ™ºèƒ½æ¨¡å‹è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.model_manager = get_model_manager()
        self.active_requests: Dict[str, ModelRequest] = {}
        self.request_history: List[Dict[str, Any]] = []
        self.task_performance_cache: Dict[str, Dict[str, float]] = {}
        
        # ä»»åŠ¡ç±»å‹æ£€æµ‹è§„åˆ™
        self.task_detection_rules = {
            TaskType.BROWSING: [
                "screenshot", "browser", "webpage", "html", "css", "javascript",
                "visual", "image", "click", "scroll", "navigate"
            ],
            TaskType.WRITING: [
                "write", "article", "blog", "content", "essay", "story",
                "creative", "narrative", "draft", "compose"
            ],
            TaskType.CODING: [
                "code", "program", "function", "class", "algorithm", "debug",
                "python", "javascript", "java", "c++", "sql", "api"
            ],
            TaskType.ANALYSIS: [
                "analyze", "data", "statistics", "report", "chart", "graph",
                "trend", "pattern", "insight", "metrics"
            ],
            TaskType.REASONING: [
                "solve", "logic", "reasoning", "problem", "think", "deduce",
                "infer", "conclude", "explain", "understand"
            ],
            TaskType.CREATIVE: [
                "creative", "design", "art", "imagination", "innovative",
                "brainstorm", "idea", "concept", "original"
            ]
        }
    
    async def dispatch_request(self, request: ModelRequest) -> ModelResponse:
        """è°ƒåº¦æ¨¡å‹è¯·æ±‚"""
        start_time = time.time()
        
        try:
            # 1. è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
            if not request.task_type:
                request.task_type = await self._detect_task_type(request.content)
            
            # 2. é€‰æ‹©æœ€ä½³æ¨¡å‹
            model_id = await self._select_optimal_model(request)
            
            if not model_id:
                return ModelResponse(
                    request_id=request.request_id,
                    model_id="",
                    content="",
                    response_time=time.time() - start_time,
                    success=False,
                    error_message="æœªæ‰¾åˆ°åˆé€‚çš„æ¨¡å‹"
                )
            
            # 3. æ‰§è¡Œè¯·æ±‚
            response = await self._execute_request(request, model_id)
            
            # 4. æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            await self._update_performance_stats(request, response)
            
            # 5. è®°å½•è¯·æ±‚å†å²
            self._record_request_history(request, response)
            
            return response
            
        except Exception as e:
            return ModelResponse(
                request_id=request.request_id,
                model_id="",
                content="",
                response_time=time.time() - start_time,
                success=False,
                error_message=str(e)
            )
    
    async def _detect_task_type(self, content: Union[str, List[Dict[str, str]]]) -> TaskType:
        """è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹"""
        # å°†å†…å®¹è½¬æ¢ä¸ºæ–‡æœ¬
        if isinstance(content, list):
            text = " ".join([msg.get("content", "") for msg in content])
        else:
            text = content
        
        text_lower = text.lower()
        
        # è®¡ç®—æ¯ç§ä»»åŠ¡ç±»å‹çš„åŒ¹é…åˆ†æ•°
        task_scores = {}
        for task_type, keywords in self.task_detection_rules.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                task_scores[task_type] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„ä»»åŠ¡ç±»å‹
        if task_scores:
            best_task = max(task_scores, key=task_scores.get)
            print(f"ğŸ¯ æ£€æµ‹åˆ°ä»»åŠ¡ç±»å‹: {best_task.value} (åŒ¹é…åº¦: {task_scores[best_task]})")
            return best_task
        else:
            print("ğŸ¯ ä½¿ç”¨é»˜è®¤ä»»åŠ¡ç±»å‹: CHAT")
            return TaskType.CHAT
    
    async def _select_optimal_model(self, request: ModelRequest) -> Optional[str]:
        """é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
        # æ„å»ºé€‰æ‹©æ ‡å‡†
        criteria = ModelSelectionCriteria(
            task_type=request.task_type,
            required_capabilities=request.required_capabilities or [],
            prefer_local=request.prefer_local,
            prefer_fast=request.prefer_fast,
            prefer_quality=request.prefer_quality,
            context_length_needed=request.context_length_needed
        )
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´åå¥½
        criteria = self._adjust_criteria_by_task(criteria, request)
        
        # é€‰æ‹©æ¨¡å‹
        model_id = await self.model_manager.select_best_model(criteria)
        
        if model_id:
            print(f"ğŸ¤– ä¸ºä»»åŠ¡ {request.task_type.value} é€‰æ‹©æ¨¡å‹: {model_id}")
        
        return model_id
    
    def _adjust_criteria_by_task(self, criteria: ModelSelectionCriteria, request: ModelRequest) -> ModelSelectionCriteria:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´é€‰æ‹©æ ‡å‡†"""
        
        if request.task_type == TaskType.BROWSING:
            # æµè§ˆå™¨ä»»åŠ¡éœ€è¦è§†è§‰èƒ½åŠ›
            criteria.required_capabilities.append(ModelCapability.VISION)
            criteria.prefer_quality = True
            
        elif request.task_type == TaskType.WRITING:
            # å†™ä½œä»»åŠ¡ä¼˜å…ˆæœ¬åœ°å°æ¨¡å‹
            criteria.prefer_local = True
            criteria.required_capabilities.append(ModelCapability.CREATIVE_WRITING)
            criteria.prefer_fast = True
            
        elif request.task_type == TaskType.CODING:
            # ä»£ç ä»»åŠ¡éœ€è¦é«˜è´¨é‡å¤§æ¨¡å‹
            criteria.required_capabilities.append(ModelCapability.CODE_GENERATION)
            criteria.prefer_quality = True
            criteria.min_quality_score = 7
            
        elif request.task_type == TaskType.ANALYSIS:
            # åˆ†æä»»åŠ¡éœ€è¦æ•°æ®åˆ†æèƒ½åŠ›
            criteria.required_capabilities.append(ModelCapability.DATA_ANALYSIS)
            criteria.prefer_quality = True
            
        elif request.task_type == TaskType.REASONING:
            # æ¨ç†ä»»åŠ¡éœ€è¦æ¨ç†èƒ½åŠ›
            criteria.required_capabilities.append(ModelCapability.REASONING)
            criteria.prefer_quality = True
            criteria.min_quality_score = 6
            
        elif request.task_type == TaskType.UTILITY:
            # å·¥å…·ä»»åŠ¡ä¼˜å…ˆå¿«é€Ÿä¾¿å®œçš„æ¨¡å‹
            criteria.prefer_fast = True
            criteria.prefer_cheap = True
            
        return criteria
    
    async def _execute_request(self, request: ModelRequest, model_id: str) -> ModelResponse:
        """æ‰§è¡Œæ¨¡å‹è¯·æ±‚"""
        start_time = time.time()
        
        try:
            # è·å–æ¨¡å‹é…ç½®
            config = self.model_manager.get_model_config(model_id)
            if not config:
                raise Exception(f"æ¨¡å‹é…ç½®ä¸å­˜åœ¨: {model_id}")
            
            # è·å–æˆ–åˆ›å»ºæä¾›å•†
            provider = await get_or_create_provider(model_id, config)
            
            # å‡†å¤‡å‚æ•°
            kwargs = {}
            if request.max_tokens:
                kwargs["max_tokens"] = request.max_tokens
            if request.temperature is not None:
                kwargs["temperature"] = request.temperature
            
            # æ‰§è¡Œè¯·æ±‚
            if request.request_type == "chat" and isinstance(request.content, list):
                result = await provider.chat(request.content, **kwargs)
            elif request.request_type == "embedding":
                if isinstance(request.content, str):
                    embedding = await provider.get_embedding(request.content)
                    result = str(embedding)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿”å›
                else:
                    raise Exception("åµŒå…¥è¯·æ±‚éœ€è¦å­—ç¬¦ä¸²å†…å®¹")
            else:
                # é»˜è®¤ç”Ÿæˆè¯·æ±‚
                if isinstance(request.content, list):
                    # å°†æ¶ˆæ¯è½¬æ¢ä¸ºæç¤º
                    prompt = self._messages_to_prompt(request.content)
                else:
                    prompt = request.content
                result = await provider.generate(prompt, **kwargs)
            
            response_time = time.time() - start_time
            
            return ModelResponse(
                request_id=request.request_id,
                model_id=model_id,
                content=result,
                response_time=response_time,
                success=True,
                metadata={"provider": config.provider.value}
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            return ModelResponse(
                request_id=request.request_id,
                model_id=model_id,
                content="",
                response_time=response_time,
                success=False,
                error_message=str(e)
            )
    
    def _messages_to_prompt(self, messages: List[Dict[str, str]]) -> str:
        """å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæç¤º"""
        prompt_parts = []
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                prompt_parts.append(f"System: {content}")
            elif role == "user":
                prompt_parts.append(f"Human: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")
        
        return "\n".join(prompt_parts) + "\nAssistant:"
    
    async def _update_performance_stats(self, request: ModelRequest, response: ModelResponse):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        if response.model_id:
            await self.model_manager.update_model_performance(
                response.model_id,
                response.response_time,
                response.success
            )
            
            # æ›´æ–°ä»»åŠ¡æ€§èƒ½ç¼“å­˜
            task_key = f"{request.task_type.value}_{response.model_id}"
            if task_key not in self.task_performance_cache:
                self.task_performance_cache[task_key] = {"total_time": 0, "count": 0, "success_count": 0}
            
            cache = self.task_performance_cache[task_key]
            cache["total_time"] += response.response_time
            cache["count"] += 1
            if response.success:
                cache["success_count"] += 1
    
    def _record_request_history(self, request: ModelRequest, response: ModelResponse):
        """è®°å½•è¯·æ±‚å†å²"""
        history_entry = {
            "timestamp": datetime.now(),
            "request_id": request.request_id,
            "task_type": request.task_type.value,
            "model_id": response.model_id,
            "response_time": response.response_time,
            "success": response.success,
            "content_length": len(str(request.content)),
            "response_length": len(response.content)
        }
        
        self.request_history.append(history_entry)
        
        # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
        if len(self.request_history) > 1000:
            self.request_history = self.request_history[-1000:]
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.request_history:
            return {"status": "no_data"}
        
        # æ€»ä½“ç»Ÿè®¡
        total_requests = len(self.request_history)
        successful_requests = sum(1 for entry in self.request_history if entry["success"])
        avg_response_time = sum(entry["response_time"] for entry in self.request_history) / total_requests
        
        # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        task_stats = {}
        for entry in self.request_history:
            task_type = entry["task_type"]
            if task_type not in task_stats:
                task_stats[task_type] = {"count": 0, "success_count": 0, "total_time": 0}
            
            task_stats[task_type]["count"] += 1
            if entry["success"]:
                task_stats[task_type]["success_count"] += 1
            task_stats[task_type]["total_time"] += entry["response_time"]
        
        # è®¡ç®—ä»»åŠ¡ç±»å‹å¹³å‡å€¼
        for task_type, stats in task_stats.items():
            stats["success_rate"] = stats["success_count"] / stats["count"] if stats["count"] > 0 else 0
            stats["avg_response_time"] = stats["total_time"] / stats["count"] if stats["count"] > 0 else 0
        
        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        model_stats = {}
        for entry in self.request_history:
            model_id = entry["model_id"]
            if model_id not in model_stats:
                model_stats[model_id] = {"count": 0, "success_count": 0, "total_time": 0}
            
            model_stats[model_id]["count"] += 1
            if entry["success"]:
                model_stats[model_id]["success_count"] += 1
            model_stats[model_id]["total_time"] += entry["response_time"]
        
        # è®¡ç®—æ¨¡å‹å¹³å‡å€¼
        for model_id, stats in model_stats.items():
            stats["success_rate"] = stats["success_count"] / stats["count"] if stats["count"] > 0 else 0
            stats["avg_response_time"] = stats["total_time"] / stats["count"] if stats["count"] > 0 else 0
        
        return {
            "summary": {
                "total_requests": total_requests,
                "successful_requests": successful_requests,
                "success_rate": successful_requests / total_requests if total_requests > 0 else 0,
                "avg_response_time": avg_response_time
            },
            "task_performance": task_stats,
            "model_performance": model_stats,
            "recent_requests": self.request_history[-10:]  # æœ€è¿‘10ä¸ªè¯·æ±‚
        }
    
    async def batch_dispatch(self, requests: List[ModelRequest]) -> List[ModelResponse]:
        """æ‰¹é‡è°ƒåº¦è¯·æ±‚"""
        tasks = [self.dispatch_request(request) for request in requests]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        processed_responses = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                processed_responses.append(ModelResponse(
                    request_id=requests[i].request_id,
                    model_id="",
                    content="",
                    response_time=0,
                    success=False,
                    error_message=str(response)
                ))
            else:
                processed_responses.append(response)
        
        return processed_responses


# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
_dispatcher = IntelligentModelDispatcher()


def get_model_dispatcher() -> IntelligentModelDispatcher:
    """è·å–æ¨¡å‹è°ƒåº¦å™¨å®ä¾‹"""
    return _dispatcher


# ä¾¿æ·å‡½æ•°
async def smart_generate(content: str, task_type: TaskType = None, **kwargs) -> str:
    """æ™ºèƒ½ç”Ÿæˆæ–‡æœ¬"""
    request = ModelRequest(
        request_id=f"gen_{int(time.time())}",
        task_type=task_type,
        content=content,
        request_type="generate",
        **kwargs
    )
    
    response = await _dispatcher.dispatch_request(request)
    if response.success:
        return response.content
    else:
        raise Exception(f"ç”Ÿæˆå¤±è´¥: {response.error_message}")


async def smart_chat(messages: List[Dict[str, str]], task_type: TaskType = None, **kwargs) -> str:
    """æ™ºèƒ½å¯¹è¯"""
    request = ModelRequest(
        request_id=f"chat_{int(time.time())}",
        task_type=task_type,
        content=messages,
        request_type="chat",
        **kwargs
    )
    
    response = await _dispatcher.dispatch_request(request)
    if response.success:
        return response.content
    else:
        raise Exception(f"å¯¹è¯å¤±è´¥: {response.error_message}")


async def smart_embedding(text: str, **kwargs) -> List[float]:
    """æ™ºèƒ½åµŒå…¥"""
    request = ModelRequest(
        request_id=f"emb_{int(time.time())}",
        task_type=TaskType.EMBEDDING,
        content=text,
        request_type="embedding",
        **kwargs
    )
    
    response = await _dispatcher.dispatch_request(request)
    if response.success:
        # è§£æåµŒå…¥ç»“æœ
        import ast
        return ast.literal_eval(response.content)
    else:
        raise Exception(f"åµŒå…¥å¤±è´¥: {response.error_message}")

#!/usr/bin/env python3
"""
å¢å¼ºæ¨¡å‹ç³»ç»Ÿåˆå§‹åŒ–è„šæœ¬
è‡ªåŠ¨é…ç½®å’Œå¯åŠ¨æ™ºèƒ½æ¨¡å‹ç®¡ç†ç³»ç»Ÿ
"""

import os
import sys
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from python.helpers.model_config_initializer import get_model_initializer, initialize_models
from python.helpers.enhanced_model_manager import get_model_manager
from python.helpers.intelligent_model_dispatcher import get_model_dispatcher


def print_banner():
    """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
    print("=" * 60)
    print("ğŸš€ é›¶å·è¡ŒåŠ¨ - å¢å¼ºæ¨¡å‹ç³»ç»Ÿåˆå§‹åŒ–")
    print("=" * 60)
    print("æ”¯æŒçš„æ¨¡å‹æä¾›å•†:")
    print("  ğŸ“¡ OpenAIå…¼å®¹ç«¯ç‚¹ (OPENAI_ENDPOINT)")
    print("  ğŸ  vLLMæœ¬åœ°æœåŠ¡ (VLLM)")
    print("  ğŸ¦™ LlamaCpp GGUFæ¨¡å‹ (LLAMACPP)")
    print("=" * 60)


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®"""
    print("\nğŸ” æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    
    config_found = False
    
    # æ£€æŸ¥OpenAIå…¼å®¹ç«¯ç‚¹
    openai_endpoint = os.getenv("OPENAI_ENDPOINT")
    openai_api_key = os.getenv("OPENAI_ENDPOINT_API_KEY")
    
    if openai_endpoint and openai_api_key:
        print(f"  âœ… OpenAIå…¼å®¹ç«¯ç‚¹: {openai_endpoint}")
        config_found = True
    else:
        print("  âš ï¸  OpenAIå…¼å®¹ç«¯ç‚¹æœªé…ç½®")
        print("     è®¾ç½® OPENAI_ENDPOINT å’Œ OPENAI_ENDPOINT_API_KEY")
    
    # æ£€æŸ¥vLLMé…ç½®
    vllm_endpoint = os.getenv("VLLM_ENDPOINT", "http://localhost:8000")
    print(f"  ğŸ“ vLLMç«¯ç‚¹: {vllm_endpoint}")
    
    # æ£€æŸ¥LlamaCppé…ç½®
    llamacpp_endpoint = os.getenv("LLAMACPP_ENDPOINT", "http://localhost:8080")
    print(f"  ğŸ“ LlamaCppç«¯ç‚¹: {llamacpp_endpoint}")
    
    # æ£€æŸ¥ä¼ ç»ŸOpenAIé…ç½®
    openai_api_key_traditional = os.getenv("OPENAI_API_KEY")
    if openai_api_key_traditional:
        print("  âœ… ä¼ ç»ŸOpenAI APIå¯†é’¥å·²é…ç½®")
        config_found = True
    
    if not config_found:
        print("\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°ä»»ä½•æ¨¡å‹é…ç½®")
        print("è¯·è‡³å°‘é…ç½®ä¸€ç§æ¨¡å‹æä¾›å•†")
    
    return config_found


def create_sample_env_file():
    """åˆ›å»ºç¤ºä¾‹ç¯å¢ƒé…ç½®æ–‡ä»¶"""
    sample_env_content = """# é›¶å·è¡ŒåŠ¨å¢å¼ºæ¨¡å‹ç³»ç»Ÿé…ç½®ç¤ºä¾‹

# OpenAIå…¼å®¹ç«¯ç‚¹é…ç½®ï¼ˆäº‘ç«¯å¤§æ¨¡å‹ï¼‰
OPENAI_ENDPOINT=https://api.openai.com/v1
OPENAI_ENDPOINT_API_KEY=your_api_key_here

# æˆ–è€…ä½¿ç”¨å…¶ä»–å…¼å®¹OpenAI APIçš„æœåŠ¡
# OPENAI_ENDPOINT=https://api.anthropic.com/v1
# OPENAI_ENDPOINT_API_KEY=your_anthropic_key

# vLLMæœ¬åœ°æœåŠ¡é…ç½®
VLLM_ENDPOINT=http://localhost:8000
VLLM_MODEL_NAME=llama-2-7b-chat
VLLM_CODE_MODEL_NAME=codellama-7b-instruct

# LlamaCppé…ç½®
LLAMACPP_ENDPOINT=http://localhost:8080
LLAMACPP_MODEL_PATH=/path/to/your/model.gguf
LLAMACPP_THREADS=4
LLAMACPP_GPU_LAYERS=0
LLAMACPP_EMBEDDING_MODEL=true

# ä¼ ç»ŸOpenAIé…ç½®ï¼ˆå¤‡ç”¨ï¼‰
OPENAI_API_KEY=your_openai_key_here

# å…¶ä»–é…ç½®
PYTHONPATH=.
"""
    
    env_file = project_root / ".env.example"
    with open(env_file, "w", encoding="utf-8") as f:
        f.write(sample_env_content)
    
    print(f"\nğŸ“„ å·²åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶: {env_file}")
    print("è¯·å¤åˆ¶ä¸º .env æ–‡ä»¶å¹¶å¡«å…¥æ‚¨çš„é…ç½®")


async def test_model_system():
    """æµ‹è¯•æ¨¡å‹ç³»ç»Ÿ"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹ç³»ç»Ÿ...")
    
    try:
        # æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨
        model_manager = get_model_manager()
        stats = model_manager.get_model_statistics()
        
        print(f"  ğŸ“Š å·²æ³¨å†Œæ¨¡å‹æ•°é‡: {stats['total_models']}")
        print(f"  âœ… å¯ç”¨æ¨¡å‹æ•°é‡: {stats['enabled_models']}")
        
        if stats['total_models'] == 0:
            print("  âš ï¸  æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹")
            return False
        
        # æµ‹è¯•è°ƒåº¦å™¨
        dispatcher = get_model_dispatcher()
        
        # ç®€å•æµ‹è¯•è¯·æ±‚
        from python.helpers.intelligent_model_dispatcher import ModelRequest, TaskType
        
        test_request = ModelRequest(
            request_id="test_001",
            task_type=TaskType.CHAT,
            content="Hello, this is a test message.",
            prefer_fast=True
        )
        
        print("  ğŸ”„ æµ‹è¯•æ¨¡å‹è°ƒåº¦...")
        response = await dispatcher.dispatch_request(test_request)
        
        if response.success:
            print(f"  âœ… æ¨¡å‹è°ƒåº¦æµ‹è¯•æˆåŠŸ")
            print(f"     ä½¿ç”¨æ¨¡å‹: {response.model_id}")
            print(f"     å“åº”æ—¶é—´: {response.response_time:.2f}ç§’")
            print(f"     å“åº”é•¿åº¦: {len(response.content)}å­—ç¬¦")
        else:
            print(f"  âŒ æ¨¡å‹è°ƒåº¦æµ‹è¯•å¤±è´¥: {response.error_message}")
            return False
        
        return True
        
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def print_usage_guide():
    """æ‰“å°ä½¿ç”¨æŒ‡å—"""
    print("\n" + "=" * 60)
    print("ğŸ“š ä½¿ç”¨æŒ‡å—")
    print("=" * 60)
    
    print("\nğŸ¯ ä»»åŠ¡ç±»å‹è‡ªåŠ¨è¯†åˆ«:")
    print("  â€¢ æµè§ˆå™¨ä»»åŠ¡ â†’ å¤šæ¨¡æ€æ¨¡å‹ (æ”¯æŒè§†è§‰)")
    print("  â€¢ å†™ä½œä»»åŠ¡ â†’ æœ¬åœ°å°æ¨¡å‹ (å¿«é€Ÿä¾¿å®œ)")
    print("  â€¢ ä»£ç ä»»åŠ¡ â†’ å¤§å‹ä»£ç æ¨¡å‹ (é«˜è´¨é‡)")
    print("  â€¢ åˆ†æä»»åŠ¡ â†’ åˆ†æä¸“ç”¨æ¨¡å‹")
    print("  â€¢ æ¨ç†ä»»åŠ¡ â†’ æ¨ç†èƒ½åŠ›å¼ºçš„æ¨¡å‹")
    
    print("\nğŸ”§ æ‰‹åŠ¨ä½¿ç”¨API:")
    print("```python")
    print("from python.helpers.intelligent_model_dispatcher import smart_generate, TaskType")
    print("")
    print("# è‡ªåŠ¨é€‰æ‹©æ¨¡å‹")
    print("result = await smart_generate('å†™ä¸€ç¯‡å…³äºAIçš„æ–‡ç« ')")
    print("")
    print("# æŒ‡å®šä»»åŠ¡ç±»å‹")
    print("code = await smart_generate('å†™ä¸€ä¸ªæ’åºç®—æ³•', task_type=TaskType.CODING)")
    print("```")
    
    print("\nâš™ï¸  é…ç½®ç®¡ç†:")
    print("  â€¢ é…ç½®æ–‡ä»¶: config/model_config.json")
    print("  â€¢ ç¯å¢ƒå˜é‡: .env")
    print("  â€¢ è¿è¡Œæ—¶è°ƒæ•´: é€šè¿‡APIåŠ¨æ€é…ç½®")
    
    print("\nğŸ“Š æ€§èƒ½ç›‘æ§:")
    print("```python")
    print("from python.helpers.intelligent_model_dispatcher import get_model_dispatcher")
    print("dispatcher = get_model_dispatcher()")
    print("report = dispatcher.get_performance_report()")
    print("```")


async def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    # æ£€æŸ¥ç¯å¢ƒ
    has_config = check_environment()
    
    if not has_config:
        create_sample_env_file()
        print("\nâŒ è¯·å…ˆé…ç½®ç¯å¢ƒå˜é‡åé‡æ–°è¿è¡Œ")
        return
    
    # åˆå§‹åŒ–æ¨¡å‹ç³»ç»Ÿ
    print("\nğŸ”§ åˆå§‹åŒ–æ¨¡å‹ç³»ç»Ÿ...")
    success = initialize_models()
    
    if not success:
        print("âŒ æ¨¡å‹ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        return
    
    # æµ‹è¯•ç³»ç»Ÿ
    test_success = await test_model_system()
    
    if test_success:
        print("\nâœ… å¢å¼ºæ¨¡å‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        print_usage_guide()
    else:
        print("\nâŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ åˆå§‹åŒ–å·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

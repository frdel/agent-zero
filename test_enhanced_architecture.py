#!/usr/bin/env python3
"""
å¢å¼ºæ¶æ„é›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•æ‰€æœ‰æ–°å¢çš„æ¶æ„ç»„ä»¶å’Œæ™ºèƒ½æ¨¡å‹ç³»ç»Ÿ
"""

import asyncio
import time
import json
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


async def test_enhanced_model_system():
    """æµ‹è¯•å¢å¼ºæ¨¡å‹ç³»ç»Ÿ"""
    print("ğŸ¤– æµ‹è¯•å¢å¼ºæ¨¡å‹ç³»ç»Ÿ...")
    
    try:
        # å¯¼å…¥æ¨¡å‹ç³»ç»Ÿ
        from python.helpers.model_config_initializer import initialize_models
        from python.helpers.enhanced_model_manager import get_model_manager
        from python.helpers.intelligent_model_dispatcher import (
            get_model_dispatcher, ModelRequest, TaskType, smart_generate
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("  ğŸ”§ åˆå§‹åŒ–æ¨¡å‹é…ç½®...")
        success = initialize_models()
        if not success:
            print("  âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            return False
        
        # æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨
        print("  ğŸ“Š æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨...")
        model_manager = get_model_manager()
        stats = model_manager.get_model_statistics()
        print(f"     å·²æ³¨å†Œæ¨¡å‹: {stats['total_models']}")
        
        if stats['total_models'] == 0:
            print("  âš ï¸  æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œè·³è¿‡æ¨¡å‹æµ‹è¯•")
            return True
        
        # æµ‹è¯•æ™ºèƒ½è°ƒåº¦
        print("  ğŸ¯ æµ‹è¯•æ™ºèƒ½æ¨¡å‹è°ƒåº¦...")
        test_cases = [
            ("å†™ä¸€ç¯‡å…³äºAIçš„çŸ­æ–‡", TaskType.WRITING),
            ("ç¼–å†™ä¸€ä¸ªPythonæ’åºå‡½æ•°", TaskType.CODING),
            ("åˆ†æè¿™ç»„æ•°æ®çš„è¶‹åŠ¿", TaskType.ANALYSIS),
            ("Hello, how are you?", TaskType.CHAT)
        ]
        
        dispatcher = get_model_dispatcher()
        
        for content, task_type in test_cases:
            try:
                request = ModelRequest(
                    request_id=f"test_{int(time.time())}",
                    task_type=task_type,
                    content=content,
                    prefer_fast=True
                )
                
                response = await dispatcher.dispatch_request(request)
                if response.success:
                    print(f"     âœ… {task_type.value}: {response.model_id} ({response.response_time:.2f}s)")
                else:
                    print(f"     âŒ {task_type.value}: {response.error_message}")
            except Exception as e:
                print(f"     âš ï¸  {task_type.value}: {e}")
        
        print("  âœ… å¢å¼ºæ¨¡å‹ç³»ç»Ÿæµ‹è¯•å®Œæˆ")
        return True
        
    except ImportError as e:
        print(f"  âš ï¸  å¢å¼ºæ¨¡å‹ç³»ç»Ÿæœªå®‰è£…: {e}")
        return True
    except Exception as e:
        print(f"  âŒ å¢å¼ºæ¨¡å‹ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_universal_framework():
    """æµ‹è¯•é€šç”¨æ€§æ¡†æ¶"""
    print("ğŸ—ï¸ æµ‹è¯•é€šç”¨æ€§æ¡†æ¶...")
    
    try:
        from architecture_enhancement.universal_framework import (
            get_plugin_manager, get_config_abstractor, get_task_planner,
            TaskType, DomainType
        )
        
        # æµ‹è¯•æ’ä»¶ç®¡ç†å™¨
        print("  ğŸ”Œ æµ‹è¯•æ’ä»¶ç®¡ç†å™¨...")
        plugin_manager = get_plugin_manager()
        capabilities = plugin_manager.get_available_capabilities()
        print(f"     å¯ç”¨èƒ½åŠ›: {len(capabilities)}")
        
        # æµ‹è¯•é…ç½®æŠ½è±¡å™¨
        print("  âš™ï¸  æµ‹è¯•é…ç½®æŠ½è±¡å™¨...")
        config_abstractor = get_config_abstractor()
        test_config = config_abstractor.get_config_for_environment(
            "docker_local", {"WORK_DIR": "/workspace"}
        )
        print(f"     é…ç½®æ¨¡æ¿: {len(test_config)} é¡¹")
        
        # æµ‹è¯•ä»»åŠ¡è§„åˆ’å™¨
        print("  ğŸ“‹ æµ‹è¯•ä»»åŠ¡è§„åˆ’å™¨...")
        task_planner = get_task_planner()
        plan = await task_planner.plan_task(
            "Create a web application",
            TaskType.DEVELOPMENT,
            DomainType.TECHNICAL
        )
        print(f"     æ‰§è¡Œæ­¥éª¤: {len(plan.get('execution_steps', []))}")
        
        print("  âœ… é€šç”¨æ€§æ¡†æ¶æµ‹è¯•å®Œæˆ")
        return True
        
    except ImportError as e:
        print(f"  âš ï¸  é€šç”¨æ€§æ¡†æ¶æœªå®‰è£…: {e}")
        return True
    except Exception as e:
        print(f"  âŒ é€šç”¨æ€§æ¡†æ¶æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_intelligence_framework():
    """æµ‹è¯•æ™ºèƒ½åŒ–æ¡†æ¶"""
    print("ğŸ§  æµ‹è¯•æ™ºèƒ½åŒ–æ¡†æ¶...")
    
    try:
        from architecture_enhancement.intelligence_framework import (
            get_reasoning_engine, get_context_engine,
            DecisionContext, ReasoningType
        )
        
        # æµ‹è¯•æ¨ç†å¼•æ“
        print("  ğŸ¤” æµ‹è¯•æ¨ç†å¼•æ“...")
        reasoning_engine = get_reasoning_engine()
        
        context = DecisionContext(
            task_description="Choose the best programming language for a web project",
            available_tools=["python", "javascript", "java"],
            current_state={"project_type": "web", "team_size": "small"},
            objectives=["fast_development", "maintainability"]
        )
        
        decision = await reasoning_engine.reason(context)
        print(f"     å†³ç­–: {decision.chosen_action}")
        print(f"     æ¨ç†æ­¥éª¤: {len(decision.reasoning_chain)}")
        print(f"     ä¿¡å¿ƒåº¦: {decision.confidence_score:.2f}")
        
        # æµ‹è¯•ä¸Šä¸‹æ–‡ç†è§£å¼•æ“
        print("  ğŸ¯ æµ‹è¯•ä¸Šä¸‹æ–‡ç†è§£å¼•æ“...")
        context_engine = get_context_engine()
        
        analysis = await context_engine.analyze_context(
            "Help me build a machine learning model for image classification",
            "test_session"
        )
        
        understanding = analysis.get("comprehensive_understanding", {})
        print(f"     æ„å›¾: {understanding.get('primary_intent', 'unknown')}")
        print(f"     é¢†åŸŸ: {understanding.get('domain', 'unknown')}")
        print(f"     ä¿¡å¿ƒåº¦: {analysis.get('context_confidence', 0):.2f}")
        
        print("  âœ… æ™ºèƒ½åŒ–æ¡†æ¶æµ‹è¯•å®Œæˆ")
        return True
        
    except ImportError as e:
        print(f"  âš ï¸  æ™ºèƒ½åŒ–æ¡†æ¶æœªå®‰è£…: {e}")
        return True
    except Exception as e:
        print(f"  âŒ æ™ºèƒ½åŒ–æ¡†æ¶æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_learning_framework():
    """æµ‹è¯•å­¦ä¹ æ¡†æ¶"""
    print("ğŸ“ æµ‹è¯•å­¦ä¹ æ¡†æ¶...")
    
    try:
        from architecture_enhancement.learning_framework import (
            get_learning_engine, get_performance_evaluator,
            LearningEvent, LearningType
        )
        
        # æµ‹è¯•å­¦ä¹ å¼•æ“
        print("  ğŸ“š æµ‹è¯•å­¦ä¹ å¼•æ“...")
        learning_engine = get_learning_engine()
        
        # åˆ›å»ºå­¦ä¹ äº‹ä»¶
        event = LearningEvent(
            event_id="test_learning_001",
            event_type="tool_usage",
            input_data={"tool_name": "code_generator", "task": "create_function"},
            output_data={"result": "success", "code_quality": 8},
            success=True,
            context={"user_id": "test_user", "domain": "programming"}
        )
        
        # æ‰§è¡Œå­¦ä¹ 
        success = await learning_engine.learn_from_interaction(event)
        print(f"     å­¦ä¹ ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        # è·å–æ¨è
        recommendations = await learning_engine.get_recommendations({
            "user_id": "test_user",
            "task_type": "programming"
        })
        print(f"     æ¨èæ•°é‡: {len(recommendations)}")
        
        # æµ‹è¯•æ€§èƒ½è¯„ä¼°å™¨
        print("  ğŸ“Š æµ‹è¯•æ€§èƒ½è¯„ä¼°å™¨...")
        evaluator = get_performance_evaluator()
        
        report = await evaluator.evaluate_performance(learning_engine)
        print(f"     æ€»ä½“è¯„åˆ†: {report.get('overall_score', 0):.2f}")
        print(f"     æ”¹è¿›å»ºè®®: {len(report.get('recommendations', []))}")
        
        print("  âœ… å­¦ä¹ æ¡†æ¶æµ‹è¯•å®Œæˆ")
        return True
        
    except ImportError as e:
        print(f"  âš ï¸  å­¦ä¹ æ¡†æ¶æœªå®‰è£…: {e}")
        return True
    except Exception as e:
        print(f"  âŒ å­¦ä¹ æ¡†æ¶æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_scalability_framework():
    """æµ‹è¯•å¯æ‰©å±•æ€§æ¡†æ¶"""
    print("ğŸ“ˆ æµ‹è¯•å¯æ‰©å±•æ€§æ¡†æ¶...")
    
    try:
        from architecture_enhancement.scalability_framework import (
            get_version_manager, get_debt_manager, get_scalability_analyzer,
            ComponentVersion, ComponentType, TechnicalDebt
        )
        
        # æµ‹è¯•ç‰ˆæœ¬ç®¡ç†å™¨
        print("  ğŸ·ï¸  æµ‹è¯•ç‰ˆæœ¬ç®¡ç†å™¨...")
        version_manager = get_version_manager()
        
        component = ComponentVersion(
            component_id="test_component",
            version="1.0.0",
            component_type=ComponentType.CORE,
            dependencies={"python": ">=3.8"},
            new_features=["feature1", "feature2"]
        )
        
        success = await version_manager.register_component(component)
        print(f"     ç»„ä»¶æ³¨å†Œ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        # æµ‹è¯•æŠ€æœ¯å€ºåŠ¡ç®¡ç†å™¨
        print("  ğŸ’³ æµ‹è¯•æŠ€æœ¯å€ºåŠ¡ç®¡ç†å™¨...")
        debt_manager = get_debt_manager()
        
        debt = TechnicalDebt(
            debt_id="test_debt_001",
            component_id="test_component",
            debt_type="code_smell",
            description="éœ€è¦é‡æ„çš„å¤æ‚å‡½æ•°",
            severity=6,
            estimated_effort=8
        )
        
        success = await debt_manager.register_debt(debt)
        print(f"     å€ºåŠ¡æ³¨å†Œ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        # æµ‹è¯•å¯æ‰©å±•æ€§åˆ†æå™¨
        print("  ğŸ” æµ‹è¯•å¯æ‰©å±•æ€§åˆ†æå™¨...")
        analyzer = get_scalability_analyzer()
        
        test_components = {
            "component1": {"dependencies": ["comp2", "comp3"]},
            "component2": {"dependencies": ["comp3"]},
            "component3": {"dependencies": []}
        }
        
        metrics = await analyzer.analyze_scalability(test_components)
        print(f"     ç»„ä»¶æ•°é‡: {metrics.component_count}")
        print(f"     ä¾èµ–å¤æ‚åº¦: {metrics.dependency_complexity:.2f}")
        print(f"     è€¦åˆåº¦: {metrics.coupling_degree:.2f}")
        
        print("  âœ… å¯æ‰©å±•æ€§æ¡†æ¶æµ‹è¯•å®Œæˆ")
        return True
        
    except ImportError as e:
        print(f"  âš ï¸  å¯æ‰©å±•æ€§æ¡†æ¶æœªå®‰è£…: {e}")
        return True
    except Exception as e:
        print(f"  âŒ å¯æ‰©å±•æ€§æ¡†æ¶æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_integration():
    """æµ‹è¯•ç»„ä»¶é›†æˆ"""
    print("ğŸ”— æµ‹è¯•ç»„ä»¶é›†æˆ...")
    
    try:
        # æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµ
        print("  ğŸ”„ æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµ...")
        
        # æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„ä»»åŠ¡å¤„ç†æµç¨‹
        task_description = "Create a Python script to analyze CSV data"
        
        # 1. ä»»åŠ¡è§„åˆ’
        from architecture_enhancement.universal_framework import get_task_planner, TaskType, DomainType
        task_planner = get_task_planner()
        plan = await task_planner.plan_task(task_description, TaskType.DEVELOPMENT, DomainType.TECHNICAL)
        
        # 2. æ™ºèƒ½æ¨ç†
        from architecture_enhancement.intelligence_framework import get_reasoning_engine, DecisionContext
        reasoning_engine = get_reasoning_engine()
        context = DecisionContext(
            task_description=task_description,
            available_tools=["python", "pandas", "matplotlib"],
            current_state={"data_format": "csv"}
        )
        decision = await reasoning_engine.reason(context)
        
        # 3. å­¦ä¹ è®°å½•
        from architecture_enhancement.learning_framework import get_learning_engine, LearningEvent
        learning_engine = get_learning_engine()
        event = LearningEvent(
            event_id="integration_test",
            event_type="workflow",
            input_data={"task": task_description, "plan": plan},
            output_data={"decision": decision.chosen_action},
            success=True
        )
        await learning_engine.learn_from_interaction(event)
        
        print("     âœ… ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•æˆåŠŸ")
        
        # 4. å¦‚æœæœ‰æ¨¡å‹ç³»ç»Ÿï¼Œæµ‹è¯•æ™ºèƒ½æ¨¡å‹é€‰æ‹©
        try:
            from python.helpers.intelligent_model_dispatcher import smart_generate, TaskType as ModelTaskType
            result = await smart_generate(
                "Generate a simple Python function",
                task_type=ModelTaskType.CODING,
                prefer_fast=True
            )
            print(f"     âœ… æ™ºèƒ½æ¨¡å‹é›†æˆæµ‹è¯•æˆåŠŸ (å“åº”é•¿åº¦: {len(result)})")
        except Exception as e:
            print(f"     âš ï¸  æ™ºèƒ½æ¨¡å‹é›†æˆè·³è¿‡: {e}")
        
        print("  âœ… ç»„ä»¶é›†æˆæµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"  âŒ ç»„ä»¶é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def generate_test_report(results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ å¢å¼ºæ¶æ„æµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    
    total_tests = len(results)
    passed_tests = sum(1 for result in results.values() if result)
    
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
    
    print("\nè¯¦ç»†ç»“æœ:")
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    if passed_tests == total_tests:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼ºæ¶æ„ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
    else:
        print(f"\nâš ï¸  æœ‰ {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³ç»„ä»¶ã€‚")
    
    # ä¿å­˜æŠ¥å‘Š
    report_data = {
        "timestamp": time.time(),
        "total_tests": total_tests,
        "passed_tests": passed_tests,
        "success_rate": passed_tests/total_tests,
        "results": results
    }
    
    with open("enhanced_architecture_test_report.json", "w") as f:
        json.dump(report_data, f, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: enhanced_architecture_test_report.json")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¢å¼ºæ¶æ„é›†æˆæµ‹è¯•...")
    print("="*60)
    
    start_time = time.time()
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_results = {}
    
    test_results["enhanced_model_system"] = await test_enhanced_model_system()
    test_results["universal_framework"] = await test_universal_framework()
    test_results["intelligence_framework"] = await test_intelligence_framework()
    test_results["learning_framework"] = await test_learning_framework()
    test_results["scalability_framework"] = await test_scalability_framework()
    test_results["integration"] = await test_integration()
    
    total_time = time.time() - start_time
    
    print(f"\nâ±ï¸  æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_test_report(test_results)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æµ‹è¯•å·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
